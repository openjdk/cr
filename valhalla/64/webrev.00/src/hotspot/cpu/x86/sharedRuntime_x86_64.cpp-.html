<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Old src/hotspot/cpu/x86/sharedRuntime_x86_64.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2003, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include &quot;precompiled.hpp&quot;
  26 #ifndef _WINDOWS
  27 #include &quot;alloca.h&quot;
  28 #endif
  29 #include &quot;asm/macroAssembler.hpp&quot;
  30 #include &quot;asm/macroAssembler.inline.hpp&quot;
  31 #include &quot;classfile/symbolTable.hpp&quot;
  32 #include &quot;code/debugInfoRec.hpp&quot;
  33 #include &quot;code/icBuffer.hpp&quot;
  34 #include &quot;code/nativeInst.hpp&quot;
  35 #include &quot;code/vtableStubs.hpp&quot;
  36 #include &quot;gc/shared/collectedHeap.hpp&quot;
  37 #include &quot;gc/shared/gcLocker.hpp&quot;
  38 #include &quot;gc/shared/barrierSet.hpp&quot;
  39 #include &quot;gc/shared/barrierSetAssembler.hpp&quot;
  40 #include &quot;interpreter/interpreter.hpp&quot;
  41 #include &quot;logging/log.hpp&quot;
  42 #include &quot;memory/resourceArea.hpp&quot;
  43 #include &quot;memory/universe.hpp&quot;
  44 #include &quot;oops/compiledICHolder.hpp&quot;
  45 #include &quot;oops/klass.inline.hpp&quot;
  46 #include &quot;runtime/safepointMechanism.hpp&quot;
  47 #include &quot;runtime/sharedRuntime.hpp&quot;
  48 #include &quot;runtime/vframeArray.hpp&quot;
  49 #include &quot;runtime/vm_version.hpp&quot;
  50 #include &quot;utilities/align.hpp&quot;
  51 #include &quot;utilities/formatBuffer.hpp&quot;
  52 #include &quot;vmreg_x86.inline.hpp&quot;
  53 #ifdef COMPILER1
  54 #include &quot;c1/c1_Runtime1.hpp&quot;
  55 #endif
  56 #ifdef COMPILER2
  57 #include &quot;opto/runtime.hpp&quot;
  58 #endif
  59 #if INCLUDE_JVMCI
  60 #include &quot;jvmci/jvmciJavaClasses.hpp&quot;
  61 #endif
  62 
  63 #define __ masm-&gt;
  64 
  65 const int StackAlignmentInSlots = StackAlignmentInBytes / VMRegImpl::stack_slot_size;
  66 
  67 class SimpleRuntimeFrame {
  68 
  69   public:
  70 
  71   // Most of the runtime stubs have this simple frame layout.
  72   // This class exists to make the layout shared in one place.
  73   // Offsets are for compiler stack slots, which are jints.
  74   enum layout {
  75     // The frame sender code expects that rbp will be in the &quot;natural&quot; place and
  76     // will override any oopMap setting for it. We must therefore force the layout
  77     // so that it agrees with the frame sender code.
  78     rbp_off = frame::arg_reg_save_area_bytes/BytesPerInt,
  79     rbp_off2,
  80     return_off, return_off2,
  81     framesize
  82   };
  83 };
  84 
  85 class RegisterSaver {
  86   // Capture info about frame layout.  Layout offsets are in jint
  87   // units because compiler frame slots are jints.
  88 #define XSAVE_AREA_BEGIN 160
  89 #define XSAVE_AREA_YMM_BEGIN 576
  90 #define XSAVE_AREA_ZMM_BEGIN 1152
  91 #define XSAVE_AREA_UPPERBANK 1664
  92 #define DEF_XMM_OFFS(regnum) xmm ## regnum ## _off = xmm_off + (regnum)*16/BytesPerInt, xmm ## regnum ## H_off
  93 #define DEF_YMM_OFFS(regnum) ymm ## regnum ## _off = ymm_off + (regnum)*16/BytesPerInt, ymm ## regnum ## H_off
  94 #define DEF_ZMM_OFFS(regnum) zmm ## regnum ## _off = zmm_off + (regnum-16)*64/BytesPerInt, zmm ## regnum ## H_off
  95   enum layout {
  96     fpu_state_off = frame::arg_reg_save_area_bytes/BytesPerInt, // fxsave save area
  97     xmm_off       = fpu_state_off + XSAVE_AREA_BEGIN/BytesPerInt,            // offset in fxsave save area
  98     DEF_XMM_OFFS(0),
  99     DEF_XMM_OFFS(1),
 100     // 2..15 are implied in range usage
 101     ymm_off = xmm_off + (XSAVE_AREA_YMM_BEGIN - XSAVE_AREA_BEGIN)/BytesPerInt,
 102     DEF_YMM_OFFS(0),
 103     DEF_YMM_OFFS(1),
 104     // 2..15 are implied in range usage
 105     zmm_high = xmm_off + (XSAVE_AREA_ZMM_BEGIN - XSAVE_AREA_BEGIN)/BytesPerInt,
 106     zmm_off = xmm_off + (XSAVE_AREA_UPPERBANK - XSAVE_AREA_BEGIN)/BytesPerInt,
 107     DEF_ZMM_OFFS(16),
 108     DEF_ZMM_OFFS(17),
 109     // 18..31 are implied in range usage
 110     fpu_state_end = fpu_state_off + ((FPUStateSizeInWords-1)*wordSize / BytesPerInt),
 111     fpu_stateH_end,
 112     r15_off, r15H_off,
 113     r14_off, r14H_off,
 114     r13_off, r13H_off,
 115     r12_off, r12H_off,
 116     r11_off, r11H_off,
 117     r10_off, r10H_off,
 118     r9_off,  r9H_off,
 119     r8_off,  r8H_off,
 120     rdi_off, rdiH_off,
 121     rsi_off, rsiH_off,
 122     ignore_off, ignoreH_off,  // extra copy of rbp
 123     rsp_off, rspH_off,
 124     rbx_off, rbxH_off,
 125     rdx_off, rdxH_off,
 126     rcx_off, rcxH_off,
 127     rax_off, raxH_off,
 128     // 16-byte stack alignment fill word: see MacroAssembler::push/pop_IU_state
 129     align_off, alignH_off,
 130     flags_off, flagsH_off,
 131     // The frame sender code expects that rbp will be in the &quot;natural&quot; place and
 132     // will override any oopMap setting for it. We must therefore force the layout
 133     // so that it agrees with the frame sender code.
 134     rbp_off, rbpH_off,        // copy of rbp we will restore
 135     return_off, returnH_off,  // slot for return address
 136     reg_save_size             // size in compiler stack slots
 137   };
 138 
 139  public:
 140   static OopMap* save_live_registers(MacroAssembler* masm, int additional_frame_words, int* total_frame_words, bool save_vectors = false);
 141   static void restore_live_registers(MacroAssembler* masm, bool restore_vectors = false);
 142 
 143   // Offsets into the register save area
 144   // Used by deoptimization when it is managing result register
 145   // values on its own
 146 
 147   static int rax_offset_in_bytes(void)    { return BytesPerInt * rax_off; }
 148   static int rdx_offset_in_bytes(void)    { return BytesPerInt * rdx_off; }
 149   static int rbx_offset_in_bytes(void)    { return BytesPerInt * rbx_off; }
 150   static int xmm0_offset_in_bytes(void)   { return BytesPerInt * xmm0_off; }
 151   static int return_offset_in_bytes(void) { return BytesPerInt * return_off; }
 152 
 153   // During deoptimization only the result registers need to be restored,
 154   // all the other values have already been extracted.
 155   static void restore_result_registers(MacroAssembler* masm);
 156 };
 157 
 158 OopMap* RegisterSaver::save_live_registers(MacroAssembler* masm, int additional_frame_words, int* total_frame_words, bool save_vectors) {
 159   int off = 0;
 160   int num_xmm_regs = XMMRegisterImpl::number_of_registers;
 161   if (UseAVX &lt; 3) {
 162     num_xmm_regs = num_xmm_regs/2;
 163   }
 164 #if COMPILER2_OR_JVMCI
 165   if (save_vectors) {
 166     assert(UseAVX &gt; 0, &quot;Vectors larger than 16 byte long are supported only with AVX&quot;);
 167     assert(MaxVectorSize &lt;= 64, &quot;Only up to 64 byte long vectors are supported&quot;);
 168   }
 169 #else
 170   assert(!save_vectors, &quot;vectors are generated only by C2 and JVMCI&quot;);
 171 #endif
 172 
 173   // Always make the frame size 16-byte aligned, both vector and non vector stacks are always allocated
 174   int frame_size_in_bytes = align_up(reg_save_size*BytesPerInt, num_xmm_regs);
 175   // OopMap frame size is in compiler stack slots (jint&#39;s) not bytes or words
 176   int frame_size_in_slots = frame_size_in_bytes / BytesPerInt;
 177   // CodeBlob frame size is in words.
 178   int frame_size_in_words = frame_size_in_bytes / wordSize;
 179   *total_frame_words = frame_size_in_words;
 180 
 181   // Save registers, fpu state, and flags.
 182   // We assume caller has already pushed the return address onto the
 183   // stack, so rsp is 8-byte aligned here.
 184   // We push rpb twice in this sequence because we want the real rbp
 185   // to be under the return like a normal enter.
 186 
 187   __ enter();          // rsp becomes 16-byte aligned here
 188   __ push_CPU_state(); // Push a multiple of 16 bytes
 189 
 190   // push cpu state handles this on EVEX enabled targets
 191   if (save_vectors) {
 192     // Save upper half of YMM registers(0..15)
 193     int base_addr = XSAVE_AREA_YMM_BEGIN;
 194     for (int n = 0; n &lt; 16; n++) {
 195       __ vextractf128_high(Address(rsp, base_addr+n*16), as_XMMRegister(n));
 196     }
 197     if (VM_Version::supports_evex()) {
 198       // Save upper half of ZMM registers(0..15)
 199       base_addr = XSAVE_AREA_ZMM_BEGIN;
 200       for (int n = 0; n &lt; 16; n++) {
 201         __ vextractf64x4_high(Address(rsp, base_addr+n*32), as_XMMRegister(n));
 202       }
 203       // Save full ZMM registers(16..num_xmm_regs)
 204       base_addr = XSAVE_AREA_UPPERBANK;
 205       off = 0;
 206       int vector_len = Assembler::AVX_512bit;
 207       for (int n = 16; n &lt; num_xmm_regs; n++) {
 208         __ evmovdqul(Address(rsp, base_addr+(off++*64)), as_XMMRegister(n), vector_len);
 209       }
 210     }
 211   } else {
 212     if (VM_Version::supports_evex()) {
 213       // Save upper bank of ZMM registers(16..31) for double/float usage
 214       int base_addr = XSAVE_AREA_UPPERBANK;
 215       off = 0;
 216       for (int n = 16; n &lt; num_xmm_regs; n++) {
 217         __ movsd(Address(rsp, base_addr+(off++*64)), as_XMMRegister(n));
 218       }
 219     }
 220   }
 221   __ vzeroupper();
 222   if (frame::arg_reg_save_area_bytes != 0) {
 223     // Allocate argument register save area
 224     __ subptr(rsp, frame::arg_reg_save_area_bytes);
 225   }
 226 
 227   // Set an oopmap for the call site.  This oopmap will map all
 228   // oop-registers and debug-info registers as callee-saved.  This
 229   // will allow deoptimization at this safepoint to find all possible
 230   // debug-info recordings, as well as let GC find all oops.
 231 
 232   OopMapSet *oop_maps = new OopMapSet();
 233   OopMap* map = new OopMap(frame_size_in_slots, 0);
 234 
 235 #define STACK_OFFSET(x) VMRegImpl::stack2reg((x))
 236 
 237   map-&gt;set_callee_saved(STACK_OFFSET( rax_off ), rax-&gt;as_VMReg());
 238   map-&gt;set_callee_saved(STACK_OFFSET( rcx_off ), rcx-&gt;as_VMReg());
 239   map-&gt;set_callee_saved(STACK_OFFSET( rdx_off ), rdx-&gt;as_VMReg());
 240   map-&gt;set_callee_saved(STACK_OFFSET( rbx_off ), rbx-&gt;as_VMReg());
 241   // rbp location is known implicitly by the frame sender code, needs no oopmap
 242   // and the location where rbp was saved by is ignored
 243   map-&gt;set_callee_saved(STACK_OFFSET( rsi_off ), rsi-&gt;as_VMReg());
 244   map-&gt;set_callee_saved(STACK_OFFSET( rdi_off ), rdi-&gt;as_VMReg());
 245   map-&gt;set_callee_saved(STACK_OFFSET( r8_off  ), r8-&gt;as_VMReg());
 246   map-&gt;set_callee_saved(STACK_OFFSET( r9_off  ), r9-&gt;as_VMReg());
 247   map-&gt;set_callee_saved(STACK_OFFSET( r10_off ), r10-&gt;as_VMReg());
 248   map-&gt;set_callee_saved(STACK_OFFSET( r11_off ), r11-&gt;as_VMReg());
 249   map-&gt;set_callee_saved(STACK_OFFSET( r12_off ), r12-&gt;as_VMReg());
 250   map-&gt;set_callee_saved(STACK_OFFSET( r13_off ), r13-&gt;as_VMReg());
 251   map-&gt;set_callee_saved(STACK_OFFSET( r14_off ), r14-&gt;as_VMReg());
 252   map-&gt;set_callee_saved(STACK_OFFSET( r15_off ), r15-&gt;as_VMReg());
 253   // For both AVX and EVEX we will use the legacy FXSAVE area for xmm0..xmm15,
 254   // on EVEX enabled targets, we get it included in the xsave area
 255   off = xmm0_off;
 256   int delta = xmm1_off - off;
 257   for (int n = 0; n &lt; 16; n++) {
 258     XMMRegister xmm_name = as_XMMRegister(n);
 259     map-&gt;set_callee_saved(STACK_OFFSET(off), xmm_name-&gt;as_VMReg());
 260     off += delta;
 261   }
 262   if(UseAVX &gt; 2) {
 263     // Obtain xmm16..xmm31 from the XSAVE area on EVEX enabled targets
 264     off = zmm16_off;
 265     delta = zmm17_off - off;
 266     for (int n = 16; n &lt; num_xmm_regs; n++) {
 267       XMMRegister zmm_name = as_XMMRegister(n);
 268       map-&gt;set_callee_saved(STACK_OFFSET(off), zmm_name-&gt;as_VMReg());
 269       off += delta;
 270     }
 271   }
 272 
 273 #if COMPILER2_OR_JVMCI
 274   if (save_vectors) {
 275     off = ymm0_off;
 276     int delta = ymm1_off - off;
 277     for (int n = 0; n &lt; 16; n++) {
 278       XMMRegister ymm_name = as_XMMRegister(n);
 279       map-&gt;set_callee_saved(STACK_OFFSET(off), ymm_name-&gt;as_VMReg()-&gt;next(4));
 280       off += delta;
 281     }
 282   }
 283 #endif // COMPILER2_OR_JVMCI
 284 
 285   // %%% These should all be a waste but we&#39;ll keep things as they were for now
 286   if (true) {
 287     map-&gt;set_callee_saved(STACK_OFFSET( raxH_off ), rax-&gt;as_VMReg()-&gt;next());
 288     map-&gt;set_callee_saved(STACK_OFFSET( rcxH_off ), rcx-&gt;as_VMReg()-&gt;next());
 289     map-&gt;set_callee_saved(STACK_OFFSET( rdxH_off ), rdx-&gt;as_VMReg()-&gt;next());
 290     map-&gt;set_callee_saved(STACK_OFFSET( rbxH_off ), rbx-&gt;as_VMReg()-&gt;next());
 291     // rbp location is known implicitly by the frame sender code, needs no oopmap
 292     map-&gt;set_callee_saved(STACK_OFFSET( rsiH_off ), rsi-&gt;as_VMReg()-&gt;next());
 293     map-&gt;set_callee_saved(STACK_OFFSET( rdiH_off ), rdi-&gt;as_VMReg()-&gt;next());
 294     map-&gt;set_callee_saved(STACK_OFFSET( r8H_off  ), r8-&gt;as_VMReg()-&gt;next());
 295     map-&gt;set_callee_saved(STACK_OFFSET( r9H_off  ), r9-&gt;as_VMReg()-&gt;next());
 296     map-&gt;set_callee_saved(STACK_OFFSET( r10H_off ), r10-&gt;as_VMReg()-&gt;next());
 297     map-&gt;set_callee_saved(STACK_OFFSET( r11H_off ), r11-&gt;as_VMReg()-&gt;next());
 298     map-&gt;set_callee_saved(STACK_OFFSET( r12H_off ), r12-&gt;as_VMReg()-&gt;next());
 299     map-&gt;set_callee_saved(STACK_OFFSET( r13H_off ), r13-&gt;as_VMReg()-&gt;next());
 300     map-&gt;set_callee_saved(STACK_OFFSET( r14H_off ), r14-&gt;as_VMReg()-&gt;next());
 301     map-&gt;set_callee_saved(STACK_OFFSET( r15H_off ), r15-&gt;as_VMReg()-&gt;next());
 302     // For both AVX and EVEX we will use the legacy FXSAVE area for xmm0..xmm15,
 303     // on EVEX enabled targets, we get it included in the xsave area
 304     off = xmm0H_off;
 305     delta = xmm1H_off - off;
 306     for (int n = 0; n &lt; 16; n++) {
 307       XMMRegister xmm_name = as_XMMRegister(n);
 308       map-&gt;set_callee_saved(STACK_OFFSET(off), xmm_name-&gt;as_VMReg()-&gt;next());
 309       off += delta;
 310     }
 311     if (UseAVX &gt; 2) {
 312       // Obtain xmm16..xmm31 from the XSAVE area on EVEX enabled targets
 313       off = zmm16H_off;
 314       delta = zmm17H_off - off;
 315       for (int n = 16; n &lt; num_xmm_regs; n++) {
 316         XMMRegister zmm_name = as_XMMRegister(n);
 317         map-&gt;set_callee_saved(STACK_OFFSET(off), zmm_name-&gt;as_VMReg()-&gt;next());
 318         off += delta;
 319       }
 320     }
 321   }
 322 
 323   return map;
 324 }
 325 
 326 void RegisterSaver::restore_live_registers(MacroAssembler* masm, bool restore_vectors) {
 327   int num_xmm_regs = XMMRegisterImpl::number_of_registers;
 328   if (UseAVX &lt; 3) {
 329     num_xmm_regs = num_xmm_regs/2;
 330   }
 331   if (frame::arg_reg_save_area_bytes != 0) {
 332     // Pop arg register save area
 333     __ addptr(rsp, frame::arg_reg_save_area_bytes);
 334   }
 335 
 336 #if COMPILER2_OR_JVMCI
 337   if (restore_vectors) {
 338     assert(UseAVX &gt; 0, &quot;Vectors larger than 16 byte long are supported only with AVX&quot;);
 339     assert(MaxVectorSize &lt;= 64, &quot;Only up to 64 byte long vectors are supported&quot;);
 340   }
 341 #else
 342   assert(!restore_vectors, &quot;vectors are generated only by C2&quot;);
 343 #endif
 344 
 345   __ vzeroupper();
 346 
 347   // On EVEX enabled targets everything is handled in pop fpu state
 348   if (restore_vectors) {
 349     // Restore upper half of YMM registers (0..15)
 350     int base_addr = XSAVE_AREA_YMM_BEGIN;
 351     for (int n = 0; n &lt; 16; n++) {
 352       __ vinsertf128_high(as_XMMRegister(n), Address(rsp, base_addr+n*16));
 353     }
 354     if (VM_Version::supports_evex()) {
 355       // Restore upper half of ZMM registers (0..15)
 356       base_addr = XSAVE_AREA_ZMM_BEGIN;
 357       for (int n = 0; n &lt; 16; n++) {
 358         __ vinsertf64x4_high(as_XMMRegister(n), Address(rsp, base_addr+n*32));
 359       }
 360       // Restore full ZMM registers(16..num_xmm_regs)
 361       base_addr = XSAVE_AREA_UPPERBANK;
 362       int vector_len = Assembler::AVX_512bit;
 363       int off = 0;
 364       for (int n = 16; n &lt; num_xmm_regs; n++) {
 365         __ evmovdqul(as_XMMRegister(n), Address(rsp, base_addr+(off++*64)), vector_len);
 366       }
 367     }
 368   } else {
 369     if (VM_Version::supports_evex()) {
 370       // Restore upper bank of ZMM registers(16..31) for double/float usage
 371       int base_addr = XSAVE_AREA_UPPERBANK;
 372       int off = 0;
 373       for (int n = 16; n &lt; num_xmm_regs; n++) {
 374         __ movsd(as_XMMRegister(n), Address(rsp, base_addr+(off++*64)));
 375       }
 376     }
 377   }
 378 
 379   // Recover CPU state
 380   __ pop_CPU_state();
 381   // Get the rbp described implicitly by the calling convention (no oopMap)
 382   __ pop(rbp);
 383 }
 384 
 385 void RegisterSaver::restore_result_registers(MacroAssembler* masm) {
 386 
 387   // Just restore result register. Only used by deoptimization. By
 388   // now any callee save register that needs to be restored to a c2
 389   // caller of the deoptee has been extracted into the vframeArray
 390   // and will be stuffed into the c2i adapter we create for later
 391   // restoration so only result registers need to be restored here.
 392 
 393   // Restore fp result register
 394   __ movdbl(xmm0, Address(rsp, xmm0_offset_in_bytes()));
 395   // Restore integer result register
 396   __ movptr(rax, Address(rsp, rax_offset_in_bytes()));
 397   __ movptr(rdx, Address(rsp, rdx_offset_in_bytes()));
 398 
 399   // Pop all of the register save are off the stack except the return address
 400   __ addptr(rsp, return_offset_in_bytes());
 401 }
 402 
 403 // Is vector&#39;s size (in bytes) bigger than a size saved by default?
 404 // 16 bytes XMM registers are saved by default using fxsave/fxrstor instructions.
 405 bool SharedRuntime::is_wide_vector(int size) {
 406   return size &gt; 16;
 407 }
 408 
 409 size_t SharedRuntime::trampoline_size() {
 410   return 16;
 411 }
 412 
 413 void SharedRuntime::generate_trampoline(MacroAssembler *masm, address destination) {
 414   __ jump(RuntimeAddress(destination));
 415 }
 416 
 417 // The java_calling_convention describes stack locations as ideal slots on
 418 // a frame with no abi restrictions. Since we must observe abi restrictions
 419 // (like the placement of the register window) the slots must be biased by
 420 // the following value.
 421 static int reg2offset_in(VMReg r) {
 422   // Account for saved rbp and return address
 423   // This should really be in_preserve_stack_slots
 424   return (r-&gt;reg2stack() + 4) * VMRegImpl::stack_slot_size;
 425 }
 426 
 427 static int reg2offset_out(VMReg r) {
 428   return (r-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;
 429 }
 430 
 431 // ---------------------------------------------------------------------------
 432 // Read the array of BasicTypes from a signature, and compute where the
 433 // arguments should go.  Values in the VMRegPair regs array refer to 4-byte
 434 // quantities.  Values less than VMRegImpl::stack0 are registers, those above
 435 // refer to 4-byte stack slots.  All stack slots are based off of the stack pointer
 436 // as framesizes are fixed.
 437 // VMRegImpl::stack0 refers to the first slot 0(sp).
 438 // and VMRegImpl::stack0+1 refers to the memory word 4-byes higher.  Register
 439 // up to RegisterImpl::number_of_registers) are the 64-bit
 440 // integer registers.
 441 
 442 // Note: the INPUTS in sig_bt are in units of Java argument words, which are
 443 // either 32-bit or 64-bit depending on the build.  The OUTPUTS are in 32-bit
 444 // units regardless of build. Of course for i486 there is no 64 bit build
 445 
 446 // The Java calling convention is a &quot;shifted&quot; version of the C ABI.
 447 // By skipping the first C ABI register we can call non-static jni methods
 448 // with small numbers of arguments without having to shuffle the arguments
 449 // at all. Since we control the java ABI we ought to at least get some
 450 // advantage out of it.
 451 
 452 int SharedRuntime::java_calling_convention(const BasicType *sig_bt,
 453                                            VMRegPair *regs,
 454                                            int total_args_passed,
 455                                            int is_outgoing) {
 456 
 457   // Create the mapping between argument positions and
 458   // registers.
 459   static const Register INT_ArgReg[Argument::n_int_register_parameters_j] = {
 460     j_rarg0, j_rarg1, j_rarg2, j_rarg3, j_rarg4, j_rarg5
 461   };
 462   static const XMMRegister FP_ArgReg[Argument::n_float_register_parameters_j] = {
 463     j_farg0, j_farg1, j_farg2, j_farg3,
 464     j_farg4, j_farg5, j_farg6, j_farg7
 465   };
 466 
 467 
 468   uint int_args = 0;
 469   uint fp_args = 0;
 470   uint stk_args = 0; // inc by 2 each time
 471 
 472   for (int i = 0; i &lt; total_args_passed; i++) {
 473     switch (sig_bt[i]) {
 474     case T_BOOLEAN:
 475     case T_CHAR:
 476     case T_BYTE:
 477     case T_SHORT:
 478     case T_INT:
 479       if (int_args &lt; Argument::n_int_register_parameters_j) {
 480         regs[i].set1(INT_ArgReg[int_args++]-&gt;as_VMReg());
 481       } else {
 482         regs[i].set1(VMRegImpl::stack2reg(stk_args));
 483         stk_args += 2;
 484       }
 485       break;
 486     case T_VOID:
 487       // halves of T_LONG or T_DOUBLE
 488       assert(i != 0 &amp;&amp; (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), &quot;expecting half&quot;);
 489       regs[i].set_bad();
 490       break;
 491     case T_LONG:
 492       assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i + 1] == T_VOID, &quot;expecting half&quot;);
 493       // fall through
 494     case T_OBJECT:
 495     case T_ARRAY:
 496     case T_ADDRESS:
 497     case T_VALUETYPE:
 498       if (int_args &lt; Argument::n_int_register_parameters_j) {
 499         regs[i].set2(INT_ArgReg[int_args++]-&gt;as_VMReg());
 500       } else {
 501         regs[i].set2(VMRegImpl::stack2reg(stk_args));
 502         stk_args += 2;
 503       }
 504       break;
 505     case T_FLOAT:
 506       if (fp_args &lt; Argument::n_float_register_parameters_j) {
 507         regs[i].set1(FP_ArgReg[fp_args++]-&gt;as_VMReg());
 508       } else {
 509         regs[i].set1(VMRegImpl::stack2reg(stk_args));
 510         stk_args += 2;
 511       }
 512       break;
 513     case T_DOUBLE:
 514       assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i + 1] == T_VOID, &quot;expecting half&quot;);
 515       if (fp_args &lt; Argument::n_float_register_parameters_j) {
 516         regs[i].set2(FP_ArgReg[fp_args++]-&gt;as_VMReg());
 517       } else {
 518         regs[i].set2(VMRegImpl::stack2reg(stk_args));
 519         stk_args += 2;
 520       }
 521       break;
 522     default:
 523       ShouldNotReachHere();
 524       break;
 525     }
 526   }
 527 
 528   return align_up(stk_args, 2);
 529 }
 530 
 531 // Same as java_calling_convention() but for multiple return
 532 // values. There&#39;s no way to store them on the stack so if we don&#39;t
 533 // have enough registers, multiple values can&#39;t be returned.
 534 const uint SharedRuntime::java_return_convention_max_int = Argument::n_int_register_parameters_j+1;
 535 const uint SharedRuntime::java_return_convention_max_float = Argument::n_float_register_parameters_j;
 536 int SharedRuntime::java_return_convention(const BasicType *sig_bt,
 537                                           VMRegPair *regs,
 538                                           int total_args_passed) {
 539   // Create the mapping between argument positions and
 540   // registers.
 541   static const Register INT_ArgReg[java_return_convention_max_int] = {
 542     rax, j_rarg5, j_rarg4, j_rarg3, j_rarg2, j_rarg1, j_rarg0
 543   };
 544   static const XMMRegister FP_ArgReg[java_return_convention_max_float] = {
 545     j_farg0, j_farg1, j_farg2, j_farg3,
 546     j_farg4, j_farg5, j_farg6, j_farg7
 547   };
 548 
 549 
 550   uint int_args = 0;
 551   uint fp_args = 0;
 552 
 553   for (int i = 0; i &lt; total_args_passed; i++) {
 554     switch (sig_bt[i]) {
 555     case T_BOOLEAN:
 556     case T_CHAR:
 557     case T_BYTE:
 558     case T_SHORT:
 559     case T_INT:
 560       if (int_args &lt; Argument::n_int_register_parameters_j+1) {
 561         regs[i].set1(INT_ArgReg[int_args]-&gt;as_VMReg());
 562         int_args++;
 563       } else {
 564         return -1;
 565       }
 566       break;
 567     case T_VOID:
 568       // halves of T_LONG or T_DOUBLE
 569       assert(i != 0 &amp;&amp; (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), &quot;expecting half&quot;);
 570       regs[i].set_bad();
 571       break;
 572     case T_LONG:
 573       assert(sig_bt[i + 1] == T_VOID, &quot;expecting half&quot;);
 574       // fall through
 575     case T_OBJECT:
 576     case T_VALUETYPE:
 577     case T_ARRAY:
 578     case T_ADDRESS:
 579     case T_METADATA:
 580       if (int_args &lt; Argument::n_int_register_parameters_j+1) {
 581         regs[i].set2(INT_ArgReg[int_args]-&gt;as_VMReg());
 582         int_args++;
 583       } else {
 584         return -1;
 585       }
 586       break;
 587     case T_FLOAT:
 588       if (fp_args &lt; Argument::n_float_register_parameters_j) {
 589         regs[i].set1(FP_ArgReg[fp_args]-&gt;as_VMReg());
 590         fp_args++;
 591       } else {
 592         return -1;
 593       }
 594       break;
 595     case T_DOUBLE:
 596       assert(sig_bt[i + 1] == T_VOID, &quot;expecting half&quot;);
 597       if (fp_args &lt; Argument::n_float_register_parameters_j) {
 598         regs[i].set2(FP_ArgReg[fp_args]-&gt;as_VMReg());
 599         fp_args++;
 600       } else {
 601         return -1;
 602       }
 603       break;
 604     default:
 605       ShouldNotReachHere();
 606       break;
 607     }
 608   }
 609 
 610   return int_args + fp_args;
 611 }
 612 
 613 // Patch the callers callsite with entry to compiled code if it exists.
 614 static void patch_callers_callsite(MacroAssembler *masm) {
 615   Label L;
 616   __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);
 617   __ jcc(Assembler::equal, L);
 618 
 619   // Save the current stack pointer
 620   __ mov(r13, rsp);
 621   // Schedule the branch target address early.
 622   // Call into the VM to patch the caller, then jump to compiled callee
 623   // rax isn&#39;t live so capture return address while we easily can
 624   __ movptr(rax, Address(rsp, 0));
 625 
 626   // align stack so push_CPU_state doesn&#39;t fault
 627   __ andptr(rsp, -(StackAlignmentInBytes));
 628   __ push_CPU_state();
 629   __ vzeroupper();
 630   // VM needs caller&#39;s callsite
 631   // VM needs target method
 632   // This needs to be a long call since we will relocate this adapter to
 633   // the codeBuffer and it may not reach
 634 
 635   // Allocate argument register save area
 636   if (frame::arg_reg_save_area_bytes != 0) {
 637     __ subptr(rsp, frame::arg_reg_save_area_bytes);
 638   }
 639   __ mov(c_rarg0, rbx);
 640   __ mov(c_rarg1, rax);
 641   __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite)));
 642 
 643   // De-allocate argument register save area
 644   if (frame::arg_reg_save_area_bytes != 0) {
 645     __ addptr(rsp, frame::arg_reg_save_area_bytes);
 646   }
 647 
 648   __ vzeroupper();
 649   __ pop_CPU_state();
 650   // restore sp
 651   __ mov(rsp, r13);
 652   __ bind(L);
 653 }
 654 
 655 // For each value type argument, sig includes the list of fields of
 656 // the value type. This utility function computes the number of
 657 // arguments for the call if value types are passed by reference (the
 658 // calling convention the interpreter expects).
 659 static int compute_total_args_passed_int(const GrowableArray&lt;SigEntry&gt;* sig_extended) {
 660   int total_args_passed = 0;
 661   if (ValueTypePassFieldsAsArgs) {
 662     for (int i = 0; i &lt; sig_extended-&gt;length(); i++) {
 663       BasicType bt = sig_extended-&gt;at(i)._bt;
 664       if (SigEntry::is_reserved_entry(sig_extended, i)) {
 665         // Ignore reserved entry
 666       } else if (bt == T_VALUETYPE) {
 667         // In sig_extended, a value type argument starts with:
 668         // T_VALUETYPE, followed by the types of the fields of the
 669         // value type and T_VOID to mark the end of the value
 670         // type. Value types are flattened so, for instance, in the
 671         // case of a value type with an int field and a value type
 672         // field that itself has 2 fields, an int and a long:
 673         // T_VALUETYPE T_INT T_VALUETYPE T_INT T_LONG T_VOID (second
 674         // slot for the T_LONG) T_VOID (inner T_VALUETYPE) T_VOID
 675         // (outer T_VALUETYPE)
 676         total_args_passed++;
 677         int vt = 1;
 678         do {
 679           i++;
 680           BasicType bt = sig_extended-&gt;at(i)._bt;
 681           BasicType prev_bt = sig_extended-&gt;at(i-1)._bt;
 682           if (bt == T_VALUETYPE) {
 683             vt++;
 684           } else if (bt == T_VOID &amp;&amp;
 685                      prev_bt != T_LONG &amp;&amp;
 686                      prev_bt != T_DOUBLE) {
 687             vt--;
 688           }
 689         } while (vt != 0);
 690       } else {
 691         total_args_passed++;
 692       }
 693     }
 694   } else {
 695     total_args_passed = sig_extended-&gt;length();
 696   }
 697   return total_args_passed;
 698 }
 699 
 700 
 701 static void gen_c2i_adapter_helper(MacroAssembler* masm,
 702                                    BasicType bt,
 703                                    BasicType prev_bt,
 704                                    size_t size_in_bytes,
 705                                    const VMRegPair&amp; reg_pair,
 706                                    const Address&amp; to,
 707                                    int extraspace,
 708                                    bool is_oop) {
 709   assert(bt != T_VALUETYPE || !ValueTypePassFieldsAsArgs, &quot;no value type here&quot;);
 710   if (bt == T_VOID) {
 711     assert(prev_bt == T_LONG || prev_bt == T_DOUBLE, &quot;missing half&quot;);
 712     return;
 713   }
 714 
 715   // Say 4 args:
 716   // i   st_off
 717   // 0   32 T_LONG
 718   // 1   24 T_VOID
 719   // 2   16 T_OBJECT
 720   // 3    8 T_BOOL
 721   // -    0 return address
 722   //
 723   // However to make thing extra confusing. Because we can fit a long/double in
 724   // a single slot on a 64 bt vm and it would be silly to break them up, the interpreter
 725   // leaves one slot empty and only stores to a single slot. In this case the
 726   // slot that is occupied is the T_VOID slot. See I said it was confusing.
 727 
 728   bool wide = (size_in_bytes == wordSize);
 729   VMReg r_1 = reg_pair.first();
 730   VMReg r_2 = reg_pair.second();
 731   assert(r_2-&gt;is_valid() == wide, &quot;invalid size&quot;);
 732   if (!r_1-&gt;is_valid()) {
 733     assert(!r_2-&gt;is_valid(), &quot;must be invalid&quot;);
 734     return;
 735   }
 736 
 737   if (!r_1-&gt;is_XMMRegister()) {
 738     Register val = rax;
 739     if (r_1-&gt;is_stack()) {
 740       int ld_off = r_1-&gt;reg2stack() * VMRegImpl::stack_slot_size + extraspace;
 741       __ load_sized_value(val, Address(rsp, ld_off), size_in_bytes, /* is_signed */ false);
 742     } else {
 743       val = r_1-&gt;as_Register();
 744     }
 745     assert_different_registers(to.base(), val, rscratch1);
 746     if (is_oop) {
 747       __ push(r13);
 748       __ push(rbx);
 749       __ store_heap_oop(to, val, rscratch1, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);
 750       __ pop(rbx);
 751       __ pop(r13);
 752     } else {
 753       __ store_sized_value(to, val, size_in_bytes);
 754     }
 755   } else {
 756     if (wide) {
 757       __ movdbl(to, r_1-&gt;as_XMMRegister());
 758     } else {
 759       __ movflt(to, r_1-&gt;as_XMMRegister());
 760     }
 761   }
 762 }
 763 
 764 static void gen_c2i_adapter(MacroAssembler *masm,
 765                             const GrowableArray&lt;SigEntry&gt;* sig_extended,
 766                             const VMRegPair *regs,
 767                             Label&amp; skip_fixup,
 768                             address start,
 769                             OopMapSet* oop_maps,
 770                             int&amp; frame_complete,
 771                             int&amp; frame_size_in_words,
 772                             bool alloc_value_receiver) {
 773   // Before we get into the guts of the C2I adapter, see if we should be here
 774   // at all.  We&#39;ve come from compiled code and are attempting to jump to the
 775   // interpreter, which means the caller made a static call to get here
 776   // (vcalls always get a compiled target if there is one).  Check for a
 777   // compiled target.  If there is one, we need to patch the caller&#39;s call.
 778   patch_callers_callsite(masm);
 779 
 780   __ bind(skip_fixup);
 781 
 782   if (ValueTypePassFieldsAsArgs) {
 783     // Is there a value type argument?
 784     bool has_value_argument = false;
 785     for (int i = 0; i &lt; sig_extended-&gt;length() &amp;&amp; !has_value_argument; i++) {
 786       has_value_argument = (sig_extended-&gt;at(i)._bt == T_VALUETYPE);
 787     }
 788     if (has_value_argument) {
 789       // There is at least a value type argument: we&#39;re coming from
 790       // compiled code so we have no buffers to back the value
 791       // types. Allocate the buffers here with a runtime call.
 792       OopMap* map = RegisterSaver::save_live_registers(masm, 0, &amp;frame_size_in_words);
 793 
 794       frame_complete = __ offset();
 795 
 796       __ set_last_Java_frame(noreg, noreg, NULL);
 797 
 798       __ mov(c_rarg0, r15_thread);
 799       __ mov(c_rarg1, rbx);
 800       __ mov64(c_rarg2, (int64_t)alloc_value_receiver);
 801       __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::allocate_value_types)));
 802 
 803       oop_maps-&gt;add_gc_map((int)(__ pc() - start), map);
 804       __ reset_last_Java_frame(false);
 805 
 806       RegisterSaver::restore_live_registers(masm);
 807 
 808       Label no_exception;
 809       __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);
 810       __ jcc(Assembler::equal, no_exception);
 811 
 812       __ movptr(Address(r15_thread, JavaThread::vm_result_offset()), (int)NULL_WORD);
 813       __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));
 814       __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
 815 
 816       __ bind(no_exception);
 817 
 818       // We get an array of objects from the runtime call
 819       __ get_vm_result(rscratch2, r15_thread); // Use rscratch2 (r11) as temporary because rscratch1 (r10) is trashed by movptr()
 820       __ get_vm_result_2(rbx, r15_thread); // TODO: required to keep the callee Method live?
 821     }
 822   }
 823 
 824   // Since all args are passed on the stack, total_args_passed *
 825   // Interpreter::stackElementSize is the space we need. Plus 1 because
 826   // we also account for the return address location since
 827   // we store it first rather than hold it in rax across all the shuffling
 828   int total_args_passed = compute_total_args_passed_int(sig_extended);
 829   int extraspace = (total_args_passed * Interpreter::stackElementSize) + wordSize;
 830 
 831   // stack is aligned, keep it that way
 832   extraspace = align_up(extraspace, 2*wordSize);
 833 
 834   // Get return address
 835   __ pop(rax);
 836 
 837   // set senderSP value
 838   __ mov(r13, rsp);
 839 
 840   __ subptr(rsp, extraspace);
 841 
 842   // Store the return address in the expected location
 843   __ movptr(Address(rsp, 0), rax);
 844 
 845   // Now write the args into the outgoing interpreter space
 846 
 847   // next_arg_comp is the next argument from the compiler point of
 848   // view (value type fields are passed in registers/on the stack). In
 849   // sig_extended, a value type argument starts with: T_VALUETYPE,
 850   // followed by the types of the fields of the value type and T_VOID
 851   // to mark the end of the value type. ignored counts the number of
 852   // T_VALUETYPE/T_VOID. next_vt_arg is the next value type argument:
 853   // used to get the buffer for that argument from the pool of buffers
 854   // we allocated above and want to pass to the
 855   // interpreter. next_arg_int is the next argument from the
 856   // interpreter point of view (value types are passed by reference).
 857   for (int next_arg_comp = 0, ignored = 0, next_vt_arg = 0, next_arg_int = 0;
 858        next_arg_comp &lt; sig_extended-&gt;length(); next_arg_comp++) {
 859     assert(ignored &lt;= next_arg_comp, &quot;shouldn&#39;t skip over more slots than there are arguments&quot;);
 860     assert(next_arg_int &lt;= total_args_passed, &quot;more arguments for the interpreter than expected?&quot;);
 861     BasicType bt = sig_extended-&gt;at(next_arg_comp)._bt;
 862     int st_off = (total_args_passed - next_arg_int) * Interpreter::stackElementSize;
 863     if (!ValueTypePassFieldsAsArgs || bt != T_VALUETYPE) {
 864       if (SigEntry::is_reserved_entry(sig_extended, next_arg_comp)) {
 865         continue; // Ignore reserved entry
 866       }
 867       int next_off = st_off - Interpreter::stackElementSize;
 868       const int offset = (bt == T_LONG || bt == T_DOUBLE) ? next_off : st_off;
 869       const VMRegPair reg_pair = regs[next_arg_comp-ignored];
 870       size_t size_in_bytes = reg_pair.second()-&gt;is_valid() ? 8 : 4;
 871       gen_c2i_adapter_helper(masm, bt, next_arg_comp &gt; 0 ? sig_extended-&gt;at(next_arg_comp-1)._bt : T_ILLEGAL,
 872                              size_in_bytes, reg_pair, Address(rsp, offset), extraspace, false);
 873       next_arg_int++;
 874 #ifdef ASSERT
 875       if (bt == T_LONG || bt == T_DOUBLE) {
 876         // Overwrite the unused slot with known junk
 877         __ mov64(rax, CONST64(0xdeadffffdeadaaaa));
 878         __ movptr(Address(rsp, st_off), rax);
 879       }
 880 #endif /* ASSERT */
 881     } else {
 882       ignored++;
 883       // get the buffer from the just allocated pool of buffers
 884       int index = arrayOopDesc::base_offset_in_bytes(T_OBJECT) + next_vt_arg * type2aelembytes(T_VALUETYPE);
 885       __ load_heap_oop(r14, Address(rscratch2, index));
 886       next_vt_arg++; next_arg_int++;
 887       int vt = 1;
 888       // write fields we get from compiled code in registers/stack
 889       // slots to the buffer: we know we are done with that value type
 890       // argument when we hit the T_VOID that acts as an end of value
 891       // type delimiter for this value type. Value types are flattened
 892       // so we might encounter embedded value types. Each entry in
 893       // sig_extended contains a field offset in the buffer.
 894       do {
 895         next_arg_comp++;
 896         BasicType bt = sig_extended-&gt;at(next_arg_comp)._bt;
 897         BasicType prev_bt = sig_extended-&gt;at(next_arg_comp-1)._bt;
 898         if (bt == T_VALUETYPE) {
 899           vt++;
 900           ignored++;
 901         } else if (bt == T_VOID &amp;&amp;
 902                    prev_bt != T_LONG &amp;&amp;
 903                    prev_bt != T_DOUBLE) {
 904           vt--;
 905           ignored++;
 906         } else if (SigEntry::is_reserved_entry(sig_extended, next_arg_comp)) {
 907           // Ignore reserved entry
 908         } else {
 909           int off = sig_extended-&gt;at(next_arg_comp)._offset;
 910           assert(off &gt; 0, &quot;offset in object should be positive&quot;);
 911           size_t size_in_bytes = is_java_primitive(bt) ? type2aelembytes(bt) : wordSize;
 912           bool is_oop = is_reference_type(bt);
 913           gen_c2i_adapter_helper(masm, bt, next_arg_comp &gt; 0 ? sig_extended-&gt;at(next_arg_comp-1)._bt : T_ILLEGAL,
 914                                  size_in_bytes, regs[next_arg_comp-ignored], Address(r14, off), extraspace, is_oop);
 915         }
 916       } while (vt != 0);
 917       // pass the buffer to the interpreter
 918       __ movptr(Address(rsp, st_off), r14);
 919     }
 920   }
 921 
 922   // Schedule the branch target address early.
 923   __ movptr(rcx, Address(rbx, in_bytes(Method::interpreter_entry_offset())));
 924   __ jmp(rcx);
 925 }
 926 
 927 static void range_check(MacroAssembler* masm, Register pc_reg, Register temp_reg,
 928                         address code_start, address code_end,
 929                         Label&amp; L_ok) {
 930   Label L_fail;
 931   __ lea(temp_reg, ExternalAddress(code_start));
 932   __ cmpptr(pc_reg, temp_reg);
 933   __ jcc(Assembler::belowEqual, L_fail);
 934   __ lea(temp_reg, ExternalAddress(code_end));
 935   __ cmpptr(pc_reg, temp_reg);
 936   __ jcc(Assembler::below, L_ok);
 937   __ bind(L_fail);
 938 }
 939 
 940 void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,
 941                                     int comp_args_on_stack,
 942                                     const GrowableArray&lt;SigEntry&gt;* sig,
 943                                     const VMRegPair *regs) {
 944 
 945   // Note: r13 contains the senderSP on entry. We must preserve it since
 946   // we may do a i2c -&gt; c2i transition if we lose a race where compiled
 947   // code goes non-entrant while we get args ready.
 948   // In addition we use r13 to locate all the interpreter args as
 949   // we must align the stack to 16 bytes on an i2c entry else we
 950   // lose alignment we expect in all compiled code and register
 951   // save code can segv when fxsave instructions find improperly
 952   // aligned stack pointer.
 953 
 954   // Adapters can be frameless because they do not require the caller
 955   // to perform additional cleanup work, such as correcting the stack pointer.
 956   // An i2c adapter is frameless because the *caller* frame, which is interpreted,
 957   // routinely repairs its own stack pointer (from interpreter_frame_last_sp),
 958   // even if a callee has modified the stack pointer.
 959   // A c2i adapter is frameless because the *callee* frame, which is interpreted,
 960   // routinely repairs its caller&#39;s stack pointer (from sender_sp, which is set
 961   // up via the senderSP register).
 962   // In other words, if *either* the caller or callee is interpreted, we can
 963   // get the stack pointer repaired after a call.
 964   // This is why c2i and i2c adapters cannot be indefinitely composed.
 965   // In particular, if a c2i adapter were to somehow call an i2c adapter,
 966   // both caller and callee would be compiled methods, and neither would
 967   // clean up the stack pointer changes performed by the two adapters.
 968   // If this happens, control eventually transfers back to the compiled
 969   // caller, but with an uncorrected stack, causing delayed havoc.
 970 
 971   // Pick up the return address
 972   __ movptr(rax, Address(rsp, 0));
 973 
 974   if (VerifyAdapterCalls &amp;&amp;
 975       (Interpreter::code() != NULL || StubRoutines::code1() != NULL)) {
 976     // So, let&#39;s test for cascading c2i/i2c adapters right now.
 977     //  assert(Interpreter::contains($return_addr) ||
 978     //         StubRoutines::contains($return_addr),
 979     //         &quot;i2c adapter must return to an interpreter frame&quot;);
 980     __ block_comment(&quot;verify_i2c { &quot;);
 981     Label L_ok;
 982     if (Interpreter::code() != NULL)
 983       range_check(masm, rax, r11,
 984                   Interpreter::code()-&gt;code_start(), Interpreter::code()-&gt;code_end(),
 985                   L_ok);
 986     if (StubRoutines::code1() != NULL)
 987       range_check(masm, rax, r11,
 988                   StubRoutines::code1()-&gt;code_begin(), StubRoutines::code1()-&gt;code_end(),
 989                   L_ok);
 990     if (StubRoutines::code2() != NULL)
 991       range_check(masm, rax, r11,
 992                   StubRoutines::code2()-&gt;code_begin(), StubRoutines::code2()-&gt;code_end(),
 993                   L_ok);
 994     const char* msg = &quot;i2c adapter must return to an interpreter frame&quot;;
 995     __ block_comment(msg);
 996     __ stop(msg);
 997     __ bind(L_ok);
 998     __ block_comment(&quot;} verify_i2ce &quot;);
 999   }
1000 
1001   // Must preserve original SP for loading incoming arguments because
1002   // we need to align the outgoing SP for compiled code.
1003   __ movptr(r11, rsp);
1004 
1005   // Cut-out for having no stack args.  Since up to 2 int/oop args are passed
1006   // in registers, we will occasionally have no stack args.
1007   int comp_words_on_stack = 0;
1008   if (comp_args_on_stack) {
1009     // Sig words on the stack are greater-than VMRegImpl::stack0.  Those in
1010     // registers are below.  By subtracting stack0, we either get a negative
1011     // number (all values in registers) or the maximum stack slot accessed.
1012 
1013     // Convert 4-byte c2 stack slots to words.
1014     comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)&gt;&gt;LogBytesPerWord;
1015     // Round up to miminum stack alignment, in wordSize
1016     comp_words_on_stack = align_up(comp_words_on_stack, 2);
1017     __ subptr(rsp, comp_words_on_stack * wordSize);
1018   }
1019 
1020 
1021   // Ensure compiled code always sees stack at proper alignment
1022   __ andptr(rsp, -16);
1023 
1024   // push the return address and misalign the stack that youngest frame always sees
1025   // as far as the placement of the call instruction
1026   __ push(rax);
1027 
1028   // Put saved SP in another register
1029   const Register saved_sp = rax;
1030   __ movptr(saved_sp, r11);
1031 
1032   // Will jump to the compiled code just as if compiled code was doing it.
1033   // Pre-load the register-jump target early, to schedule it better.
1034   __ movptr(r11, Address(rbx, in_bytes(Method::from_compiled_value_offset())));
1035 
1036 #if INCLUDE_JVMCI
1037   if (EnableJVMCI || UseAOT) {
1038     // check if this call should be routed towards a specific entry point
1039     __ cmpptr(Address(r15_thread, in_bytes(JavaThread::jvmci_alternate_call_target_offset())), 0);
1040     Label no_alternative_target;
1041     __ jcc(Assembler::equal, no_alternative_target);
1042     __ movptr(r11, Address(r15_thread, in_bytes(JavaThread::jvmci_alternate_call_target_offset())));
1043     __ movptr(Address(r15_thread, in_bytes(JavaThread::jvmci_alternate_call_target_offset())), 0);
1044     __ bind(no_alternative_target);
1045   }
1046 #endif // INCLUDE_JVMCI
1047 
1048   int total_args_passed = sig-&gt;length();
1049 
1050   // Now generate the shuffle code.  Pick up all register args and move the
1051   // rest through the floating point stack top.
1052   for (int i = 0; i &lt; total_args_passed; i++) {
1053     BasicType bt = sig-&gt;at(i)._bt;
1054     assert(bt != T_VALUETYPE, &quot;i2c adapter doesn&#39;t unpack value args&quot;);
1055     if (bt == T_VOID) {
1056       // Longs and doubles are passed in native word order, but misaligned
1057       // in the 32-bit build.
1058       BasicType prev_bt = (i &gt; 0) ? sig-&gt;at(i-1)._bt : T_ILLEGAL;
1059       assert(i &gt; 0 &amp;&amp; (prev_bt == T_LONG || prev_bt == T_DOUBLE), &quot;missing half&quot;);
1060       continue;
1061     }
1062 
1063     // Pick up 0, 1 or 2 words from SP+offset.
1064 
1065     assert(!regs[i].second()-&gt;is_valid() || regs[i].first()-&gt;next() == regs[i].second(),
1066             &quot;scrambled load targets?&quot;);
1067     // Load in argument order going down.
1068     int ld_off = (total_args_passed - i)*Interpreter::stackElementSize;
1069     // Point to interpreter value (vs. tag)
1070     int next_off = ld_off - Interpreter::stackElementSize;
1071     //
1072     //
1073     //
1074     VMReg r_1 = regs[i].first();
1075     VMReg r_2 = regs[i].second();
1076     if (!r_1-&gt;is_valid()) {
1077       assert(!r_2-&gt;is_valid(), &quot;&quot;);
1078       continue;
1079     }
1080     if (r_1-&gt;is_stack()) {
1081       // Convert stack slot to an SP offset (+ wordSize to account for return address )
1082       int st_off = regs[i].first()-&gt;reg2stack()*VMRegImpl::stack_slot_size + wordSize;
1083 
1084       // We can use r13 as a temp here because compiled code doesn&#39;t need r13 as an input
1085       // and if we end up going thru a c2i because of a miss a reasonable value of r13
1086       // will be generated.
1087       if (!r_2-&gt;is_valid()) {
1088         // sign extend???
1089         __ movl(r13, Address(saved_sp, ld_off));
1090         __ movptr(Address(rsp, st_off), r13);
1091       } else {
1092         //
1093         // We are using two optoregs. This can be either T_OBJECT, T_ADDRESS, T_LONG, or T_DOUBLE
1094         // the interpreter allocates two slots but only uses one for thr T_LONG or T_DOUBLE case
1095         // So we must adjust where to pick up the data to match the interpreter.
1096         //
1097         // Interpreter local[n] == MSW, local[n+1] == LSW however locals
1098         // are accessed as negative so LSW is at LOW address
1099 
1100         // ld_off is MSW so get LSW
1101         const int offset = (bt==T_LONG||bt==T_DOUBLE)?
1102                            next_off : ld_off;
1103         __ movq(r13, Address(saved_sp, offset));
1104         // st_off is LSW (i.e. reg.first())
1105         __ movq(Address(rsp, st_off), r13);
1106       }
1107     } else if (r_1-&gt;is_Register()) {  // Register argument
1108       Register r = r_1-&gt;as_Register();
1109       assert(r != rax, &quot;must be different&quot;);
1110       if (r_2-&gt;is_valid()) {
1111         //
1112         // We are using two VMRegs. This can be either T_OBJECT, T_ADDRESS, T_LONG, or T_DOUBLE
1113         // the interpreter allocates two slots but only uses one for thr T_LONG or T_DOUBLE case
1114         // So we must adjust where to pick up the data to match the interpreter.
1115 
1116         const int offset = (bt==T_LONG||bt==T_DOUBLE)?
1117                            next_off : ld_off;
1118 
1119         // this can be a misaligned move
1120         __ movq(r, Address(saved_sp, offset));
1121       } else {
1122         // sign extend and use a full word?
1123         __ movl(r, Address(saved_sp, ld_off));
1124       }
1125     } else {
1126       if (!r_2-&gt;is_valid()) {
1127         __ movflt(r_1-&gt;as_XMMRegister(), Address(saved_sp, ld_off));
1128       } else {
1129         __ movdbl(r_1-&gt;as_XMMRegister(), Address(saved_sp, next_off));
1130       }
1131     }
1132   }
1133 
1134   // 6243940 We might end up in handle_wrong_method if
1135   // the callee is deoptimized as we race thru here. If that
1136   // happens we don&#39;t want to take a safepoint because the
1137   // caller frame will look interpreted and arguments are now
1138   // &quot;compiled&quot; so it is much better to make this transition
1139   // invisible to the stack walking code. Unfortunately if
1140   // we try and find the callee by normal means a safepoint
1141   // is possible. So we stash the desired callee in the thread
1142   // and the vm will find there should this case occur.
1143 
1144   __ movptr(Address(r15_thread, JavaThread::callee_target_offset()), rbx);
1145 
1146   // put Method* where a c2i would expect should we end up there
1147   // only needed because of c2 resolve stubs return Method* as a result in
1148   // rax
1149   __ mov(rax, rbx);
1150   __ jmp(r11);
1151 }
1152 
1153 static void gen_inline_cache_check(MacroAssembler *masm, Label&amp; skip_fixup) {
1154   Label ok;
1155 
1156   Register holder = rax;
1157   Register receiver = j_rarg0;
1158   Register temp = rbx;
1159 
1160   __ load_klass(temp, receiver);
1161   __ cmpptr(temp, Address(holder, CompiledICHolder::holder_klass_offset()));
1162   __ movptr(rbx, Address(holder, CompiledICHolder::holder_metadata_offset()));
1163   __ jcc(Assembler::equal, ok);
1164   __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
1165 
1166   __ bind(ok);
1167   // Method might have been compiled since the call site was patched to
1168   // interpreted if that is the case treat it as a miss so we can get
1169   // the call site corrected.
1170   __ cmpptr(Address(rbx, in_bytes(Method::code_offset())), (int32_t)NULL_WORD);
1171   __ jcc(Assembler::equal, skip_fixup);
1172   __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
1173 }
1174 
1175 // ---------------------------------------------------------------
1176 AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,
1177                                                             int comp_args_on_stack,
1178                                                             const GrowableArray&lt;SigEntry&gt;* sig,
1179                                                             const VMRegPair* regs,
1180                                                             const GrowableArray&lt;SigEntry&gt;* sig_cc,
1181                                                             const VMRegPair* regs_cc,
1182                                                             const GrowableArray&lt;SigEntry&gt;* sig_cc_ro,
1183                                                             const VMRegPair* regs_cc_ro,
1184                                                             AdapterFingerPrint* fingerprint,
1185                                                             AdapterBlob*&amp; new_adapter) {
1186   address i2c_entry = __ pc();
1187   gen_i2c_adapter(masm, comp_args_on_stack, sig, regs);
1188 
1189   // -------------------------------------------------------------------------
1190   // Generate a C2I adapter.  On entry we know rbx holds the Method* during calls
1191   // to the interpreter.  The args start out packed in the compiled layout.  They
1192   // need to be unpacked into the interpreter layout.  This will almost always
1193   // require some stack space.  We grow the current (compiled) stack, then repack
1194   // the args.  We  finally end in a jump to the generic interpreter entry point.
1195   // On exit from the interpreter, the interpreter will restore our SP (lest the
1196   // compiled code, which relys solely on SP and not RBP, get sick).
1197 
1198   address c2i_unverified_entry = __ pc();
1199   Label skip_fixup;
1200 
1201   gen_inline_cache_check(masm, skip_fixup);
1202 
1203   OopMapSet* oop_maps = new OopMapSet();
1204   int frame_complete = CodeOffsets::frame_never_safe;
1205   int frame_size_in_words = 0;
1206 
1207   // Scalarized c2i adapter with non-scalarized receiver (i.e., don&#39;t pack receiver)
1208   address c2i_value_ro_entry = __ pc();
1209   if (regs_cc != regs_cc_ro) {
1210     Label unused;
1211     gen_c2i_adapter(masm, sig_cc_ro, regs_cc_ro, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);
1212     skip_fixup = unused;
1213   }
1214 
1215   // Scalarized c2i adapter
1216   address c2i_entry = __ pc();
1217 
1218   // Class initialization barrier for static methods
1219   address c2i_no_clinit_check_entry = NULL;
1220   if (VM_Version::supports_fast_class_init_checks()) {
1221     Label L_skip_barrier;
1222     Register method = rbx;
1223 
1224     { // Bypass the barrier for non-static methods
1225       Register flags  = rscratch1;
1226       __ movl(flags, Address(method, Method::access_flags_offset()));
1227       __ testl(flags, JVM_ACC_STATIC);
1228       __ jcc(Assembler::zero, L_skip_barrier); // non-static
1229     }
1230 
1231     Register klass = rscratch1;
1232     __ load_method_holder(klass, method);
1233     __ clinit_barrier(klass, r15_thread, &amp;L_skip_barrier /*L_fast_path*/);
1234 
1235     __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); // slow path
1236 
1237     __ bind(L_skip_barrier);
1238     c2i_no_clinit_check_entry = __ pc();
1239   }
1240 
1241   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
1242   bs-&gt;c2i_entry_barrier(masm);
1243 
1244   gen_c2i_adapter(masm, sig_cc, regs_cc, skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, true);
1245 
1246   address c2i_unverified_value_entry = c2i_unverified_entry;
1247 
1248   // Non-scalarized c2i adapter
1249   address c2i_value_entry = c2i_entry;
1250   if (regs != regs_cc) {
1251     Label value_entry_skip_fixup;
1252     c2i_unverified_value_entry = __ pc();
1253     gen_inline_cache_check(masm, value_entry_skip_fixup);
1254 
1255     c2i_value_entry = __ pc();
1256     Label unused;
1257     gen_c2i_adapter(masm, sig, regs, value_entry_skip_fixup, i2c_entry, oop_maps, frame_complete, frame_size_in_words, false);
1258   }
1259 
1260   __ flush();
1261 
1262   // The c2i adapters might safepoint and trigger a GC. The caller must make sure that
1263   // the GC knows about the location of oop argument locations passed to the c2i adapter.
1264   bool caller_must_gc_arguments = (regs != regs_cc);
1265   new_adapter = AdapterBlob::create(masm-&gt;code(), frame_complete, frame_size_in_words, oop_maps, caller_must_gc_arguments);
1266 
1267   return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_value_entry, c2i_value_ro_entry, c2i_unverified_entry, c2i_unverified_value_entry, c2i_no_clinit_check_entry);
1268 }
1269 
1270 int SharedRuntime::c_calling_convention(const BasicType *sig_bt,
1271                                          VMRegPair *regs,
1272                                          VMRegPair *regs2,
1273                                          int total_args_passed) {
1274   assert(regs2 == NULL, &quot;not needed on x86&quot;);
1275 // We return the amount of VMRegImpl stack slots we need to reserve for all
1276 // the arguments NOT counting out_preserve_stack_slots.
1277 
1278 // NOTE: These arrays will have to change when c1 is ported
1279 #ifdef _WIN64
1280     static const Register INT_ArgReg[Argument::n_int_register_parameters_c] = {
1281       c_rarg0, c_rarg1, c_rarg2, c_rarg3
1282     };
1283     static const XMMRegister FP_ArgReg[Argument::n_float_register_parameters_c] = {
1284       c_farg0, c_farg1, c_farg2, c_farg3
1285     };
1286 #else
1287     static const Register INT_ArgReg[Argument::n_int_register_parameters_c] = {
1288       c_rarg0, c_rarg1, c_rarg2, c_rarg3, c_rarg4, c_rarg5
1289     };
1290     static const XMMRegister FP_ArgReg[Argument::n_float_register_parameters_c] = {
1291       c_farg0, c_farg1, c_farg2, c_farg3,
1292       c_farg4, c_farg5, c_farg6, c_farg7
1293     };
1294 #endif // _WIN64
1295 
1296 
1297     uint int_args = 0;
1298     uint fp_args = 0;
1299     uint stk_args = 0; // inc by 2 each time
1300 
1301     for (int i = 0; i &lt; total_args_passed; i++) {
1302       switch (sig_bt[i]) {
1303       case T_BOOLEAN:
1304       case T_CHAR:
1305       case T_BYTE:
1306       case T_SHORT:
1307       case T_INT:
1308         if (int_args &lt; Argument::n_int_register_parameters_c) {
1309           regs[i].set1(INT_ArgReg[int_args++]-&gt;as_VMReg());
1310 #ifdef _WIN64
1311           fp_args++;
1312           // Allocate slots for callee to stuff register args the stack.
1313           stk_args += 2;
1314 #endif
1315         } else {
1316           regs[i].set1(VMRegImpl::stack2reg(stk_args));
1317           stk_args += 2;
1318         }
1319         break;
1320       case T_LONG:
1321         assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i + 1] == T_VOID, &quot;expecting half&quot;);
1322         // fall through
1323       case T_OBJECT:
1324       case T_ARRAY:
1325       case T_VALUETYPE:
1326       case T_ADDRESS:
1327       case T_METADATA:
1328         if (int_args &lt; Argument::n_int_register_parameters_c) {
1329           regs[i].set2(INT_ArgReg[int_args++]-&gt;as_VMReg());
1330 #ifdef _WIN64
1331           fp_args++;
1332           stk_args += 2;
1333 #endif
1334         } else {
1335           regs[i].set2(VMRegImpl::stack2reg(stk_args));
1336           stk_args += 2;
1337         }
1338         break;
1339       case T_FLOAT:
1340         if (fp_args &lt; Argument::n_float_register_parameters_c) {
1341           regs[i].set1(FP_ArgReg[fp_args++]-&gt;as_VMReg());
1342 #ifdef _WIN64
1343           int_args++;
1344           // Allocate slots for callee to stuff register args the stack.
1345           stk_args += 2;
1346 #endif
1347         } else {
1348           regs[i].set1(VMRegImpl::stack2reg(stk_args));
1349           stk_args += 2;
1350         }
1351         break;
1352       case T_DOUBLE:
1353         assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i + 1] == T_VOID, &quot;expecting half&quot;);
1354         if (fp_args &lt; Argument::n_float_register_parameters_c) {
1355           regs[i].set2(FP_ArgReg[fp_args++]-&gt;as_VMReg());
1356 #ifdef _WIN64
1357           int_args++;
1358           // Allocate slots for callee to stuff register args the stack.
1359           stk_args += 2;
1360 #endif
1361         } else {
1362           regs[i].set2(VMRegImpl::stack2reg(stk_args));
1363           stk_args += 2;
1364         }
1365         break;
1366       case T_VOID: // Halves of longs and doubles
1367         assert(i != 0 &amp;&amp; (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), &quot;expecting half&quot;);
1368         regs[i].set_bad();
1369         break;
1370       default:
1371         ShouldNotReachHere();
1372         break;
1373       }
1374     }
1375 #ifdef _WIN64
1376   // windows abi requires that we always allocate enough stack space
1377   // for 4 64bit registers to be stored down.
1378   if (stk_args &lt; 8) {
1379     stk_args = 8;
1380   }
1381 #endif // _WIN64
1382 
1383   return stk_args;
1384 }
1385 
1386 // On 64 bit we will store integer like items to the stack as
1387 // 64 bits items (sparc abi) even though java would only store
1388 // 32bits for a parameter. On 32bit it will simply be 32 bits
1389 // So this routine will do 32-&gt;32 on 32bit and 32-&gt;64 on 64bit
1390 static void move32_64(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {
1391   if (src.first()-&gt;is_stack()) {
1392     if (dst.first()-&gt;is_stack()) {
1393       // stack to stack
1394       __ movslq(rax, Address(rbp, reg2offset_in(src.first())));
1395       __ movq(Address(rsp, reg2offset_out(dst.first())), rax);
1396     } else {
1397       // stack to reg
1398       __ movslq(dst.first()-&gt;as_Register(), Address(rbp, reg2offset_in(src.first())));
1399     }
1400   } else if (dst.first()-&gt;is_stack()) {
1401     // reg to stack
1402     // Do we really have to sign extend???
1403     // __ movslq(src.first()-&gt;as_Register(), src.first()-&gt;as_Register());
1404     __ movq(Address(rsp, reg2offset_out(dst.first())), src.first()-&gt;as_Register());
1405   } else {
1406     // Do we really have to sign extend???
1407     // __ movslq(dst.first()-&gt;as_Register(), src.first()-&gt;as_Register());
1408     if (dst.first() != src.first()) {
1409       __ movq(dst.first()-&gt;as_Register(), src.first()-&gt;as_Register());
1410     }
1411   }
1412 }
1413 
1414 static void move_ptr(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {
1415   if (src.first()-&gt;is_stack()) {
1416     if (dst.first()-&gt;is_stack()) {
1417       // stack to stack
1418       __ movq(rax, Address(rbp, reg2offset_in(src.first())));
1419       __ movq(Address(rsp, reg2offset_out(dst.first())), rax);
1420     } else {
1421       // stack to reg
1422       __ movq(dst.first()-&gt;as_Register(), Address(rbp, reg2offset_in(src.first())));
1423     }
1424   } else if (dst.first()-&gt;is_stack()) {
1425     // reg to stack
1426     __ movq(Address(rsp, reg2offset_out(dst.first())), src.first()-&gt;as_Register());
1427   } else {
1428     if (dst.first() != src.first()) {
1429       __ movq(dst.first()-&gt;as_Register(), src.first()-&gt;as_Register());
1430     }
1431   }
1432 }
1433 
1434 // An oop arg. Must pass a handle not the oop itself
1435 static void object_move(MacroAssembler* masm,
1436                         OopMap* map,
1437                         int oop_handle_offset,
1438                         int framesize_in_slots,
1439                         VMRegPair src,
1440                         VMRegPair dst,
1441                         bool is_receiver,
1442                         int* receiver_offset) {
1443 
1444   // must pass a handle. First figure out the location we use as a handle
1445 
1446   Register rHandle = dst.first()-&gt;is_stack() ? rax : dst.first()-&gt;as_Register();
1447 
1448   // See if oop is NULL if it is we need no handle
1449 
1450   if (src.first()-&gt;is_stack()) {
1451 
1452     // Oop is already on the stack as an argument
1453     int offset_in_older_frame = src.first()-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
1454     map-&gt;set_oop(VMRegImpl::stack2reg(offset_in_older_frame + framesize_in_slots));
1455     if (is_receiver) {
1456       *receiver_offset = (offset_in_older_frame + framesize_in_slots) * VMRegImpl::stack_slot_size;
1457     }
1458 
1459     __ cmpptr(Address(rbp, reg2offset_in(src.first())), (int32_t)NULL_WORD);
1460     __ lea(rHandle, Address(rbp, reg2offset_in(src.first())));
1461     // conditionally move a NULL
1462     __ cmovptr(Assembler::equal, rHandle, Address(rbp, reg2offset_in(src.first())));
1463   } else {
1464 
1465     // Oop is in an a register we must store it to the space we reserve
1466     // on the stack for oop_handles and pass a handle if oop is non-NULL
1467 
1468     const Register rOop = src.first()-&gt;as_Register();
1469     int oop_slot;
1470     if (rOop == j_rarg0)
1471       oop_slot = 0;
1472     else if (rOop == j_rarg1)
1473       oop_slot = 1;
1474     else if (rOop == j_rarg2)
1475       oop_slot = 2;
1476     else if (rOop == j_rarg3)
1477       oop_slot = 3;
1478     else if (rOop == j_rarg4)
1479       oop_slot = 4;
1480     else {
1481       assert(rOop == j_rarg5, &quot;wrong register&quot;);
1482       oop_slot = 5;
1483     }
1484 
1485     oop_slot = oop_slot * VMRegImpl::slots_per_word + oop_handle_offset;
1486     int offset = oop_slot*VMRegImpl::stack_slot_size;
1487 
1488     map-&gt;set_oop(VMRegImpl::stack2reg(oop_slot));
1489     // Store oop in handle area, may be NULL
1490     __ movptr(Address(rsp, offset), rOop);
1491     if (is_receiver) {
1492       *receiver_offset = offset;
1493     }
1494 
1495     __ cmpptr(rOop, (int32_t)NULL_WORD);
1496     __ lea(rHandle, Address(rsp, offset));
1497     // conditionally move a NULL from the handle area where it was just stored
1498     __ cmovptr(Assembler::equal, rHandle, Address(rsp, offset));
1499   }
1500 
1501   // If arg is on the stack then place it otherwise it is already in correct reg.
1502   if (dst.first()-&gt;is_stack()) {
1503     __ movptr(Address(rsp, reg2offset_out(dst.first())), rHandle);
1504   }
1505 }
1506 
1507 // A float arg may have to do float reg int reg conversion
1508 static void float_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {
1509   assert(!src.second()-&gt;is_valid() &amp;&amp; !dst.second()-&gt;is_valid(), &quot;bad float_move&quot;);
1510 
1511   // The calling conventions assures us that each VMregpair is either
1512   // all really one physical register or adjacent stack slots.
1513   // This greatly simplifies the cases here compared to sparc.
1514 
1515   if (src.first()-&gt;is_stack()) {
1516     if (dst.first()-&gt;is_stack()) {
1517       __ movl(rax, Address(rbp, reg2offset_in(src.first())));
1518       __ movptr(Address(rsp, reg2offset_out(dst.first())), rax);
1519     } else {
1520       // stack to reg
1521       assert(dst.first()-&gt;is_XMMRegister(), &quot;only expect xmm registers as parameters&quot;);
1522       __ movflt(dst.first()-&gt;as_XMMRegister(), Address(rbp, reg2offset_in(src.first())));
1523     }
1524   } else if (dst.first()-&gt;is_stack()) {
1525     // reg to stack
1526     assert(src.first()-&gt;is_XMMRegister(), &quot;only expect xmm registers as parameters&quot;);
1527     __ movflt(Address(rsp, reg2offset_out(dst.first())), src.first()-&gt;as_XMMRegister());
1528   } else {
1529     // reg to reg
1530     // In theory these overlap but the ordering is such that this is likely a nop
1531     if ( src.first() != dst.first()) {
1532       __ movdbl(dst.first()-&gt;as_XMMRegister(),  src.first()-&gt;as_XMMRegister());
1533     }
1534   }
1535 }
1536 
1537 // A long move
1538 static void long_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {
1539 
1540   // The calling conventions assures us that each VMregpair is either
1541   // all really one physical register or adjacent stack slots.
1542   // This greatly simplifies the cases here compared to sparc.
1543 
1544   if (src.is_single_phys_reg() ) {
1545     if (dst.is_single_phys_reg()) {
1546       if (dst.first() != src.first()) {
1547         __ mov(dst.first()-&gt;as_Register(), src.first()-&gt;as_Register());
1548       }
1549     } else {
1550       assert(dst.is_single_reg(), &quot;not a stack pair&quot;);
1551       __ movq(Address(rsp, reg2offset_out(dst.first())), src.first()-&gt;as_Register());
1552     }
1553   } else if (dst.is_single_phys_reg()) {
1554     assert(src.is_single_reg(),  &quot;not a stack pair&quot;);
1555     __ movq(dst.first()-&gt;as_Register(), Address(rbp, reg2offset_out(src.first())));
1556   } else {
1557     assert(src.is_single_reg() &amp;&amp; dst.is_single_reg(), &quot;not stack pairs&quot;);
1558     __ movq(rax, Address(rbp, reg2offset_in(src.first())));
1559     __ movq(Address(rsp, reg2offset_out(dst.first())), rax);
1560   }
1561 }
1562 
1563 // A double move
1564 static void double_move(MacroAssembler* masm, VMRegPair src, VMRegPair dst) {
1565 
1566   // The calling conventions assures us that each VMregpair is either
1567   // all really one physical register or adjacent stack slots.
1568   // This greatly simplifies the cases here compared to sparc.
1569 
1570   if (src.is_single_phys_reg() ) {
1571     if (dst.is_single_phys_reg()) {
1572       // In theory these overlap but the ordering is such that this is likely a nop
1573       if ( src.first() != dst.first()) {
1574         __ movdbl(dst.first()-&gt;as_XMMRegister(), src.first()-&gt;as_XMMRegister());
1575       }
1576     } else {
1577       assert(dst.is_single_reg(), &quot;not a stack pair&quot;);
1578       __ movdbl(Address(rsp, reg2offset_out(dst.first())), src.first()-&gt;as_XMMRegister());
1579     }
1580   } else if (dst.is_single_phys_reg()) {
1581     assert(src.is_single_reg(),  &quot;not a stack pair&quot;);
1582     __ movdbl(dst.first()-&gt;as_XMMRegister(), Address(rbp, reg2offset_out(src.first())));
1583   } else {
1584     assert(src.is_single_reg() &amp;&amp; dst.is_single_reg(), &quot;not stack pairs&quot;);
1585     __ movq(rax, Address(rbp, reg2offset_in(src.first())));
1586     __ movq(Address(rsp, reg2offset_out(dst.first())), rax);
1587   }
1588 }
1589 
1590 
1591 void SharedRuntime::save_native_result(MacroAssembler *masm, BasicType ret_type, int frame_slots) {
1592   // We always ignore the frame_slots arg and just use the space just below frame pointer
1593   // which by this time is free to use
1594   switch (ret_type) {
1595   case T_FLOAT:
1596     __ movflt(Address(rbp, -wordSize), xmm0);
1597     break;
1598   case T_DOUBLE:
1599     __ movdbl(Address(rbp, -wordSize), xmm0);
1600     break;
1601   case T_VOID:  break;
1602   default: {
1603     __ movptr(Address(rbp, -wordSize), rax);
1604     }
1605   }
1606 }
1607 
1608 void SharedRuntime::restore_native_result(MacroAssembler *masm, BasicType ret_type, int frame_slots) {
1609   // We always ignore the frame_slots arg and just use the space just below frame pointer
1610   // which by this time is free to use
1611   switch (ret_type) {
1612   case T_FLOAT:
1613     __ movflt(xmm0, Address(rbp, -wordSize));
1614     break;
1615   case T_DOUBLE:
1616     __ movdbl(xmm0, Address(rbp, -wordSize));
1617     break;
1618   case T_VOID:  break;
1619   default: {
1620     __ movptr(rax, Address(rbp, -wordSize));
1621     }
1622   }
1623 }
1624 
1625 static void save_args(MacroAssembler *masm, int arg_count, int first_arg, VMRegPair *args) {
1626     for ( int i = first_arg ; i &lt; arg_count ; i++ ) {
1627       if (args[i].first()-&gt;is_Register()) {
1628         __ push(args[i].first()-&gt;as_Register());
1629       } else if (args[i].first()-&gt;is_XMMRegister()) {
1630         __ subptr(rsp, 2*wordSize);
1631         __ movdbl(Address(rsp, 0), args[i].first()-&gt;as_XMMRegister());
1632       }
1633     }
1634 }
1635 
1636 static void restore_args(MacroAssembler *masm, int arg_count, int first_arg, VMRegPair *args) {
1637     for ( int i = arg_count - 1 ; i &gt;= first_arg ; i-- ) {
1638       if (args[i].first()-&gt;is_Register()) {
1639         __ pop(args[i].first()-&gt;as_Register());
1640       } else if (args[i].first()-&gt;is_XMMRegister()) {
1641         __ movdbl(args[i].first()-&gt;as_XMMRegister(), Address(rsp, 0));
1642         __ addptr(rsp, 2*wordSize);
1643       }
1644     }
1645 }
1646 
1647 
1648 static void save_or_restore_arguments(MacroAssembler* masm,
1649                                       const int stack_slots,
1650                                       const int total_in_args,
1651                                       const int arg_save_area,
1652                                       OopMap* map,
1653                                       VMRegPair* in_regs,
1654                                       BasicType* in_sig_bt) {
1655   // if map is non-NULL then the code should store the values,
1656   // otherwise it should load them.
1657   int slot = arg_save_area;
1658   // Save down double word first
1659   for ( int i = 0; i &lt; total_in_args; i++) {
1660     if (in_regs[i].first()-&gt;is_XMMRegister() &amp;&amp; in_sig_bt[i] == T_DOUBLE) {
1661       int offset = slot * VMRegImpl::stack_slot_size;
1662       slot += VMRegImpl::slots_per_word;
1663       assert(slot &lt;= stack_slots, &quot;overflow&quot;);
1664       if (map != NULL) {
1665         __ movdbl(Address(rsp, offset), in_regs[i].first()-&gt;as_XMMRegister());
1666       } else {
1667         __ movdbl(in_regs[i].first()-&gt;as_XMMRegister(), Address(rsp, offset));
1668       }
1669     }
1670     if (in_regs[i].first()-&gt;is_Register() &amp;&amp;
1671         (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_ARRAY)) {
1672       int offset = slot * VMRegImpl::stack_slot_size;
1673       if (map != NULL) {
1674         __ movq(Address(rsp, offset), in_regs[i].first()-&gt;as_Register());
1675         if (in_sig_bt[i] == T_ARRAY) {
1676           map-&gt;set_oop(VMRegImpl::stack2reg(slot));
1677         }
1678       } else {
1679         __ movq(in_regs[i].first()-&gt;as_Register(), Address(rsp, offset));
1680       }
1681       slot += VMRegImpl::slots_per_word;
1682     }
1683   }
1684   // Save or restore single word registers
1685   for ( int i = 0; i &lt; total_in_args; i++) {
1686     if (in_regs[i].first()-&gt;is_Register()) {
1687       int offset = slot * VMRegImpl::stack_slot_size;
1688       slot++;
1689       assert(slot &lt;= stack_slots, &quot;overflow&quot;);
1690 
1691       // Value is in an input register pass we must flush it to the stack
1692       const Register reg = in_regs[i].first()-&gt;as_Register();
1693       switch (in_sig_bt[i]) {
1694         case T_BOOLEAN:
1695         case T_CHAR:
1696         case T_BYTE:
1697         case T_SHORT:
1698         case T_INT:
1699           if (map != NULL) {
1700             __ movl(Address(rsp, offset), reg);
1701           } else {
1702             __ movl(reg, Address(rsp, offset));
1703           }
1704           break;
1705         case T_ARRAY:
1706         case T_LONG:
1707           // handled above
1708           break;
1709         case T_OBJECT:
1710         case T_VALUETYPE:
1711         default: ShouldNotReachHere();
1712       }
1713     } else if (in_regs[i].first()-&gt;is_XMMRegister()) {
1714       if (in_sig_bt[i] == T_FLOAT) {
1715         int offset = slot * VMRegImpl::stack_slot_size;
1716         slot++;
1717         assert(slot &lt;= stack_slots, &quot;overflow&quot;);
1718         if (map != NULL) {
1719           __ movflt(Address(rsp, offset), in_regs[i].first()-&gt;as_XMMRegister());
1720         } else {
1721           __ movflt(in_regs[i].first()-&gt;as_XMMRegister(), Address(rsp, offset));
1722         }
1723       }
1724     } else if (in_regs[i].first()-&gt;is_stack()) {
1725       if (in_sig_bt[i] == T_ARRAY &amp;&amp; map != NULL) {
1726         int offset_in_older_frame = in_regs[i].first()-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
1727         map-&gt;set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));
1728       }
1729     }
1730   }
1731 }
1732 
1733 // Pin object, return pinned object or null in rax
1734 static void gen_pin_object(MacroAssembler* masm,
1735                            VMRegPair reg) {
1736   __ block_comment(&quot;gen_pin_object {&quot;);
1737 
1738   // rax always contains oop, either incoming or
1739   // pinned.
1740   Register tmp_reg = rax;
1741 
1742   Label is_null;
1743   VMRegPair tmp;
1744   VMRegPair in_reg = reg;
1745 
1746   tmp.set_ptr(tmp_reg-&gt;as_VMReg());
1747   if (reg.first()-&gt;is_stack()) {
1748     // Load the arg up from the stack
1749     move_ptr(masm, reg, tmp);
1750     reg = tmp;
1751   } else {
1752     __ movptr(rax, reg.first()-&gt;as_Register());
1753   }
1754   __ testptr(reg.first()-&gt;as_Register(), reg.first()-&gt;as_Register());
1755   __ jccb(Assembler::equal, is_null);
1756 
1757   if (reg.first()-&gt;as_Register() != c_rarg1) {
1758     __ movptr(c_rarg1, reg.first()-&gt;as_Register());
1759   }
1760 
1761   __ call_VM_leaf(
1762     CAST_FROM_FN_PTR(address, SharedRuntime::pin_object),
1763     r15_thread, c_rarg1);
1764 
1765   __ bind(is_null);
1766   __ block_comment(&quot;} gen_pin_object&quot;);
1767 }
1768 
1769 // Unpin object
1770 static void gen_unpin_object(MacroAssembler* masm,
1771                              VMRegPair reg) {
1772   __ block_comment(&quot;gen_unpin_object {&quot;);
1773   Label is_null;
1774 
1775   if (reg.first()-&gt;is_stack()) {
1776     __ movptr(c_rarg1, Address(rbp, reg2offset_in(reg.first())));
1777   } else if (reg.first()-&gt;as_Register() != c_rarg1) {
1778     __ movptr(c_rarg1, reg.first()-&gt;as_Register());
1779   }
1780 
1781   __ testptr(c_rarg1, c_rarg1);
1782   __ jccb(Assembler::equal, is_null);
1783 
1784   __ call_VM_leaf(
1785     CAST_FROM_FN_PTR(address, SharedRuntime::unpin_object),
1786     r15_thread, c_rarg1);
1787 
1788   __ bind(is_null);
1789   __ block_comment(&quot;} gen_unpin_object&quot;);
1790 }
1791 
1792 // Check GCLocker::needs_gc and enter the runtime if it&#39;s true.  This
1793 // keeps a new JNI critical region from starting until a GC has been
1794 // forced.  Save down any oops in registers and describe them in an
1795 // OopMap.
1796 static void check_needs_gc_for_critical_native(MacroAssembler* masm,
1797                                                int stack_slots,
1798                                                int total_c_args,
1799                                                int total_in_args,
1800                                                int arg_save_area,
1801                                                OopMapSet* oop_maps,
1802                                                VMRegPair* in_regs,
1803                                                BasicType* in_sig_bt) {
1804   __ block_comment(&quot;check GCLocker::needs_gc&quot;);
1805   Label cont;
1806   __ cmp8(ExternalAddress((address)GCLocker::needs_gc_address()), false);
1807   __ jcc(Assembler::equal, cont);
1808 
1809   // Save down any incoming oops and call into the runtime to halt for a GC
1810 
1811   OopMap* map = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
1812   save_or_restore_arguments(masm, stack_slots, total_in_args,
1813                             arg_save_area, map, in_regs, in_sig_bt);
1814 
1815   address the_pc = __ pc();
1816   oop_maps-&gt;add_gc_map( __ offset(), map);
1817   __ set_last_Java_frame(rsp, noreg, the_pc);
1818 
1819   __ block_comment(&quot;block_for_jni_critical&quot;);
1820   __ movptr(c_rarg0, r15_thread);
1821   __ mov(r12, rsp); // remember sp
1822   __ subptr(rsp, frame::arg_reg_save_area_bytes); // windows
1823   __ andptr(rsp, -16); // align stack as required by ABI
1824   __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::block_for_jni_critical)));
1825   __ mov(rsp, r12); // restore sp
1826   __ reinit_heapbase();
1827 
1828   __ reset_last_Java_frame(false);
1829 
1830   save_or_restore_arguments(masm, stack_slots, total_in_args,
1831                             arg_save_area, NULL, in_regs, in_sig_bt);
1832   __ bind(cont);
1833 #ifdef ASSERT
1834   if (StressCriticalJNINatives) {
1835     // Stress register saving
1836     OopMap* map = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
1837     save_or_restore_arguments(masm, stack_slots, total_in_args,
1838                               arg_save_area, map, in_regs, in_sig_bt);
1839     // Destroy argument registers
1840     for (int i = 0; i &lt; total_in_args - 1; i++) {
1841       if (in_regs[i].first()-&gt;is_Register()) {
1842         const Register reg = in_regs[i].first()-&gt;as_Register();
1843         __ xorptr(reg, reg);
1844       } else if (in_regs[i].first()-&gt;is_XMMRegister()) {
1845         __ xorpd(in_regs[i].first()-&gt;as_XMMRegister(), in_regs[i].first()-&gt;as_XMMRegister());
1846       } else if (in_regs[i].first()-&gt;is_FloatRegister()) {
1847         ShouldNotReachHere();
1848       } else if (in_regs[i].first()-&gt;is_stack()) {
1849         // Nothing to do
1850       } else {
1851         ShouldNotReachHere();
1852       }
1853       if (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_DOUBLE) {
1854         i++;
1855       }
1856     }
1857 
1858     save_or_restore_arguments(masm, stack_slots, total_in_args,
1859                               arg_save_area, NULL, in_regs, in_sig_bt);
1860   }
1861 #endif
1862 }
1863 
1864 // Unpack an array argument into a pointer to the body and the length
1865 // if the array is non-null, otherwise pass 0 for both.
1866 static void unpack_array_argument(MacroAssembler* masm, VMRegPair reg, BasicType in_elem_type, VMRegPair body_arg, VMRegPair length_arg) {
1867   Register tmp_reg = rax;
1868   assert(!body_arg.first()-&gt;is_Register() || body_arg.first()-&gt;as_Register() != tmp_reg,
1869          &quot;possible collision&quot;);
1870   assert(!length_arg.first()-&gt;is_Register() || length_arg.first()-&gt;as_Register() != tmp_reg,
1871          &quot;possible collision&quot;);
1872 
1873   __ block_comment(&quot;unpack_array_argument {&quot;);
1874 
1875   // Pass the length, ptr pair
1876   Label is_null, done;
1877   VMRegPair tmp;
1878   tmp.set_ptr(tmp_reg-&gt;as_VMReg());
1879   if (reg.first()-&gt;is_stack()) {
1880     // Load the arg up from the stack
1881     move_ptr(masm, reg, tmp);
1882     reg = tmp;
1883   }
1884   __ testptr(reg.first()-&gt;as_Register(), reg.first()-&gt;as_Register());
1885   __ jccb(Assembler::equal, is_null);
1886   __ lea(tmp_reg, Address(reg.first()-&gt;as_Register(), arrayOopDesc::base_offset_in_bytes(in_elem_type)));
1887   move_ptr(masm, tmp, body_arg);
1888   // load the length relative to the body.
1889   __ movl(tmp_reg, Address(tmp_reg, arrayOopDesc::length_offset_in_bytes() -
1890                            arrayOopDesc::base_offset_in_bytes(in_elem_type)));
1891   move32_64(masm, tmp, length_arg);
1892   __ jmpb(done);
1893   __ bind(is_null);
1894   // Pass zeros
1895   __ xorptr(tmp_reg, tmp_reg);
1896   move_ptr(masm, tmp, body_arg);
1897   move32_64(masm, tmp, length_arg);
1898   __ bind(done);
1899 
1900   __ block_comment(&quot;} unpack_array_argument&quot;);
1901 }
1902 
1903 
1904 // Different signatures may require very different orders for the move
1905 // to avoid clobbering other arguments.  There&#39;s no simple way to
1906 // order them safely.  Compute a safe order for issuing stores and
1907 // break any cycles in those stores.  This code is fairly general but
1908 // it&#39;s not necessary on the other platforms so we keep it in the
1909 // platform dependent code instead of moving it into a shared file.
1910 // (See bugs 7013347 &amp; 7145024.)
1911 // Note that this code is specific to LP64.
1912 class ComputeMoveOrder: public StackObj {
1913   class MoveOperation: public ResourceObj {
1914     friend class ComputeMoveOrder;
1915    private:
1916     VMRegPair        _src;
1917     VMRegPair        _dst;
1918     int              _src_index;
1919     int              _dst_index;
1920     bool             _processed;
1921     MoveOperation*  _next;
1922     MoveOperation*  _prev;
1923 
1924     static int get_id(VMRegPair r) {
1925       return r.first()-&gt;value();
1926     }
1927 
1928    public:
1929     MoveOperation(int src_index, VMRegPair src, int dst_index, VMRegPair dst):
1930       _src(src)
1931     , _dst(dst)
1932     , _src_index(src_index)
1933     , _dst_index(dst_index)
1934     , _processed(false)
1935     , _next(NULL)
1936     , _prev(NULL) {
1937     }
1938 
1939     VMRegPair src() const              { return _src; }
1940     int src_id() const                 { return get_id(src()); }
1941     int src_index() const              { return _src_index; }
1942     VMRegPair dst() const              { return _dst; }
1943     void set_dst(int i, VMRegPair dst) { _dst_index = i, _dst = dst; }
1944     int dst_index() const              { return _dst_index; }
1945     int dst_id() const                 { return get_id(dst()); }
1946     MoveOperation* next() const       { return _next; }
1947     MoveOperation* prev() const       { return _prev; }
1948     void set_processed()               { _processed = true; }
1949     bool is_processed() const          { return _processed; }
1950 
1951     // insert
1952     void break_cycle(VMRegPair temp_register) {
1953       // create a new store following the last store
1954       // to move from the temp_register to the original
1955       MoveOperation* new_store = new MoveOperation(-1, temp_register, dst_index(), dst());
1956 
1957       // break the cycle of links and insert new_store at the end
1958       // break the reverse link.
1959       MoveOperation* p = prev();
1960       assert(p-&gt;next() == this, &quot;must be&quot;);
1961       _prev = NULL;
1962       p-&gt;_next = new_store;
1963       new_store-&gt;_prev = p;
1964 
1965       // change the original store to save it&#39;s value in the temp.
1966       set_dst(-1, temp_register);
1967     }
1968 
1969     void link(GrowableArray&lt;MoveOperation*&gt;&amp; killer) {
1970       // link this store in front the store that it depends on
1971       MoveOperation* n = killer.at_grow(src_id(), NULL);
1972       if (n != NULL) {
1973         assert(_next == NULL &amp;&amp; n-&gt;_prev == NULL, &quot;shouldn&#39;t have been set yet&quot;);
1974         _next = n;
1975         n-&gt;_prev = this;
1976       }
1977     }
1978   };
1979 
1980  private:
1981   GrowableArray&lt;MoveOperation*&gt; edges;
1982 
1983  public:
1984   ComputeMoveOrder(int total_in_args, VMRegPair* in_regs, int total_c_args, VMRegPair* out_regs,
1985                     BasicType* in_sig_bt, GrowableArray&lt;int&gt;&amp; arg_order, VMRegPair tmp_vmreg) {
1986     // Move operations where the dest is the stack can all be
1987     // scheduled first since they can&#39;t interfere with the other moves.
1988     for (int i = total_in_args - 1, c_arg = total_c_args - 1; i &gt;= 0; i--, c_arg--) {
1989       if (in_sig_bt[i] == T_ARRAY) {
1990         c_arg--;
1991         if (out_regs[c_arg].first()-&gt;is_stack() &amp;&amp;
1992             out_regs[c_arg + 1].first()-&gt;is_stack()) {
1993           arg_order.push(i);
1994           arg_order.push(c_arg);
1995         } else {
1996           if (out_regs[c_arg].first()-&gt;is_stack() ||
1997               in_regs[i].first() == out_regs[c_arg].first()) {
1998             add_edge(i, in_regs[i].first(), c_arg, out_regs[c_arg + 1]);
1999           } else {
2000             add_edge(i, in_regs[i].first(), c_arg, out_regs[c_arg]);
2001           }
2002         }
2003       } else if (in_sig_bt[i] == T_VOID) {
2004         arg_order.push(i);
2005         arg_order.push(c_arg);
2006       } else {
2007         if (out_regs[c_arg].first()-&gt;is_stack() ||
2008             in_regs[i].first() == out_regs[c_arg].first()) {
2009           arg_order.push(i);
2010           arg_order.push(c_arg);
2011         } else {
2012           add_edge(i, in_regs[i].first(), c_arg, out_regs[c_arg]);
2013         }
2014       }
2015     }
2016     // Break any cycles in the register moves and emit the in the
2017     // proper order.
2018     GrowableArray&lt;MoveOperation*&gt;* stores = get_store_order(tmp_vmreg);
2019     for (int i = 0; i &lt; stores-&gt;length(); i++) {
2020       arg_order.push(stores-&gt;at(i)-&gt;src_index());
2021       arg_order.push(stores-&gt;at(i)-&gt;dst_index());
2022     }
2023  }
2024 
2025   // Collected all the move operations
2026   void add_edge(int src_index, VMRegPair src, int dst_index, VMRegPair dst) {
2027     if (src.first() == dst.first()) return;
2028     edges.append(new MoveOperation(src_index, src, dst_index, dst));
2029   }
2030 
2031   // Walk the edges breaking cycles between moves.  The result list
2032   // can be walked in order to produce the proper set of loads
2033   GrowableArray&lt;MoveOperation*&gt;* get_store_order(VMRegPair temp_register) {
2034     // Record which moves kill which values
2035     GrowableArray&lt;MoveOperation*&gt; killer;
2036     for (int i = 0; i &lt; edges.length(); i++) {
2037       MoveOperation* s = edges.at(i);
2038       assert(killer.at_grow(s-&gt;dst_id(), NULL) == NULL, &quot;only one killer&quot;);
2039       killer.at_put_grow(s-&gt;dst_id(), s, NULL);
2040     }
2041     assert(killer.at_grow(MoveOperation::get_id(temp_register), NULL) == NULL,
2042            &quot;make sure temp isn&#39;t in the registers that are killed&quot;);
2043 
2044     // create links between loads and stores
2045     for (int i = 0; i &lt; edges.length(); i++) {
2046       edges.at(i)-&gt;link(killer);
2047     }
2048 
2049     // at this point, all the move operations are chained together
2050     // in a doubly linked list.  Processing it backwards finds
2051     // the beginning of the chain, forwards finds the end.  If there&#39;s
2052     // a cycle it can be broken at any point,  so pick an edge and walk
2053     // backward until the list ends or we end where we started.
2054     GrowableArray&lt;MoveOperation*&gt;* stores = new GrowableArray&lt;MoveOperation*&gt;();
2055     for (int e = 0; e &lt; edges.length(); e++) {
2056       MoveOperation* s = edges.at(e);
2057       if (!s-&gt;is_processed()) {
2058         MoveOperation* start = s;
2059         // search for the beginning of the chain or cycle
2060         while (start-&gt;prev() != NULL &amp;&amp; start-&gt;prev() != s) {
2061           start = start-&gt;prev();
2062         }
2063         if (start-&gt;prev() == s) {
2064           start-&gt;break_cycle(temp_register);
2065         }
2066         // walk the chain forward inserting to store list
2067         while (start != NULL) {
2068           stores-&gt;append(start);
2069           start-&gt;set_processed();
2070           start = start-&gt;next();
2071         }
2072       }
2073     }
2074     return stores;
2075   }
2076 };
2077 
2078 static void verify_oop_args(MacroAssembler* masm,
2079                             const methodHandle&amp; method,
2080                             const BasicType* sig_bt,
2081                             const VMRegPair* regs) {
2082   Register temp_reg = rbx;  // not part of any compiled calling seq
2083   if (VerifyOops) {
2084     for (int i = 0; i &lt; method-&gt;size_of_parameters(); i++) {
2085       if (is_reference_type(sig_bt[i])) {
2086         VMReg r = regs[i].first();
2087         assert(r-&gt;is_valid(), &quot;bad oop arg&quot;);
2088         if (r-&gt;is_stack()) {
2089           __ movptr(temp_reg, Address(rsp, r-&gt;reg2stack() * VMRegImpl::stack_slot_size + wordSize));
2090           __ verify_oop(temp_reg);
2091         } else {
2092           __ verify_oop(r-&gt;as_Register());
2093         }
2094       }
2095     }
2096   }
2097 }
2098 
2099 static void gen_special_dispatch(MacroAssembler* masm,
2100                                  const methodHandle&amp; method,
2101                                  const BasicType* sig_bt,
2102                                  const VMRegPair* regs) {
2103   verify_oop_args(masm, method, sig_bt, regs);
2104   vmIntrinsics::ID iid = method-&gt;intrinsic_id();
2105 
2106   // Now write the args into the outgoing interpreter space
2107   bool     has_receiver   = false;
2108   Register receiver_reg   = noreg;
2109   int      member_arg_pos = -1;
2110   Register member_reg     = noreg;
2111   int      ref_kind       = MethodHandles::signature_polymorphic_intrinsic_ref_kind(iid);
2112   if (ref_kind != 0) {
2113     member_arg_pos = method-&gt;size_of_parameters() - 1;  // trailing MemberName argument
2114     member_reg = rbx;  // known to be free at this point
2115     has_receiver = MethodHandles::ref_kind_has_receiver(ref_kind);
2116   } else if (iid == vmIntrinsics::_invokeBasic) {
2117     has_receiver = true;
2118   } else {
2119     fatal(&quot;unexpected intrinsic id %d&quot;, iid);
2120   }
2121 
2122   if (member_reg != noreg) {
2123     // Load the member_arg into register, if necessary.
2124     SharedRuntime::check_member_name_argument_is_last_argument(method, sig_bt, regs);
2125     VMReg r = regs[member_arg_pos].first();
2126     if (r-&gt;is_stack()) {
2127       __ movptr(member_reg, Address(rsp, r-&gt;reg2stack() * VMRegImpl::stack_slot_size + wordSize));
2128     } else {
2129       // no data motion is needed
2130       member_reg = r-&gt;as_Register();
2131     }
2132   }
2133 
2134   if (has_receiver) {
2135     // Make sure the receiver is loaded into a register.
2136     assert(method-&gt;size_of_parameters() &gt; 0, &quot;oob&quot;);
2137     assert(sig_bt[0] == T_OBJECT, &quot;receiver argument must be an object&quot;);
2138     VMReg r = regs[0].first();
2139     assert(r-&gt;is_valid(), &quot;bad receiver arg&quot;);
2140     if (r-&gt;is_stack()) {
2141       // Porting note:  This assumes that compiled calling conventions always
2142       // pass the receiver oop in a register.  If this is not true on some
2143       // platform, pick a temp and load the receiver from stack.
2144       fatal(&quot;receiver always in a register&quot;);
2145       receiver_reg = j_rarg0;  // known to be free at this point
2146       __ movptr(receiver_reg, Address(rsp, r-&gt;reg2stack() * VMRegImpl::stack_slot_size + wordSize));
2147     } else {
2148       // no data motion is needed
2149       receiver_reg = r-&gt;as_Register();
2150     }
2151   }
2152 
2153   // Figure out which address we are really jumping to:
2154   MethodHandles::generate_method_handle_dispatch(masm, iid,
2155                                                  receiver_reg, member_reg, /*for_compiler_entry:*/ true);
2156 }
2157 
2158 // ---------------------------------------------------------------------------
2159 // Generate a native wrapper for a given method.  The method takes arguments
2160 // in the Java compiled code convention, marshals them to the native
2161 // convention (handlizes oops, etc), transitions to native, makes the call,
2162 // returns to java state (possibly blocking), unhandlizes any result and
2163 // returns.
2164 //
2165 // Critical native functions are a shorthand for the use of
2166 // GetPrimtiveArrayCritical and disallow the use of any other JNI
2167 // functions.  The wrapper is expected to unpack the arguments before
2168 // passing them to the callee and perform checks before and after the
2169 // native call to ensure that they GCLocker
2170 // lock_critical/unlock_critical semantics are followed.  Some other
2171 // parts of JNI setup are skipped like the tear down of the JNI handle
2172 // block and the check for pending exceptions it&#39;s impossible for them
2173 // to be thrown.
2174 //
2175 // They are roughly structured like this:
2176 //    if (GCLocker::needs_gc())
2177 //      SharedRuntime::block_for_jni_critical();
2178 //    tranistion to thread_in_native
2179 //    unpack arrray arguments and call native entry point
2180 //    check for safepoint in progress
2181 //    check if any thread suspend flags are set
2182 //      call into JVM and possible unlock the JNI critical
2183 //      if a GC was suppressed while in the critical native.
2184 //    transition back to thread_in_Java
2185 //    return to caller
2186 //
2187 nmethod* SharedRuntime::generate_native_wrapper(MacroAssembler* masm,
2188                                                 const methodHandle&amp; method,
2189                                                 int compile_id,
2190                                                 BasicType* in_sig_bt,
2191                                                 VMRegPair* in_regs,
2192                                                 BasicType ret_type,
2193                                                 address critical_entry) {
2194   if (method-&gt;is_method_handle_intrinsic()) {
2195     vmIntrinsics::ID iid = method-&gt;intrinsic_id();
2196     intptr_t start = (intptr_t)__ pc();
2197     int vep_offset = ((intptr_t)__ pc()) - start;
2198     gen_special_dispatch(masm,
2199                          method,
2200                          in_sig_bt,
2201                          in_regs);
2202     int frame_complete = ((intptr_t)__ pc()) - start;  // not complete, period
2203     __ flush();
2204     int stack_slots = SharedRuntime::out_preserve_stack_slots();  // no out slots at all, actually
2205     return nmethod::new_native_nmethod(method,
2206                                        compile_id,
2207                                        masm-&gt;code(),
2208                                        vep_offset,
2209                                        frame_complete,
2210                                        stack_slots / VMRegImpl::slots_per_word,
2211                                        in_ByteSize(-1),
2212                                        in_ByteSize(-1),
2213                                        (OopMapSet*)NULL);
2214   }
2215   bool is_critical_native = true;
2216   address native_func = critical_entry;
2217   if (native_func == NULL) {
2218     native_func = method-&gt;native_function();
2219     is_critical_native = false;
2220   }
2221   assert(native_func != NULL, &quot;must have function&quot;);
2222 
2223   // An OopMap for lock (and class if static)
2224   OopMapSet *oop_maps = new OopMapSet();
2225   intptr_t start = (intptr_t)__ pc();
2226 
2227   // We have received a description of where all the java arg are located
2228   // on entry to the wrapper. We need to convert these args to where
2229   // the jni function will expect them. To figure out where they go
2230   // we convert the java signature to a C signature by inserting
2231   // the hidden arguments as arg[0] and possibly arg[1] (static method)
2232 
2233   const int total_in_args = method-&gt;size_of_parameters();
2234   int total_c_args = total_in_args;
2235   if (!is_critical_native) {
2236     total_c_args += 1;
2237     if (method-&gt;is_static()) {
2238       total_c_args++;
2239     }
2240   } else {
2241     for (int i = 0; i &lt; total_in_args; i++) {
2242       if (in_sig_bt[i] == T_ARRAY) {
2243         total_c_args++;
2244       }
2245     }
2246   }
2247 
2248   BasicType* out_sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_c_args);
2249   VMRegPair* out_regs   = NEW_RESOURCE_ARRAY(VMRegPair, total_c_args);
2250   BasicType* in_elem_bt = NULL;
2251 
2252   int argc = 0;
2253   if (!is_critical_native) {
2254     out_sig_bt[argc++] = T_ADDRESS;
2255     if (method-&gt;is_static()) {
2256       out_sig_bt[argc++] = T_OBJECT;
2257     }
2258 
2259     for (int i = 0; i &lt; total_in_args ; i++ ) {
2260       out_sig_bt[argc++] = in_sig_bt[i];
2261     }
2262   } else {
2263     in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_in_args);
2264     SignatureStream ss(method-&gt;signature());
2265     for (int i = 0; i &lt; total_in_args ; i++ ) {
2266       if (in_sig_bt[i] == T_ARRAY) {
2267         // Arrays are passed as int, elem* pair
2268         out_sig_bt[argc++] = T_INT;
2269         out_sig_bt[argc++] = T_ADDRESS;
2270         ss.skip_array_prefix(1);  // skip one &#39;[&#39;
2271         assert(ss.is_primitive(), &quot;primitive type expected&quot;);
2272         in_elem_bt[i] = ss.type();
2273       } else {
2274         out_sig_bt[argc++] = in_sig_bt[i];
2275         in_elem_bt[i] = T_VOID;
2276       }
2277       if (in_sig_bt[i] != T_VOID) {
2278         assert(in_sig_bt[i] == ss.type() ||
2279                in_sig_bt[i] == T_ARRAY, &quot;must match&quot;);
2280         ss.next();
2281       }
2282     }
2283   }
2284 
2285   // Now figure out where the args must be stored and how much stack space
2286   // they require.
2287   int out_arg_slots;
2288   out_arg_slots = c_calling_convention(out_sig_bt, out_regs, NULL, total_c_args);
2289 
2290   // Compute framesize for the wrapper.  We need to handlize all oops in
2291   // incoming registers
2292 
2293   // Calculate the total number of stack slots we will need.
2294 
2295   // First count the abi requirement plus all of the outgoing args
2296   int stack_slots = SharedRuntime::out_preserve_stack_slots() + out_arg_slots;
2297 
2298   // Now the space for the inbound oop handle area
2299   int total_save_slots = 6 * VMRegImpl::slots_per_word;  // 6 arguments passed in registers
2300   if (is_critical_native) {
2301     // Critical natives may have to call out so they need a save area
2302     // for register arguments.
2303     int double_slots = 0;
2304     int single_slots = 0;
2305     for ( int i = 0; i &lt; total_in_args; i++) {
2306       if (in_regs[i].first()-&gt;is_Register()) {
2307         const Register reg = in_regs[i].first()-&gt;as_Register();
2308         switch (in_sig_bt[i]) {
2309           case T_BOOLEAN:
2310           case T_BYTE:
2311           case T_SHORT:
2312           case T_CHAR:
2313           case T_INT:  single_slots++; break;
2314           case T_ARRAY:  // specific to LP64 (7145024)
2315           case T_LONG: double_slots++; break;
2316           default:  ShouldNotReachHere();
2317         }
2318       } else if (in_regs[i].first()-&gt;is_XMMRegister()) {
2319         switch (in_sig_bt[i]) {
2320           case T_FLOAT:  single_slots++; break;
2321           case T_DOUBLE: double_slots++; break;
2322           default:  ShouldNotReachHere();
2323         }
2324       } else if (in_regs[i].first()-&gt;is_FloatRegister()) {
2325         ShouldNotReachHere();
2326       }
2327     }
2328     total_save_slots = double_slots * 2 + single_slots;
2329     // align the save area
2330     if (double_slots != 0) {
2331       stack_slots = align_up(stack_slots, 2);
2332     }
2333   }
2334 
2335   int oop_handle_offset = stack_slots;
2336   stack_slots += total_save_slots;
2337 
2338   // Now any space we need for handlizing a klass if static method
2339 
2340   int klass_slot_offset = 0;
2341   int klass_offset = -1;
2342   int lock_slot_offset = 0;
2343   bool is_static = false;
2344 
2345   if (method-&gt;is_static()) {
2346     klass_slot_offset = stack_slots;
2347     stack_slots += VMRegImpl::slots_per_word;
2348     klass_offset = klass_slot_offset * VMRegImpl::stack_slot_size;
2349     is_static = true;
2350   }
2351 
2352   // Plus a lock if needed
2353 
2354   if (method-&gt;is_synchronized()) {
2355     lock_slot_offset = stack_slots;
2356     stack_slots += VMRegImpl::slots_per_word;
2357   }
2358 
2359   // Now a place (+2) to save return values or temp during shuffling
2360   // + 4 for return address (which we own) and saved rbp
2361   stack_slots += 6;
2362 
2363   // Ok The space we have allocated will look like:
2364   //
2365   //
2366   // FP-&gt; |                     |
2367   //      |---------------------|
2368   //      | 2 slots for moves   |
2369   //      |---------------------|
2370   //      | lock box (if sync)  |
2371   //      |---------------------| &lt;- lock_slot_offset
2372   //      | klass (if static)   |
2373   //      |---------------------| &lt;- klass_slot_offset
2374   //      | oopHandle area      |
2375   //      |---------------------| &lt;- oop_handle_offset (6 java arg registers)
2376   //      | outbound memory     |
2377   //      | based arguments     |
2378   //      |                     |
2379   //      |---------------------|
2380   //      |                     |
2381   // SP-&gt; | out_preserved_slots |
2382   //
2383   //
2384 
2385 
2386   // Now compute actual number of stack words we need rounding to make
2387   // stack properly aligned.
2388   stack_slots = align_up(stack_slots, StackAlignmentInSlots);
2389 
2390   int stack_size = stack_slots * VMRegImpl::stack_slot_size;
2391 
2392   // First thing make an ic check to see if we should even be here
2393 
2394   // We are free to use all registers as temps without saving them and
2395   // restoring them except rbp. rbp is the only callee save register
2396   // as far as the interpreter and the compiler(s) are concerned.
2397 
2398 
2399   const Register ic_reg = rax;
2400   const Register receiver = j_rarg0;
2401 
2402   Label hit;
2403   Label exception_pending;
2404 
2405   assert_different_registers(ic_reg, receiver, rscratch1);
2406   __ verify_oop(receiver);
2407   __ load_klass(rscratch1, receiver);
2408   __ cmpq(ic_reg, rscratch1);
2409   __ jcc(Assembler::equal, hit);
2410 
2411   __ jump(RuntimeAddress(SharedRuntime::get_ic_miss_stub()));
2412 
2413   // Verified entry point must be aligned
2414   __ align(8);
2415 
2416   __ bind(hit);
2417 
2418   int vep_offset = ((intptr_t)__ pc()) - start;
2419 
2420   if (VM_Version::supports_fast_class_init_checks() &amp;&amp; method-&gt;needs_clinit_barrier()) {
2421     Label L_skip_barrier;
2422     Register klass = r10;
2423     __ mov_metadata(klass, method-&gt;method_holder()); // InstanceKlass*
2424     __ clinit_barrier(klass, r15_thread, &amp;L_skip_barrier /*L_fast_path*/);
2425 
2426     __ jump(RuntimeAddress(SharedRuntime::get_handle_wrong_method_stub())); // slow path
2427 
2428     __ bind(L_skip_barrier);
2429   }
2430 
2431 #ifdef COMPILER1
2432   // For Object.hashCode, System.identityHashCode try to pull hashCode from object header if available.
2433   if ((InlineObjectHash &amp;&amp; method-&gt;intrinsic_id() == vmIntrinsics::_hashCode) || (method-&gt;intrinsic_id() == vmIntrinsics::_identityHashCode)) {
2434     inline_check_hashcode_from_object_header(masm, method, j_rarg0 /*obj_reg*/, rax /*result*/);
2435   }
2436 #endif // COMPILER1
2437 
2438   // The instruction at the verified entry point must be 5 bytes or longer
2439   // because it can be patched on the fly by make_non_entrant. The stack bang
2440   // instruction fits that requirement.
2441 
2442   // Generate stack overflow check
2443 
2444   if (UseStackBanging) {
2445     __ bang_stack_with_offset((int)JavaThread::stack_shadow_zone_size());
2446   } else {
2447     // need a 5 byte instruction to allow MT safe patching to non-entrant
2448     __ fat_nop();
2449   }
2450 
2451   // Generate a new frame for the wrapper.
2452   __ enter();
2453   // -2 because return address is already present and so is saved rbp
2454   __ subptr(rsp, stack_size - 2*wordSize);
2455 
2456   BarrierSetAssembler* bs = BarrierSet::barrier_set()-&gt;barrier_set_assembler();
2457   bs-&gt;nmethod_entry_barrier(masm);
2458 
2459   // Frame is now completed as far as size and linkage.
2460   int frame_complete = ((intptr_t)__ pc()) - start;
2461 
2462     if (UseRTMLocking) {
2463       // Abort RTM transaction before calling JNI
2464       // because critical section will be large and will be
2465       // aborted anyway. Also nmethod could be deoptimized.
2466       __ xabort(0);
2467     }
2468 
2469 #ifdef ASSERT
2470     {
2471       Label L;
2472       __ mov(rax, rsp);
2473       __ andptr(rax, -16); // must be 16 byte boundary (see amd64 ABI)
2474       __ cmpptr(rax, rsp);
2475       __ jcc(Assembler::equal, L);
2476       __ stop(&quot;improperly aligned stack&quot;);
2477       __ bind(L);
2478     }
2479 #endif /* ASSERT */
2480 
2481 
2482   // We use r14 as the oop handle for the receiver/klass
2483   // It is callee save so it survives the call to native
2484 
2485   const Register oop_handle_reg = r14;
2486 
2487   if (is_critical_native &amp;&amp; !Universe::heap()-&gt;supports_object_pinning()) {
2488     check_needs_gc_for_critical_native(masm, stack_slots, total_c_args, total_in_args,
2489                                        oop_handle_offset, oop_maps, in_regs, in_sig_bt);
2490   }
2491 
2492   //
2493   // We immediately shuffle the arguments so that any vm call we have to
2494   // make from here on out (sync slow path, jvmti, etc.) we will have
2495   // captured the oops from our caller and have a valid oopMap for
2496   // them.
2497 
2498   // -----------------
2499   // The Grand Shuffle
2500 
2501   // The Java calling convention is either equal (linux) or denser (win64) than the
2502   // c calling convention. However the because of the jni_env argument the c calling
2503   // convention always has at least one more (and two for static) arguments than Java.
2504   // Therefore if we move the args from java -&gt; c backwards then we will never have
2505   // a register-&gt;register conflict and we don&#39;t have to build a dependency graph
2506   // and figure out how to break any cycles.
2507   //
2508 
2509   // Record esp-based slot for receiver on stack for non-static methods
2510   int receiver_offset = -1;
2511 
2512   // This is a trick. We double the stack slots so we can claim
2513   // the oops in the caller&#39;s frame. Since we are sure to have
2514   // more args than the caller doubling is enough to make
2515   // sure we can capture all the incoming oop args from the
2516   // caller.
2517   //
2518   OopMap* map = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
2519 
2520   // Mark location of rbp (someday)
2521   // map-&gt;set_callee_saved(VMRegImpl::stack2reg( stack_slots - 2), stack_slots * 2, 0, vmreg(rbp));
2522 
2523   // Use eax, ebx as temporaries during any memory-memory moves we have to do
2524   // All inbound args are referenced based on rbp and all outbound args via rsp.
2525 
2526 
2527 #ifdef ASSERT
2528   bool reg_destroyed[RegisterImpl::number_of_registers];
2529   bool freg_destroyed[XMMRegisterImpl::number_of_registers];
2530   for ( int r = 0 ; r &lt; RegisterImpl::number_of_registers ; r++ ) {
2531     reg_destroyed[r] = false;
2532   }
2533   for ( int f = 0 ; f &lt; XMMRegisterImpl::number_of_registers ; f++ ) {
2534     freg_destroyed[f] = false;
2535   }
2536 
2537 #endif /* ASSERT */
2538 
2539   // This may iterate in two different directions depending on the
2540   // kind of native it is.  The reason is that for regular JNI natives
2541   // the incoming and outgoing registers are offset upwards and for
2542   // critical natives they are offset down.
2543   GrowableArray&lt;int&gt; arg_order(2 * total_in_args);
2544   // Inbound arguments that need to be pinned for critical natives
2545   GrowableArray&lt;int&gt; pinned_args(total_in_args);
2546   // Current stack slot for storing register based array argument
2547   int pinned_slot = oop_handle_offset;
2548 
2549   VMRegPair tmp_vmreg;
2550   tmp_vmreg.set2(rbx-&gt;as_VMReg());
2551 
2552   if (!is_critical_native) {
2553     for (int i = total_in_args - 1, c_arg = total_c_args - 1; i &gt;= 0; i--, c_arg--) {
2554       arg_order.push(i);
2555       arg_order.push(c_arg);
2556     }
2557   } else {
2558     // Compute a valid move order, using tmp_vmreg to break any cycles
2559     ComputeMoveOrder cmo(total_in_args, in_regs, total_c_args, out_regs, in_sig_bt, arg_order, tmp_vmreg);
2560   }
2561 
2562   int temploc = -1;
2563   for (int ai = 0; ai &lt; arg_order.length(); ai += 2) {
2564     int i = arg_order.at(ai);
2565     int c_arg = arg_order.at(ai + 1);
2566     __ block_comment(err_msg(&quot;move %d -&gt; %d&quot;, i, c_arg));
2567     if (c_arg == -1) {
2568       assert(is_critical_native, &quot;should only be required for critical natives&quot;);
2569       // This arg needs to be moved to a temporary
2570       __ mov(tmp_vmreg.first()-&gt;as_Register(), in_regs[i].first()-&gt;as_Register());
2571       in_regs[i] = tmp_vmreg;
2572       temploc = i;
2573       continue;
2574     } else if (i == -1) {
2575       assert(is_critical_native, &quot;should only be required for critical natives&quot;);
2576       // Read from the temporary location
2577       assert(temploc != -1, &quot;must be valid&quot;);
2578       i = temploc;
2579       temploc = -1;
2580     }
2581 #ifdef ASSERT
2582     if (in_regs[i].first()-&gt;is_Register()) {
2583       assert(!reg_destroyed[in_regs[i].first()-&gt;as_Register()-&gt;encoding()], &quot;destroyed reg!&quot;);
2584     } else if (in_regs[i].first()-&gt;is_XMMRegister()) {
2585       assert(!freg_destroyed[in_regs[i].first()-&gt;as_XMMRegister()-&gt;encoding()], &quot;destroyed reg!&quot;);
2586     }
2587     if (out_regs[c_arg].first()-&gt;is_Register()) {
2588       reg_destroyed[out_regs[c_arg].first()-&gt;as_Register()-&gt;encoding()] = true;
2589     } else if (out_regs[c_arg].first()-&gt;is_XMMRegister()) {
2590       freg_destroyed[out_regs[c_arg].first()-&gt;as_XMMRegister()-&gt;encoding()] = true;
2591     }
2592 #endif /* ASSERT */
2593     switch (in_sig_bt[i]) {
2594       case T_ARRAY:
2595         if (is_critical_native) {
2596           // pin before unpack
2597           if (Universe::heap()-&gt;supports_object_pinning()) {
2598             save_args(masm, total_c_args, 0, out_regs);
2599             gen_pin_object(masm, in_regs[i]);
2600             pinned_args.append(i);
2601             restore_args(masm, total_c_args, 0, out_regs);
2602 
2603             // rax has pinned array
2604             VMRegPair result_reg;
2605             result_reg.set_ptr(rax-&gt;as_VMReg());
2606             move_ptr(masm, result_reg, in_regs[i]);
2607             if (!in_regs[i].first()-&gt;is_stack()) {
2608               assert(pinned_slot &lt;= stack_slots, &quot;overflow&quot;);
2609               move_ptr(masm, result_reg, VMRegImpl::stack2reg(pinned_slot));
2610               pinned_slot += VMRegImpl::slots_per_word;
2611             }
2612           }
2613           unpack_array_argument(masm, in_regs[i], in_elem_bt[i], out_regs[c_arg + 1], out_regs[c_arg]);
2614           c_arg++;
2615 #ifdef ASSERT
2616           if (out_regs[c_arg].first()-&gt;is_Register()) {
2617             reg_destroyed[out_regs[c_arg].first()-&gt;as_Register()-&gt;encoding()] = true;
2618           } else if (out_regs[c_arg].first()-&gt;is_XMMRegister()) {
2619             freg_destroyed[out_regs[c_arg].first()-&gt;as_XMMRegister()-&gt;encoding()] = true;
2620           }
2621 #endif
2622           break;
2623         }
2624       case T_VALUETYPE:
2625       case T_OBJECT:
2626         assert(!is_critical_native, &quot;no oop arguments&quot;);
2627         object_move(masm, map, oop_handle_offset, stack_slots, in_regs[i], out_regs[c_arg],
2628                     ((i == 0) &amp;&amp; (!is_static)),
2629                     &amp;receiver_offset);
2630         break;
2631       case T_VOID:
2632         break;
2633 
2634       case T_FLOAT:
2635         float_move(masm, in_regs[i], out_regs[c_arg]);
2636           break;
2637 
2638       case T_DOUBLE:
2639         assert( i + 1 &lt; total_in_args &amp;&amp;
2640                 in_sig_bt[i + 1] == T_VOID &amp;&amp;
2641                 out_sig_bt[c_arg+1] == T_VOID, &quot;bad arg list&quot;);
2642         double_move(masm, in_regs[i], out_regs[c_arg]);
2643         break;
2644 
2645       case T_LONG :
2646         long_move(masm, in_regs[i], out_regs[c_arg]);
2647         break;
2648 
2649       case T_ADDRESS: assert(false, &quot;found T_ADDRESS in java args&quot;);
2650 
2651       default:
2652         move32_64(masm, in_regs[i], out_regs[c_arg]);
2653     }
2654   }
2655 
2656   int c_arg;
2657 
2658   // Pre-load a static method&#39;s oop into r14.  Used both by locking code and
2659   // the normal JNI call code.
2660   if (!is_critical_native) {
2661     // point c_arg at the first arg that is already loaded in case we
2662     // need to spill before we call out
2663     c_arg = total_c_args - total_in_args;
2664 
2665     if (method-&gt;is_static()) {
2666 
2667       //  load oop into a register
2668       __ movoop(oop_handle_reg, JNIHandles::make_local(method-&gt;method_holder()-&gt;java_mirror()));
2669 
2670       // Now handlize the static class mirror it&#39;s known not-null.
2671       __ movptr(Address(rsp, klass_offset), oop_handle_reg);
2672       map-&gt;set_oop(VMRegImpl::stack2reg(klass_slot_offset));
2673 
2674       // Now get the handle
2675       __ lea(oop_handle_reg, Address(rsp, klass_offset));
2676       // store the klass handle as second argument
2677       __ movptr(c_rarg1, oop_handle_reg);
2678       // and protect the arg if we must spill
2679       c_arg--;
2680     }
2681   } else {
2682     // For JNI critical methods we need to save all registers in save_args.
2683     c_arg = 0;
2684   }
2685 
2686   // Change state to native (we save the return address in the thread, since it might not
2687   // be pushed on the stack when we do a a stack traversal). It is enough that the pc()
2688   // points into the right code segment. It does not have to be the correct return pc.
2689   // We use the same pc/oopMap repeatedly when we call out
2690 
2691   intptr_t the_pc = (intptr_t) __ pc();
2692   oop_maps-&gt;add_gc_map(the_pc - start, map);
2693 
2694   __ set_last_Java_frame(rsp, noreg, (address)the_pc);
2695 
2696 
2697   // We have all of the arguments setup at this point. We must not touch any register
2698   // argument registers at this point (what if we save/restore them there are no oop?
2699 
2700   {
2701     SkipIfEqual skip(masm, &amp;DTraceMethodProbes, false);
2702     // protect the args we&#39;ve loaded
2703     save_args(masm, total_c_args, c_arg, out_regs);
2704     __ mov_metadata(c_rarg1, method());
2705     __ call_VM_leaf(
2706       CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_entry),
2707       r15_thread, c_rarg1);
2708     restore_args(masm, total_c_args, c_arg, out_regs);
2709   }
2710 
2711   // RedefineClasses() tracing support for obsolete method entry
2712   if (log_is_enabled(Trace, redefine, class, obsolete)) {
2713     // protect the args we&#39;ve loaded
2714     save_args(masm, total_c_args, c_arg, out_regs);
2715     __ mov_metadata(c_rarg1, method());
2716     __ call_VM_leaf(
2717       CAST_FROM_FN_PTR(address, SharedRuntime::rc_trace_method_entry),
2718       r15_thread, c_rarg1);
2719     restore_args(masm, total_c_args, c_arg, out_regs);
2720   }
2721 
2722   // Lock a synchronized method
2723 
2724   // Register definitions used by locking and unlocking
2725 
2726   const Register swap_reg = rax;  // Must use rax for cmpxchg instruction
2727   const Register obj_reg  = rbx;  // Will contain the oop
2728   const Register lock_reg = r13;  // Address of compiler lock object (BasicLock)
2729   const Register old_hdr  = r13;  // value of old header at unlock time
2730 
2731   Label slow_path_lock;
2732   Label lock_done;
2733 
2734   if (method-&gt;is_synchronized()) {
2735     assert(!is_critical_native, &quot;unhandled&quot;);
2736 
2737 
2738     const int mark_word_offset = BasicLock::displaced_header_offset_in_bytes();
2739 
2740     // Get the handle (the 2nd argument)
2741     __ mov(oop_handle_reg, c_rarg1);
2742 
2743     // Get address of the box
2744 
2745     __ lea(lock_reg, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));
2746 
2747     // Load the oop from the handle
2748     __ movptr(obj_reg, Address(oop_handle_reg, 0));
2749 
2750     __ resolve(IS_NOT_NULL, obj_reg);
2751     if (UseBiasedLocking) {
2752       __ biased_locking_enter(lock_reg, obj_reg, swap_reg, rscratch1, false, lock_done, &amp;slow_path_lock);
2753     }
2754 
2755     // Load immediate 1 into swap_reg %rax
2756     __ movl(swap_reg, 1);
2757 
2758     // Load (object-&gt;mark() | 1) into swap_reg %rax
2759     __ orptr(swap_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
2760     if (EnableValhalla &amp;&amp; !UseBiasedLocking) {
2761       // For slow path is_always_locked, using biased, which is never natural for !UseBiasLocking
2762       __ andptr(swap_reg, ~((int) markWord::biased_lock_bit_in_place));
2763     }
2764 
2765     // Save (object-&gt;mark() | 1) into BasicLock&#39;s displaced header
2766     __ movptr(Address(lock_reg, mark_word_offset), swap_reg);
2767 
2768     // src -&gt; dest iff dest == rax else rax &lt;- dest
2769     __ lock();
2770     __ cmpxchgptr(lock_reg, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
2771     __ jcc(Assembler::equal, lock_done);
2772 
2773     // Hmm should this move to the slow path code area???
2774 
2775     // Test if the oopMark is an obvious stack pointer, i.e.,
2776     //  1) (mark &amp; 3) == 0, and
2777     //  2) rsp &lt;= mark &lt; mark + os::pagesize()
2778     // These 3 tests can be done by evaluating the following
2779     // expression: ((mark - rsp) &amp; (3 - os::vm_page_size())),
2780     // assuming both stack pointer and pagesize have their
2781     // least significant 2 bits clear.
2782     // NOTE: the oopMark is in swap_reg %rax as the result of cmpxchg
2783 
2784     __ subptr(swap_reg, rsp);
2785     __ andptr(swap_reg, 3 - os::vm_page_size());
2786 
2787     // Save the test result, for recursive case, the result is zero
2788     __ movptr(Address(lock_reg, mark_word_offset), swap_reg);
2789     __ jcc(Assembler::notEqual, slow_path_lock);
2790 
2791     // Slow path will re-enter here
2792 
2793     __ bind(lock_done);
2794   }
2795 
2796 
2797   // Finally just about ready to make the JNI call
2798 
2799 
2800   // get JNIEnv* which is first argument to native
2801   if (!is_critical_native) {
2802     __ lea(c_rarg0, Address(r15_thread, in_bytes(JavaThread::jni_environment_offset())));
2803   }
2804 
2805   // Now set thread in native
2806   __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native);
2807 
2808   __ call(RuntimeAddress(native_func));
2809 
2810   // Verify or restore cpu control state after JNI call
2811   __ restore_cpu_control_state_after_jni();
2812 
2813   // Unpack native results.
2814   switch (ret_type) {
2815   case T_BOOLEAN: __ c2bool(rax);            break;
2816   case T_CHAR   : __ movzwl(rax, rax);      break;
2817   case T_BYTE   : __ sign_extend_byte (rax); break;
2818   case T_SHORT  : __ sign_extend_short(rax); break;
2819   case T_INT    : /* nothing to do */        break;
2820   case T_DOUBLE :
2821   case T_FLOAT  :
2822     // Result is in xmm0 we&#39;ll save as needed
2823     break;
2824   case T_ARRAY:                 // Really a handle
2825   case T_VALUETYPE:             // Really a handle
2826   case T_OBJECT:                // Really a handle
2827       break; // can&#39;t de-handlize until after safepoint check
2828   case T_VOID: break;
2829   case T_LONG: break;
2830   default       : ShouldNotReachHere();
2831   }
2832 
2833   // unpin pinned arguments
2834   pinned_slot = oop_handle_offset;
2835   if (pinned_args.length() &gt; 0) {
2836     // save return value that may be overwritten otherwise.
2837     save_native_result(masm, ret_type, stack_slots);
2838     for (int index = 0; index &lt; pinned_args.length(); index ++) {
2839       int i = pinned_args.at(index);
2840       assert(pinned_slot &lt;= stack_slots, &quot;overflow&quot;);
2841       if (!in_regs[i].first()-&gt;is_stack()) {
2842         int offset = pinned_slot * VMRegImpl::stack_slot_size;
2843         __ movq(in_regs[i].first()-&gt;as_Register(), Address(rsp, offset));
2844         pinned_slot += VMRegImpl::slots_per_word;
2845       }
2846       gen_unpin_object(masm, in_regs[i]);
2847     }
2848     restore_native_result(masm, ret_type, stack_slots);
2849   }
2850 
2851   // Switch thread to &quot;native transition&quot; state before reading the synchronization state.
2852   // This additional state is necessary because reading and testing the synchronization
2853   // state is not atomic w.r.t. GC, as this scenario demonstrates:
2854   //     Java thread A, in _thread_in_native state, loads _not_synchronized and is preempted.
2855   //     VM thread changes sync state to synchronizing and suspends threads for GC.
2856   //     Thread A is resumed to finish this native method, but doesn&#39;t block here since it
2857   //     didn&#39;t see any synchronization is progress, and escapes.
2858   __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_native_trans);
2859 
2860   // Force this write out before the read below
2861   __ membar(Assembler::Membar_mask_bits(
2862               Assembler::LoadLoad | Assembler::LoadStore |
2863               Assembler::StoreLoad | Assembler::StoreStore));
2864 
2865   Label after_transition;
2866 
2867   // check for safepoint operation in progress and/or pending suspend requests
2868   {
2869     Label Continue;
2870     Label slow_path;
2871 
2872     __ safepoint_poll(slow_path, r15_thread, rscratch1);
2873 
2874     __ cmpl(Address(r15_thread, JavaThread::suspend_flags_offset()), 0);
2875     __ jcc(Assembler::equal, Continue);
2876     __ bind(slow_path);
2877 
2878     // Don&#39;t use call_VM as it will see a possible pending exception and forward it
2879     // and never return here preventing us from clearing _last_native_pc down below.
2880     // Also can&#39;t use call_VM_leaf either as it will check to see if rsi &amp; rdi are
2881     // preserved and correspond to the bcp/locals pointers. So we do a runtime call
2882     // by hand.
2883     //
2884     __ vzeroupper();
2885     save_native_result(masm, ret_type, stack_slots);
2886     __ mov(c_rarg0, r15_thread);
2887     __ mov(r12, rsp); // remember sp
2888     __ subptr(rsp, frame::arg_reg_save_area_bytes); // windows
2889     __ andptr(rsp, -16); // align stack as required by ABI
2890     if (!is_critical_native) {
2891       __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans)));
2892     } else {
2893       __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans_and_transition)));
2894     }
2895     __ mov(rsp, r12); // restore sp
2896     __ reinit_heapbase();
2897     // Restore any method result value
2898     restore_native_result(masm, ret_type, stack_slots);
2899 
2900     if (is_critical_native) {
2901       // The call above performed the transition to thread_in_Java so
2902       // skip the transition logic below.
2903       __ jmpb(after_transition);
2904     }
2905 
2906     __ bind(Continue);
2907   }
2908 
2909   // change thread state
2910   __ movl(Address(r15_thread, JavaThread::thread_state_offset()), _thread_in_Java);
2911   __ bind(after_transition);
2912 
2913   Label reguard;
2914   Label reguard_done;
2915   __ cmpl(Address(r15_thread, JavaThread::stack_guard_state_offset()), JavaThread::stack_guard_yellow_reserved_disabled);
2916   __ jcc(Assembler::equal, reguard);
2917   __ bind(reguard_done);
2918 
2919   // native result if any is live
2920 
2921   // Unlock
2922   Label unlock_done;
2923   Label slow_path_unlock;
2924   if (method-&gt;is_synchronized()) {
2925 
2926     // Get locked oop from the handle we passed to jni
2927     __ movptr(obj_reg, Address(oop_handle_reg, 0));
2928     __ resolve(IS_NOT_NULL, obj_reg);
2929 
2930     Label done;
2931 
2932     if (UseBiasedLocking) {
2933       __ biased_locking_exit(obj_reg, old_hdr, done);
2934     }
2935 
2936     // Simple recursive lock?
2937 
2938     __ cmpptr(Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size), (int32_t)NULL_WORD);
2939     __ jcc(Assembler::equal, done);
2940 
2941     // Must save rax if if it is live now because cmpxchg must use it
2942     if (ret_type != T_FLOAT &amp;&amp; ret_type != T_DOUBLE &amp;&amp; ret_type != T_VOID) {
2943       save_native_result(masm, ret_type, stack_slots);
2944     }
2945 
2946 
2947     // get address of the stack lock
2948     __ lea(rax, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));
2949     //  get old displaced header
2950     __ movptr(old_hdr, Address(rax, 0));
2951 
2952     // Atomic swap old header if oop still contains the stack lock
2953     __ lock();
2954     __ cmpxchgptr(old_hdr, Address(obj_reg, oopDesc::mark_offset_in_bytes()));
2955     __ jcc(Assembler::notEqual, slow_path_unlock);
2956 
2957     // slow path re-enters here
2958     __ bind(unlock_done);
2959     if (ret_type != T_FLOAT &amp;&amp; ret_type != T_DOUBLE &amp;&amp; ret_type != T_VOID) {
2960       restore_native_result(masm, ret_type, stack_slots);
2961     }
2962 
2963     __ bind(done);
2964 
2965   }
2966   {
2967     SkipIfEqual skip(masm, &amp;DTraceMethodProbes, false);
2968     save_native_result(masm, ret_type, stack_slots);
2969     __ mov_metadata(c_rarg1, method());
2970     __ call_VM_leaf(
2971          CAST_FROM_FN_PTR(address, SharedRuntime::dtrace_method_exit),
2972          r15_thread, c_rarg1);
2973     restore_native_result(masm, ret_type, stack_slots);
2974   }
2975 
2976   __ reset_last_Java_frame(false);
2977 
2978   // Unbox oop result, e.g. JNIHandles::resolve value.
2979   if (is_reference_type(ret_type)) {
2980     __ resolve_jobject(rax /* value */,
2981                        r15_thread /* thread */,
2982                        rcx /* tmp */);
2983   }
2984 
2985   if (CheckJNICalls) {
2986     // clear_pending_jni_exception_check
2987     __ movptr(Address(r15_thread, JavaThread::pending_jni_exception_check_fn_offset()), NULL_WORD);
2988   }
2989 
2990   if (!is_critical_native) {
2991     // reset handle block
2992     __ movptr(rcx, Address(r15_thread, JavaThread::active_handles_offset()));
2993     __ movl(Address(rcx, JNIHandleBlock::top_offset_in_bytes()), (int32_t)NULL_WORD);
2994   }
2995 
2996   // pop our frame
2997 
2998   __ leave();
2999 
3000   if (!is_critical_native) {
3001     // Any exception pending?
3002     __ cmpptr(Address(r15_thread, in_bytes(Thread::pending_exception_offset())), (int32_t)NULL_WORD);
3003     __ jcc(Assembler::notEqual, exception_pending);
3004   }
3005 
3006   // Return
3007 
3008   __ ret(0);
3009 
3010   // Unexpected paths are out of line and go here
3011 
3012   if (!is_critical_native) {
3013     // forward the exception
3014     __ bind(exception_pending);
3015 
3016     // and forward the exception
3017     __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
3018   }
3019 
3020   // Slow path locking &amp; unlocking
3021   if (method-&gt;is_synchronized()) {
3022 
3023     // BEGIN Slow path lock
3024     __ bind(slow_path_lock);
3025 
3026     // has last_Java_frame setup. No exceptions so do vanilla call not call_VM
3027     // args are (oop obj, BasicLock* lock, JavaThread* thread)
3028 
3029     // protect the args we&#39;ve loaded
3030     save_args(masm, total_c_args, c_arg, out_regs);
3031 
3032     __ mov(c_rarg0, obj_reg);
3033     __ mov(c_rarg1, lock_reg);
3034     __ mov(c_rarg2, r15_thread);
3035 
3036     // Not a leaf but we have last_Java_frame setup as we want
3037     __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_locking_C), 3);
3038     restore_args(masm, total_c_args, c_arg, out_regs);
3039 
3040 #ifdef ASSERT
3041     { Label L;
3042     __ cmpptr(Address(r15_thread, in_bytes(Thread::pending_exception_offset())), (int32_t)NULL_WORD);
3043     __ jcc(Assembler::equal, L);
3044     __ stop(&quot;no pending exception allowed on exit from monitorenter&quot;);
3045     __ bind(L);
3046     }
3047 #endif
3048     __ jmp(lock_done);
3049 
3050     // END Slow path lock
3051 
3052     // BEGIN Slow path unlock
3053     __ bind(slow_path_unlock);
3054 
3055     // If we haven&#39;t already saved the native result we must save it now as xmm registers
3056     // are still exposed.
3057     __ vzeroupper();
3058     if (ret_type == T_FLOAT || ret_type == T_DOUBLE ) {
3059       save_native_result(masm, ret_type, stack_slots);
3060     }
3061 
3062     __ lea(c_rarg1, Address(rsp, lock_slot_offset * VMRegImpl::stack_slot_size));
3063 
3064     __ mov(c_rarg0, obj_reg);
3065     __ mov(c_rarg2, r15_thread);
3066     __ mov(r12, rsp); // remember sp
3067     __ subptr(rsp, frame::arg_reg_save_area_bytes); // windows
3068     __ andptr(rsp, -16); // align stack as required by ABI
3069 
3070     // Save pending exception around call to VM (which contains an EXCEPTION_MARK)
3071     // NOTE that obj_reg == rbx currently
3072     __ movptr(rbx, Address(r15_thread, in_bytes(Thread::pending_exception_offset())));
3073     __ movptr(Address(r15_thread, in_bytes(Thread::pending_exception_offset())), (int32_t)NULL_WORD);
3074 
3075     // args are (oop obj, BasicLock* lock, JavaThread* thread)
3076     __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C)));
3077     __ mov(rsp, r12); // restore sp
3078     __ reinit_heapbase();
3079 #ifdef ASSERT
3080     {
3081       Label L;
3082       __ cmpptr(Address(r15_thread, in_bytes(Thread::pending_exception_offset())), (int)NULL_WORD);
3083       __ jcc(Assembler::equal, L);
3084       __ stop(&quot;no pending exception allowed on exit complete_monitor_unlocking_C&quot;);
3085       __ bind(L);
3086     }
3087 #endif /* ASSERT */
3088 
3089     __ movptr(Address(r15_thread, in_bytes(Thread::pending_exception_offset())), rbx);
3090 
3091     if (ret_type == T_FLOAT || ret_type == T_DOUBLE ) {
3092       restore_native_result(masm, ret_type, stack_slots);
3093     }
3094     __ jmp(unlock_done);
3095 
3096     // END Slow path unlock
3097 
3098   } // synchronized
3099 
3100   // SLOW PATH Reguard the stack if needed
3101 
3102   __ bind(reguard);
3103   __ vzeroupper();
3104   save_native_result(masm, ret_type, stack_slots);
3105   __ mov(r12, rsp); // remember sp
3106   __ subptr(rsp, frame::arg_reg_save_area_bytes); // windows
3107   __ andptr(rsp, -16); // align stack as required by ABI
3108   __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages)));
3109   __ mov(rsp, r12); // restore sp
3110   __ reinit_heapbase();
3111   restore_native_result(masm, ret_type, stack_slots);
3112   // and continue
3113   __ jmp(reguard_done);
3114 
3115 
3116 
3117   __ flush();
3118 
3119   nmethod *nm = nmethod::new_native_nmethod(method,
3120                                             compile_id,
3121                                             masm-&gt;code(),
3122                                             vep_offset,
3123                                             frame_complete,
3124                                             stack_slots / VMRegImpl::slots_per_word,
3125                                             (is_static ? in_ByteSize(klass_offset) : in_ByteSize(receiver_offset)),
3126                                             in_ByteSize(lock_slot_offset*VMRegImpl::stack_slot_size),
3127                                             oop_maps);
3128 
3129   if (is_critical_native) {
3130     nm-&gt;set_lazy_critical_native(true);
3131   }
3132 
3133   return nm;
3134 
3135 }
3136 
3137 // this function returns the adjust size (in number of words) to a c2i adapter
3138 // activation for use during deoptimization
3139 int Deoptimization::last_frame_adjust(int callee_parameters, int callee_locals ) {
3140   return (callee_locals - callee_parameters) * Interpreter::stackElementWords;
3141 }
3142 
3143 
3144 uint SharedRuntime::out_preserve_stack_slots() {
3145   return 0;
3146 }
3147 
3148 //------------------------------generate_deopt_blob----------------------------
3149 void SharedRuntime::generate_deopt_blob() {
3150   // Allocate space for the code
3151   ResourceMark rm;
3152   // Setup code generation tools
3153   int pad = 0;
3154 #if INCLUDE_JVMCI
3155   if (EnableJVMCI || UseAOT) {
3156     pad += 512; // Increase the buffer size when compiling for JVMCI
3157   }
3158 #endif
3159   CodeBuffer buffer(&quot;deopt_blob&quot;, 2048+pad, 1024);
3160   MacroAssembler* masm = new MacroAssembler(&amp;buffer);
3161   int frame_size_in_words;
3162   OopMap* map = NULL;
3163   OopMapSet *oop_maps = new OopMapSet();
3164 
3165   // -------------
3166   // This code enters when returning to a de-optimized nmethod.  A return
3167   // address has been pushed on the the stack, and return values are in
3168   // registers.
3169   // If we are doing a normal deopt then we were called from the patched
3170   // nmethod from the point we returned to the nmethod. So the return
3171   // address on the stack is wrong by NativeCall::instruction_size
3172   // We will adjust the value so it looks like we have the original return
3173   // address on the stack (like when we eagerly deoptimized).
3174   // In the case of an exception pending when deoptimizing, we enter
3175   // with a return address on the stack that points after the call we patched
3176   // into the exception handler. We have the following register state from,
3177   // e.g., the forward exception stub (see stubGenerator_x86_64.cpp).
3178   //    rax: exception oop
3179   //    rbx: exception handler
3180   //    rdx: throwing pc
3181   // So in this case we simply jam rdx into the useless return address and
3182   // the stack looks just like we want.
3183   //
3184   // At this point we need to de-opt.  We save the argument return
3185   // registers.  We call the first C routine, fetch_unroll_info().  This
3186   // routine captures the return values and returns a structure which
3187   // describes the current frame size and the sizes of all replacement frames.
3188   // The current frame is compiled code and may contain many inlined
3189   // functions, each with their own JVM state.  We pop the current frame, then
3190   // push all the new frames.  Then we call the C routine unpack_frames() to
3191   // populate these frames.  Finally unpack_frames() returns us the new target
3192   // address.  Notice that callee-save registers are BLOWN here; they have
3193   // already been captured in the vframeArray at the time the return PC was
3194   // patched.
3195   address start = __ pc();
3196   Label cont;
3197 
3198   // Prolog for non exception case!
3199 
3200   // Save everything in sight.
3201   map = RegisterSaver::save_live_registers(masm, 0, &amp;frame_size_in_words);
3202 
3203   // Normal deoptimization.  Save exec mode for unpack_frames.
3204   __ movl(r14, Deoptimization::Unpack_deopt); // callee-saved
3205   __ jmp(cont);
3206 
3207   int reexecute_offset = __ pc() - start;
3208 #if INCLUDE_JVMCI &amp;&amp; !defined(COMPILER1)
3209   if (EnableJVMCI &amp;&amp; UseJVMCICompiler) {
3210     // JVMCI does not use this kind of deoptimization
3211     __ should_not_reach_here();
3212   }
3213 #endif
3214 
3215   // Reexecute case
3216   // return address is the pc describes what bci to do re-execute at
3217 
3218   // No need to update map as each call to save_live_registers will produce identical oopmap
3219   (void) RegisterSaver::save_live_registers(masm, 0, &amp;frame_size_in_words);
3220 
3221   __ movl(r14, Deoptimization::Unpack_reexecute); // callee-saved
3222   __ jmp(cont);
3223 
3224 #if INCLUDE_JVMCI
3225   Label after_fetch_unroll_info_call;
3226   int implicit_exception_uncommon_trap_offset = 0;
3227   int uncommon_trap_offset = 0;
3228 
3229   if (EnableJVMCI || UseAOT) {
3230     implicit_exception_uncommon_trap_offset = __ pc() - start;
3231 
3232     __ pushptr(Address(r15_thread, in_bytes(JavaThread::jvmci_implicit_exception_pc_offset())));
3233     __ movptr(Address(r15_thread, in_bytes(JavaThread::jvmci_implicit_exception_pc_offset())), (int32_t)NULL_WORD);
3234 
3235     uncommon_trap_offset = __ pc() - start;
3236 
3237     // Save everything in sight.
3238     RegisterSaver::save_live_registers(masm, 0, &amp;frame_size_in_words);
3239     // fetch_unroll_info needs to call last_java_frame()
3240     __ set_last_Java_frame(noreg, noreg, NULL);
3241 
3242     __ movl(c_rarg1, Address(r15_thread, in_bytes(JavaThread::pending_deoptimization_offset())));
3243     __ movl(Address(r15_thread, in_bytes(JavaThread::pending_deoptimization_offset())), -1);
3244 
3245     __ movl(r14, (int32_t)Deoptimization::Unpack_reexecute);
3246     __ mov(c_rarg0, r15_thread);
3247     __ movl(c_rarg2, r14); // exec mode
3248     __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::uncommon_trap)));
3249     oop_maps-&gt;add_gc_map( __ pc()-start, map-&gt;deep_copy());
3250 
3251     __ reset_last_Java_frame(false);
3252 
3253     __ jmp(after_fetch_unroll_info_call);
3254   } // EnableJVMCI
3255 #endif // INCLUDE_JVMCI
3256 
3257   int exception_offset = __ pc() - start;
3258 
3259   // Prolog for exception case
3260 
3261   // all registers are dead at this entry point, except for rax, and
3262   // rdx which contain the exception oop and exception pc
3263   // respectively.  Set them in TLS and fall thru to the
3264   // unpack_with_exception_in_tls entry point.
3265 
3266   __ movptr(Address(r15_thread, JavaThread::exception_pc_offset()), rdx);
3267   __ movptr(Address(r15_thread, JavaThread::exception_oop_offset()), rax);
3268 
3269   int exception_in_tls_offset = __ pc() - start;
3270 
3271   // new implementation because exception oop is now passed in JavaThread
3272 
3273   // Prolog for exception case
3274   // All registers must be preserved because they might be used by LinearScan
3275   // Exceptiop oop and throwing PC are passed in JavaThread
3276   // tos: stack at point of call to method that threw the exception (i.e. only
3277   // args are on the stack, no return address)
3278 
3279   // make room on stack for the return address
3280   // It will be patched later with the throwing pc. The correct value is not
3281   // available now because loading it from memory would destroy registers.
3282   __ push(0);
3283 
3284   // Save everything in sight.
3285   map = RegisterSaver::save_live_registers(masm, 0, &amp;frame_size_in_words);
3286 
3287   // Now it is safe to overwrite any register
3288 
3289   // Deopt during an exception.  Save exec mode for unpack_frames.
3290   __ movl(r14, Deoptimization::Unpack_exception); // callee-saved
3291 
3292   // load throwing pc from JavaThread and patch it as the return address
3293   // of the current frame. Then clear the field in JavaThread
3294 
3295   __ movptr(rdx, Address(r15_thread, JavaThread::exception_pc_offset()));
3296   __ movptr(Address(rbp, wordSize), rdx);
3297   __ movptr(Address(r15_thread, JavaThread::exception_pc_offset()), (int32_t)NULL_WORD);
3298 
3299 #ifdef ASSERT
3300   // verify that there is really an exception oop in JavaThread
3301   __ movptr(rax, Address(r15_thread, JavaThread::exception_oop_offset()));
3302   __ verify_oop(rax);
3303 
3304   // verify that there is no pending exception
3305   Label no_pending_exception;
3306   __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));
3307   __ testptr(rax, rax);
3308   __ jcc(Assembler::zero, no_pending_exception);
3309   __ stop(&quot;must not have pending exception here&quot;);
3310   __ bind(no_pending_exception);
3311 #endif
3312 
3313   __ bind(cont);
3314 
3315   // Call C code.  Need thread and this frame, but NOT official VM entry
3316   // crud.  We cannot block on this call, no GC can happen.
3317   //
3318   // UnrollBlock* fetch_unroll_info(JavaThread* thread)
3319 
3320   // fetch_unroll_info needs to call last_java_frame().
3321 
3322   __ set_last_Java_frame(noreg, noreg, NULL);
3323 #ifdef ASSERT
3324   { Label L;
3325     __ cmpptr(Address(r15_thread,
3326                     JavaThread::last_Java_fp_offset()),
3327             (int32_t)0);
3328     __ jcc(Assembler::equal, L);
3329     __ stop(&quot;SharedRuntime::generate_deopt_blob: last_Java_fp not cleared&quot;);
3330     __ bind(L);
3331   }
3332 #endif // ASSERT
3333   __ mov(c_rarg0, r15_thread);
3334   __ movl(c_rarg1, r14); // exec_mode
3335   __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::fetch_unroll_info)));
3336 
3337   // Need to have an oopmap that tells fetch_unroll_info where to
3338   // find any register it might need.
3339   oop_maps-&gt;add_gc_map(__ pc() - start, map);
3340 
3341   __ reset_last_Java_frame(false);
3342 
3343 #if INCLUDE_JVMCI
3344   if (EnableJVMCI || UseAOT) {
3345     __ bind(after_fetch_unroll_info_call);
3346   }
3347 #endif
3348 
3349   // Load UnrollBlock* into rdi
3350   __ mov(rdi, rax);
3351 
3352   __ movl(r14, Address(rdi, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()));
3353    Label noException;
3354   __ cmpl(r14, Deoptimization::Unpack_exception);   // Was exception pending?
3355   __ jcc(Assembler::notEqual, noException);
3356   __ movptr(rax, Address(r15_thread, JavaThread::exception_oop_offset()));
3357   // QQQ this is useless it was NULL above
3358   __ movptr(rdx, Address(r15_thread, JavaThread::exception_pc_offset()));
3359   __ movptr(Address(r15_thread, JavaThread::exception_oop_offset()), (int32_t)NULL_WORD);
3360   __ movptr(Address(r15_thread, JavaThread::exception_pc_offset()), (int32_t)NULL_WORD);
3361 
3362   __ verify_oop(rax);
3363 
3364   // Overwrite the result registers with the exception results.
3365   __ movptr(Address(rsp, RegisterSaver::rax_offset_in_bytes()), rax);
3366   // I think this is useless
3367   __ movptr(Address(rsp, RegisterSaver::rdx_offset_in_bytes()), rdx);
3368 
3369   __ bind(noException);
3370 
3371   // Only register save data is on the stack.
3372   // Now restore the result registers.  Everything else is either dead
3373   // or captured in the vframeArray.
3374   RegisterSaver::restore_result_registers(masm);
3375 
3376   // All of the register save area has been popped of the stack. Only the
3377   // return address remains.
3378 
3379   // Pop all the frames we must move/replace.
3380   //
3381   // Frame picture (youngest to oldest)
3382   // 1: self-frame (no frame link)
3383   // 2: deopting frame  (no frame link)
3384   // 3: caller of deopting frame (could be compiled/interpreted).
3385   //
3386   // Note: by leaving the return address of self-frame on the stack
3387   // and using the size of frame 2 to adjust the stack
3388   // when we are done the return to frame 3 will still be on the stack.
3389 
3390   // Pop deoptimized frame
3391   __ movl(rcx, Address(rdi, Deoptimization::UnrollBlock::size_of_deoptimized_frame_offset_in_bytes()));
3392   __ addptr(rsp, rcx);
3393 
3394   // rsp should be pointing at the return address to the caller (3)
3395 
3396   // Pick up the initial fp we should save
3397   // restore rbp before stack bang because if stack overflow is thrown it needs to be pushed (and preserved)
3398   __ movptr(rbp, Address(rdi, Deoptimization::UnrollBlock::initial_info_offset_in_bytes()));
3399 
3400 #ifdef ASSERT
3401   // Compilers generate code that bang the stack by as much as the
3402   // interpreter would need. So this stack banging should never
3403   // trigger a fault. Verify that it does not on non product builds.
3404   if (UseStackBanging) {
3405     __ movl(rbx, Address(rdi, Deoptimization::UnrollBlock::total_frame_sizes_offset_in_bytes()));
3406     __ bang_stack_size(rbx, rcx);
3407   }
3408 #endif
3409 
3410   // Load address of array of frame pcs into rcx
3411   __ movptr(rcx, Address(rdi, Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes()));
3412 
3413   // Trash the old pc
3414   __ addptr(rsp, wordSize);
3415 
3416   // Load address of array of frame sizes into rsi
3417   __ movptr(rsi, Address(rdi, Deoptimization::UnrollBlock::frame_sizes_offset_in_bytes()));
3418 
3419   // Load counter into rdx
3420   __ movl(rdx, Address(rdi, Deoptimization::UnrollBlock::number_of_frames_offset_in_bytes()));
3421 
3422   // Now adjust the caller&#39;s stack to make up for the extra locals
3423   // but record the original sp so that we can save it in the skeletal interpreter
3424   // frame and the stack walking of interpreter_sender will get the unextended sp
3425   // value and not the &quot;real&quot; sp value.
3426 
3427   const Register sender_sp = r8;
3428 
3429   __ mov(sender_sp, rsp);
3430   __ movl(rbx, Address(rdi,
3431                        Deoptimization::UnrollBlock::
3432                        caller_adjustment_offset_in_bytes()));
3433   __ subptr(rsp, rbx);
3434 
3435   // Push interpreter frames in a loop
3436   Label loop;
3437   __ bind(loop);
3438   __ movptr(rbx, Address(rsi, 0));      // Load frame size
3439   __ subptr(rbx, 2*wordSize);           // We&#39;ll push pc and ebp by hand
3440   __ pushptr(Address(rcx, 0));          // Save return address
3441   __ enter();                           // Save old &amp; set new ebp
3442   __ subptr(rsp, rbx);                  // Prolog
3443   // This value is corrected by layout_activation_impl
3444   __ movptr(Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize), (int32_t)NULL_WORD );
3445   __ movptr(Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize), sender_sp); // Make it walkable
3446   __ mov(sender_sp, rsp);               // Pass sender_sp to next frame
3447   __ addptr(rsi, wordSize);             // Bump array pointer (sizes)
3448   __ addptr(rcx, wordSize);             // Bump array pointer (pcs)
3449   __ decrementl(rdx);                   // Decrement counter
3450   __ jcc(Assembler::notZero, loop);
3451   __ pushptr(Address(rcx, 0));          // Save final return address
3452 
3453   // Re-push self-frame
3454   __ enter();                           // Save old &amp; set new ebp
3455 
3456   // Allocate a full sized register save area.
3457   // Return address and rbp are in place, so we allocate two less words.
3458   __ subptr(rsp, (frame_size_in_words - 2) * wordSize);
3459 
3460   // Restore frame locals after moving the frame
3461   __ movdbl(Address(rsp, RegisterSaver::xmm0_offset_in_bytes()), xmm0);
3462   __ movptr(Address(rsp, RegisterSaver::rax_offset_in_bytes()), rax);
3463 
3464   // Call C code.  Need thread but NOT official VM entry
3465   // crud.  We cannot block on this call, no GC can happen.  Call should
3466   // restore return values to their stack-slots with the new SP.
3467   //
3468   // void Deoptimization::unpack_frames(JavaThread* thread, int exec_mode)
3469 
3470   // Use rbp because the frames look interpreted now
3471   // Save &quot;the_pc&quot; since it cannot easily be retrieved using the last_java_SP after we aligned SP.
3472   // Don&#39;t need the precise return PC here, just precise enough to point into this code blob.
3473   address the_pc = __ pc();
3474   __ set_last_Java_frame(noreg, rbp, the_pc);
3475 
3476   __ andptr(rsp, -(StackAlignmentInBytes));  // Fix stack alignment as required by ABI
3477   __ mov(c_rarg0, r15_thread);
3478   __ movl(c_rarg1, r14); // second arg: exec_mode
3479   __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames)));
3480   // Revert SP alignment after call since we&#39;re going to do some SP relative addressing below
3481   __ movptr(rsp, Address(r15_thread, JavaThread::last_Java_sp_offset()));
3482 
3483   // Set an oopmap for the call site
3484   // Use the same PC we used for the last java frame
3485   oop_maps-&gt;add_gc_map(the_pc - start,
3486                        new OopMap( frame_size_in_words, 0 ));
3487 
3488   // Clear fp AND pc
3489   __ reset_last_Java_frame(true);
3490 
3491   // Collect return values
3492   __ movdbl(xmm0, Address(rsp, RegisterSaver::xmm0_offset_in_bytes()));
3493   __ movptr(rax, Address(rsp, RegisterSaver::rax_offset_in_bytes()));
3494   // I think this is useless (throwing pc?)
3495   __ movptr(rdx, Address(rsp, RegisterSaver::rdx_offset_in_bytes()));
3496 
3497   // Pop self-frame.
3498   __ leave();                           // Epilog
3499 
3500   // Jump to interpreter
3501   __ ret(0);
3502 
3503   // Make sure all code is generated
3504   masm-&gt;flush();
3505 
3506   _deopt_blob = DeoptimizationBlob::create(&amp;buffer, oop_maps, 0, exception_offset, reexecute_offset, frame_size_in_words);
3507   _deopt_blob-&gt;set_unpack_with_exception_in_tls_offset(exception_in_tls_offset);
3508 #if INCLUDE_JVMCI
3509   if (EnableJVMCI || UseAOT) {
3510     _deopt_blob-&gt;set_uncommon_trap_offset(uncommon_trap_offset);
3511     _deopt_blob-&gt;set_implicit_exception_uncommon_trap_offset(implicit_exception_uncommon_trap_offset);
3512   }
3513 #endif
3514 }
3515 
3516 #ifdef COMPILER2
3517 //------------------------------generate_uncommon_trap_blob--------------------
3518 void SharedRuntime::generate_uncommon_trap_blob() {
3519   // Allocate space for the code
3520   ResourceMark rm;
3521   // Setup code generation tools
3522   CodeBuffer buffer(&quot;uncommon_trap_blob&quot;, 2048, 1024);
3523   MacroAssembler* masm = new MacroAssembler(&amp;buffer);
3524 
3525   assert(SimpleRuntimeFrame::framesize % 4 == 0, &quot;sp not 16-byte aligned&quot;);
3526 
3527   address start = __ pc();
3528 
3529   if (UseRTMLocking) {
3530     // Abort RTM transaction before possible nmethod deoptimization.
3531     __ xabort(0);
3532   }
3533 
3534   // Push self-frame.  We get here with a return address on the
3535   // stack, so rsp is 8-byte aligned until we allocate our frame.
3536   __ subptr(rsp, SimpleRuntimeFrame::return_off &lt;&lt; LogBytesPerInt); // Epilog!
3537 
3538   // No callee saved registers. rbp is assumed implicitly saved
3539   __ movptr(Address(rsp, SimpleRuntimeFrame::rbp_off &lt;&lt; LogBytesPerInt), rbp);
3540 
3541   // compiler left unloaded_class_index in j_rarg0 move to where the
3542   // runtime expects it.
3543   __ movl(c_rarg1, j_rarg0);
3544 
3545   __ set_last_Java_frame(noreg, noreg, NULL);
3546 
3547   // Call C code.  Need thread but NOT official VM entry
3548   // crud.  We cannot block on this call, no GC can happen.  Call should
3549   // capture callee-saved registers as well as return values.
3550   // Thread is in rdi already.
3551   //
3552   // UnrollBlock* uncommon_trap(JavaThread* thread, jint unloaded_class_index);
3553 
3554   __ mov(c_rarg0, r15_thread);
3555   __ movl(c_rarg2, Deoptimization::Unpack_uncommon_trap);
3556   __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::uncommon_trap)));
3557 
3558   // Set an oopmap for the call site
3559   OopMapSet* oop_maps = new OopMapSet();
3560   OopMap* map = new OopMap(SimpleRuntimeFrame::framesize, 0);
3561 
3562   // location of rbp is known implicitly by the frame sender code
3563 
3564   oop_maps-&gt;add_gc_map(__ pc() - start, map);
3565 
3566   __ reset_last_Java_frame(false);
3567 
3568   // Load UnrollBlock* into rdi
3569   __ mov(rdi, rax);
3570 
3571 #ifdef ASSERT
3572   { Label L;
3573     __ cmpptr(Address(rdi, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()),
3574             (int32_t)Deoptimization::Unpack_uncommon_trap);
3575     __ jcc(Assembler::equal, L);
3576     __ stop(&quot;SharedRuntime::generate_deopt_blob: expected Unpack_uncommon_trap&quot;);
3577     __ bind(L);
3578   }
3579 #endif
3580 
3581   // Pop all the frames we must move/replace.
3582   //
3583   // Frame picture (youngest to oldest)
3584   // 1: self-frame (no frame link)
3585   // 2: deopting frame  (no frame link)
3586   // 3: caller of deopting frame (could be compiled/interpreted).
3587 
3588   // Pop self-frame.  We have no frame, and must rely only on rax and rsp.
3589   __ addptr(rsp, (SimpleRuntimeFrame::framesize - 2) &lt;&lt; LogBytesPerInt); // Epilog!
3590 
3591   // Pop deoptimized frame (int)
3592   __ movl(rcx, Address(rdi,
3593                        Deoptimization::UnrollBlock::
3594                        size_of_deoptimized_frame_offset_in_bytes()));
3595   __ addptr(rsp, rcx);
3596 
3597   // rsp should be pointing at the return address to the caller (3)
3598 
3599   // Pick up the initial fp we should save
3600   // restore rbp before stack bang because if stack overflow is thrown it needs to be pushed (and preserved)
3601   __ movptr(rbp, Address(rdi, Deoptimization::UnrollBlock::initial_info_offset_in_bytes()));
3602 
3603 #ifdef ASSERT
3604   // Compilers generate code that bang the stack by as much as the
3605   // interpreter would need. So this stack banging should never
3606   // trigger a fault. Verify that it does not on non product builds.
3607   if (UseStackBanging) {
3608     __ movl(rbx, Address(rdi ,Deoptimization::UnrollBlock::total_frame_sizes_offset_in_bytes()));
3609     __ bang_stack_size(rbx, rcx);
3610   }
3611 #endif
3612 
3613   // Load address of array of frame pcs into rcx (address*)
3614   __ movptr(rcx, Address(rdi, Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes()));
3615 
3616   // Trash the return pc
3617   __ addptr(rsp, wordSize);
3618 
3619   // Load address of array of frame sizes into rsi (intptr_t*)
3620   __ movptr(rsi, Address(rdi, Deoptimization::UnrollBlock:: frame_sizes_offset_in_bytes()));
3621 
3622   // Counter
3623   __ movl(rdx, Address(rdi, Deoptimization::UnrollBlock:: number_of_frames_offset_in_bytes())); // (int)
3624 
3625   // Now adjust the caller&#39;s stack to make up for the extra locals but
3626   // record the original sp so that we can save it in the skeletal
3627   // interpreter frame and the stack walking of interpreter_sender
3628   // will get the unextended sp value and not the &quot;real&quot; sp value.
3629 
3630   const Register sender_sp = r8;
3631 
3632   __ mov(sender_sp, rsp);
3633   __ movl(rbx, Address(rdi, Deoptimization::UnrollBlock:: caller_adjustment_offset_in_bytes())); // (int)
3634   __ subptr(rsp, rbx);
3635 
3636   // Push interpreter frames in a loop
3637   Label loop;
3638   __ bind(loop);
3639   __ movptr(rbx, Address(rsi, 0)); // Load frame size
3640   __ subptr(rbx, 2 * wordSize);    // We&#39;ll push pc and rbp by hand
3641   __ pushptr(Address(rcx, 0));     // Save return address
3642   __ enter();                      // Save old &amp; set new rbp
3643   __ subptr(rsp, rbx);             // Prolog
3644   __ movptr(Address(rbp, frame::interpreter_frame_sender_sp_offset * wordSize),
3645             sender_sp);            // Make it walkable
3646   // This value is corrected by layout_activation_impl
3647   __ movptr(Address(rbp, frame::interpreter_frame_last_sp_offset * wordSize), (int32_t)NULL_WORD );
3648   __ mov(sender_sp, rsp);          // Pass sender_sp to next frame
3649   __ addptr(rsi, wordSize);        // Bump array pointer (sizes)
3650   __ addptr(rcx, wordSize);        // Bump array pointer (pcs)
3651   __ decrementl(rdx);              // Decrement counter
3652   __ jcc(Assembler::notZero, loop);
3653   __ pushptr(Address(rcx, 0));     // Save final return address
3654 
3655   // Re-push self-frame
3656   __ enter();                 // Save old &amp; set new rbp
3657   __ subptr(rsp, (SimpleRuntimeFrame::framesize - 4) &lt;&lt; LogBytesPerInt);
3658                               // Prolog
3659 
3660   // Use rbp because the frames look interpreted now
3661   // Save &quot;the_pc&quot; since it cannot easily be retrieved using the last_java_SP after we aligned SP.
3662   // Don&#39;t need the precise return PC here, just precise enough to point into this code blob.
3663   address the_pc = __ pc();
3664   __ set_last_Java_frame(noreg, rbp, the_pc);
3665 
3666   // Call C code.  Need thread but NOT official VM entry
3667   // crud.  We cannot block on this call, no GC can happen.  Call should
3668   // restore return values to their stack-slots with the new SP.
3669   // Thread is in rdi already.
3670   //
3671   // BasicType unpack_frames(JavaThread* thread, int exec_mode);
3672 
3673   __ andptr(rsp, -(StackAlignmentInBytes)); // Align SP as required by ABI
3674   __ mov(c_rarg0, r15_thread);
3675   __ movl(c_rarg1, Deoptimization::Unpack_uncommon_trap);
3676   __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames)));
3677 
3678   // Set an oopmap for the call site
3679   // Use the same PC we used for the last java frame
3680   oop_maps-&gt;add_gc_map(the_pc - start, new OopMap(SimpleRuntimeFrame::framesize, 0));
3681 
3682   // Clear fp AND pc
3683   __ reset_last_Java_frame(true);
3684 
3685   // Pop self-frame.
3686   __ leave();                 // Epilog
3687 
3688   // Jump to interpreter
3689   __ ret(0);
3690 
3691   // Make sure all code is generated
3692   masm-&gt;flush();
3693 
3694   _uncommon_trap_blob =  UncommonTrapBlob::create(&amp;buffer, oop_maps,
3695                                                  SimpleRuntimeFrame::framesize &gt;&gt; 1);
3696 }
3697 #endif // COMPILER2
3698 
3699 
3700 //------------------------------generate_handler_blob------
3701 //
3702 // Generate a special Compile2Runtime blob that saves all registers,
3703 // and setup oopmap.
3704 //
3705 SafepointBlob* SharedRuntime::generate_handler_blob(address call_ptr, int poll_type) {
3706   assert(StubRoutines::forward_exception_entry() != NULL,
3707          &quot;must be generated before&quot;);
3708 
3709   ResourceMark rm;
3710   OopMapSet *oop_maps = new OopMapSet();
3711   OopMap* map;
3712 
3713   // Allocate space for the code.  Setup code generation tools.
3714   CodeBuffer buffer(&quot;handler_blob&quot;, 2048, 1024);
3715   MacroAssembler* masm = new MacroAssembler(&amp;buffer);
3716 
3717   address start   = __ pc();
3718   address call_pc = NULL;
3719   int frame_size_in_words;
3720   bool cause_return = (poll_type == POLL_AT_RETURN);
3721   bool save_vectors = (poll_type == POLL_AT_VECTOR_LOOP);
3722 
3723   if (UseRTMLocking) {
3724     // Abort RTM transaction before calling runtime
3725     // because critical section will be large and will be
3726     // aborted anyway. Also nmethod could be deoptimized.
3727     __ xabort(0);
3728   }
3729 
3730   // Make room for return address (or push it again)
3731   if (!cause_return) {
3732     __ push(rbx);
3733   }
3734 
3735   // Save registers, fpu state, and flags
3736   map = RegisterSaver::save_live_registers(masm, 0, &amp;frame_size_in_words, save_vectors);
3737 
3738   // The following is basically a call_VM.  However, we need the precise
3739   // address of the call in order to generate an oopmap. Hence, we do all the
3740   // work outselves.
3741 
3742   __ set_last_Java_frame(noreg, noreg, NULL);
3743 
3744   // The return address must always be correct so that frame constructor never
3745   // sees an invalid pc.
3746 
3747   if (!cause_return) {
3748     // Get the return pc saved by the signal handler and stash it in its appropriate place on the stack.
3749     // Additionally, rbx is a callee saved register and we can look at it later to determine
3750     // if someone changed the return address for us!
3751     __ movptr(rbx, Address(r15_thread, JavaThread::saved_exception_pc_offset()));
3752     __ movptr(Address(rbp, wordSize), rbx);
3753   }
3754 
3755   // Do the call
3756   __ mov(c_rarg0, r15_thread);
3757   __ call(RuntimeAddress(call_ptr));
3758 
3759   // Set an oopmap for the call site.  This oopmap will map all
3760   // oop-registers and debug-info registers as callee-saved.  This
3761   // will allow deoptimization at this safepoint to find all possible
3762   // debug-info recordings, as well as let GC find all oops.
3763 
3764   oop_maps-&gt;add_gc_map( __ pc() - start, map);
3765 
3766   Label noException;
3767 
3768   __ reset_last_Java_frame(false);
3769 
3770   __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);
3771   __ jcc(Assembler::equal, noException);
3772 
3773   // Exception pending
3774 
3775   RegisterSaver::restore_live_registers(masm, save_vectors);
3776 
3777   __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
3778 
3779   // No exception case
3780   __ bind(noException);
3781 
3782   Label no_adjust;
3783 #ifdef ASSERT
3784   Label bail;
3785 #endif
3786   if (!cause_return) {
3787     Label no_prefix, not_special;
3788 
3789     // If our stashed return pc was modified by the runtime we avoid touching it
3790     __ cmpptr(rbx, Address(rbp, wordSize));
3791     __ jccb(Assembler::notEqual, no_adjust);
3792 
3793     // Skip over the poll instruction.
3794     // See NativeInstruction::is_safepoint_poll()
3795     // Possible encodings:
3796     //      85 00       test   %eax,(%rax)
3797     //      85 01       test   %eax,(%rcx)
3798     //      85 02       test   %eax,(%rdx)
3799     //      85 03       test   %eax,(%rbx)
3800     //      85 06       test   %eax,(%rsi)
3801     //      85 07       test   %eax,(%rdi)
3802     //
3803     //   41 85 00       test   %eax,(%r8)
3804     //   41 85 01       test   %eax,(%r9)
3805     //   41 85 02       test   %eax,(%r10)
3806     //   41 85 03       test   %eax,(%r11)
3807     //   41 85 06       test   %eax,(%r14)
3808     //   41 85 07       test   %eax,(%r15)
3809     //
3810     //      85 04 24    test   %eax,(%rsp)
3811     //   41 85 04 24    test   %eax,(%r12)
3812     //      85 45 00    test   %eax,0x0(%rbp)
3813     //   41 85 45 00    test   %eax,0x0(%r13)
3814 
3815     __ cmpb(Address(rbx, 0), NativeTstRegMem::instruction_rex_b_prefix);
3816     __ jcc(Assembler::notEqual, no_prefix);
3817     __ addptr(rbx, 1);
3818     __ bind(no_prefix);
3819 #ifdef ASSERT
3820     __ movptr(rax, rbx); // remember where 0x85 should be, for verification below
3821 #endif
3822     // r12/r13/rsp/rbp base encoding takes 3 bytes with the following register values:
3823     // r12/rsp 0x04
3824     // r13/rbp 0x05
3825     __ movzbq(rcx, Address(rbx, 1));
3826     __ andptr(rcx, 0x07); // looking for 0x04 .. 0x05
3827     __ subptr(rcx, 4);    // looking for 0x00 .. 0x01
3828     __ cmpptr(rcx, 1);
3829     __ jcc(Assembler::above, not_special);
3830     __ addptr(rbx, 1);
3831     __ bind(not_special);
3832 #ifdef ASSERT
3833     // Verify the correct encoding of the poll we&#39;re about to skip.
3834     __ cmpb(Address(rax, 0), NativeTstRegMem::instruction_code_memXregl);
3835     __ jcc(Assembler::notEqual, bail);
3836     // Mask out the modrm bits
3837     __ testb(Address(rax, 1), NativeTstRegMem::modrm_mask);
3838     // rax encodes to 0, so if the bits are nonzero it&#39;s incorrect
3839     __ jcc(Assembler::notZero, bail);
3840 #endif
3841     // Adjust return pc forward to step over the safepoint poll instruction
3842     __ addptr(rbx, 2);
3843     __ movptr(Address(rbp, wordSize), rbx);
3844   }
3845 
3846   __ bind(no_adjust);
3847   // Normal exit, restore registers and exit.
3848   RegisterSaver::restore_live_registers(masm, save_vectors);
3849   __ ret(0);
3850 
3851 #ifdef ASSERT
3852   __ bind(bail);
3853   __ stop(&quot;Attempting to adjust pc to skip safepoint poll but the return point is not what we expected&quot;);
3854 #endif
3855 
3856   // Make sure all code is generated
3857   masm-&gt;flush();
3858 
3859   // Fill-out other meta info
3860   return SafepointBlob::create(&amp;buffer, oop_maps, frame_size_in_words);
3861 }
3862 
3863 //
3864 // generate_resolve_blob - call resolution (static/virtual/opt-virtual/ic-miss
3865 //
3866 // Generate a stub that calls into vm to find out the proper destination
3867 // of a java call. All the argument registers are live at this point
3868 // but since this is generic code we don&#39;t know what they are and the caller
3869 // must do any gc of the args.
3870 //
3871 RuntimeStub* SharedRuntime::generate_resolve_blob(address destination, const char* name) {
3872   assert (StubRoutines::forward_exception_entry() != NULL, &quot;must be generated before&quot;);
3873 
3874   // allocate space for the code
3875   ResourceMark rm;
3876 
3877   CodeBuffer buffer(name, 1000, 512);
3878   MacroAssembler* masm                = new MacroAssembler(&amp;buffer);
3879 
3880   int frame_size_in_words;
3881 
3882   OopMapSet *oop_maps = new OopMapSet();
3883   OopMap* map = NULL;
3884 
3885   int start = __ offset();
3886 
3887   map = RegisterSaver::save_live_registers(masm, 0, &amp;frame_size_in_words);
3888 
3889   int frame_complete = __ offset();
3890 
3891   __ set_last_Java_frame(noreg, noreg, NULL);
3892 
3893   __ mov(c_rarg0, r15_thread);
3894 
3895   __ call(RuntimeAddress(destination));
3896 
3897 
3898   // Set an oopmap for the call site.
3899   // We need this not only for callee-saved registers, but also for volatile
3900   // registers that the compiler might be keeping live across a safepoint.
3901 
3902   oop_maps-&gt;add_gc_map( __ offset() - start, map);
3903 
3904   // rax contains the address we are going to jump to assuming no exception got installed
3905 
3906   // clear last_Java_sp
3907   __ reset_last_Java_frame(false);
3908   // check for pending exceptions
3909   Label pending;
3910   __ cmpptr(Address(r15_thread, Thread::pending_exception_offset()), (int32_t)NULL_WORD);
3911   __ jcc(Assembler::notEqual, pending);
3912 
3913   // get the returned Method*
3914   __ get_vm_result_2(rbx, r15_thread);
3915   __ movptr(Address(rsp, RegisterSaver::rbx_offset_in_bytes()), rbx);
3916 
3917   __ movptr(Address(rsp, RegisterSaver::rax_offset_in_bytes()), rax);
3918 
3919   RegisterSaver::restore_live_registers(masm);
3920 
3921   // We are back the the original state on entry and ready to go.
3922 
3923   __ jmp(rax);
3924 
3925   // Pending exception after the safepoint
3926 
3927   __ bind(pending);
3928 
3929   RegisterSaver::restore_live_registers(masm);
3930 
3931   // exception pending =&gt; remove activation and forward to exception handler
3932 
3933   __ movptr(Address(r15_thread, JavaThread::vm_result_offset()), (int)NULL_WORD);
3934 
3935   __ movptr(rax, Address(r15_thread, Thread::pending_exception_offset()));
3936   __ jump(RuntimeAddress(StubRoutines::forward_exception_entry()));
3937 
3938   // -------------
3939   // make sure all code is generated
3940   masm-&gt;flush();
3941 
3942   // return the  blob
3943   // frame_size_words or bytes??
3944   return RuntimeStub::new_runtime_stub(name, &amp;buffer, frame_complete, frame_size_in_words, oop_maps, true);
3945 }
3946 
3947 
3948 //------------------------------Montgomery multiplication------------------------
3949 //
3950 
3951 #ifndef _WINDOWS
3952 
3953 #define ASM_SUBTRACT
3954 
3955 #ifdef ASM_SUBTRACT
3956 // Subtract 0:b from carry:a.  Return carry.
3957 static unsigned long
3958 sub(unsigned long a[], unsigned long b[], unsigned long carry, long len) {
3959   long i = 0, cnt = len;
3960   unsigned long tmp;
3961   asm volatile(&quot;clc; &quot;
3962                &quot;0: ; &quot;
3963                &quot;mov (%[b], %[i], 8), %[tmp]; &quot;
3964                &quot;sbb %[tmp], (%[a], %[i], 8); &quot;
3965                &quot;inc %[i]; dec %[cnt]; &quot;
3966                &quot;jne 0b; &quot;
3967                &quot;mov %[carry], %[tmp]; sbb $0, %[tmp]; &quot;
3968                : [i]&quot;+r&quot;(i), [cnt]&quot;+r&quot;(cnt), [tmp]&quot;=&amp;r&quot;(tmp)
3969                : [a]&quot;r&quot;(a), [b]&quot;r&quot;(b), [carry]&quot;r&quot;(carry)
3970                : &quot;memory&quot;);
3971   return tmp;
3972 }
3973 #else // ASM_SUBTRACT
3974 typedef int __attribute__((mode(TI))) int128;
3975 
3976 // Subtract 0:b from carry:a.  Return carry.
3977 static unsigned long
3978 sub(unsigned long a[], unsigned long b[], unsigned long carry, int len) {
3979   int128 tmp = 0;
3980   int i;
3981   for (i = 0; i &lt; len; i++) {
3982     tmp += a[i];
3983     tmp -= b[i];
3984     a[i] = tmp;
3985     tmp &gt;&gt;= 64;
3986     assert(-1 &lt;= tmp &amp;&amp; tmp &lt;= 0, &quot;invariant&quot;);
3987   }
3988   return tmp + carry;
3989 }
3990 #endif // ! ASM_SUBTRACT
3991 
3992 // Multiply (unsigned) Long A by Long B, accumulating the double-
3993 // length result into the accumulator formed of T0, T1, and T2.
3994 #define MACC(A, B, T0, T1, T2)                                  \
3995 do {                                                            \
3996   unsigned long hi, lo;                                         \
3997   __asm__ (&quot;mul %5; add %%rax, %2; adc %%rdx, %3; adc $0, %4&quot;   \
3998            : &quot;=&amp;d&quot;(hi), &quot;=a&quot;(lo), &quot;+r&quot;(T0), &quot;+r&quot;(T1), &quot;+g&quot;(T2)  \
3999            : &quot;r&quot;(A), &quot;a&quot;(B) : &quot;cc&quot;);                            \
4000  } while(0)
4001 
4002 // As above, but add twice the double-length result into the
4003 // accumulator.
4004 #define MACC2(A, B, T0, T1, T2)                                 \
4005 do {                                                            \
4006   unsigned long hi, lo;                                         \
4007   __asm__ (&quot;mul %5; add %%rax, %2; adc %%rdx, %3; adc $0, %4; &quot; \
4008            &quot;add %%rax, %2; adc %%rdx, %3; adc $0, %4&quot;           \
4009            : &quot;=&amp;d&quot;(hi), &quot;=a&quot;(lo), &quot;+r&quot;(T0), &quot;+r&quot;(T1), &quot;+g&quot;(T2)  \
4010            : &quot;r&quot;(A), &quot;a&quot;(B) : &quot;cc&quot;);                            \
4011  } while(0)
4012 
4013 // Fast Montgomery multiplication.  The derivation of the algorithm is
4014 // in  A Cryptographic Library for the Motorola DSP56000,
4015 // Dusse and Kaliski, Proc. EUROCRYPT 90, pp. 230-237.
4016 
4017 static void __attribute__((noinline))
4018 montgomery_multiply(unsigned long a[], unsigned long b[], unsigned long n[],
4019                     unsigned long m[], unsigned long inv, int len) {
4020   unsigned long t0 = 0, t1 = 0, t2 = 0; // Triple-precision accumulator
4021   int i;
4022 
4023   assert(inv * n[0] == -1UL, &quot;broken inverse in Montgomery multiply&quot;);
4024 
4025   for (i = 0; i &lt; len; i++) {
4026     int j;
4027     for (j = 0; j &lt; i; j++) {
4028       MACC(a[j], b[i-j], t0, t1, t2);
4029       MACC(m[j], n[i-j], t0, t1, t2);
4030     }
4031     MACC(a[i], b[0], t0, t1, t2);
4032     m[i] = t0 * inv;
4033     MACC(m[i], n[0], t0, t1, t2);
4034 
4035     assert(t0 == 0, &quot;broken Montgomery multiply&quot;);
4036 
4037     t0 = t1; t1 = t2; t2 = 0;
4038   }
4039 
4040   for (i = len; i &lt; 2*len; i++) {
4041     int j;
4042     for (j = i-len+1; j &lt; len; j++) {
4043       MACC(a[j], b[i-j], t0, t1, t2);
4044       MACC(m[j], n[i-j], t0, t1, t2);
4045     }
4046     m[i-len] = t0;
4047     t0 = t1; t1 = t2; t2 = 0;
4048   }
4049 
4050   while (t0)
4051     t0 = sub(m, n, t0, len);
4052 }
4053 
4054 // Fast Montgomery squaring.  This uses asymptotically 25% fewer
4055 // multiplies so it should be up to 25% faster than Montgomery
4056 // multiplication.  However, its loop control is more complex and it
4057 // may actually run slower on some machines.
4058 
4059 static void __attribute__((noinline))
4060 montgomery_square(unsigned long a[], unsigned long n[],
4061                   unsigned long m[], unsigned long inv, int len) {
4062   unsigned long t0 = 0, t1 = 0, t2 = 0; // Triple-precision accumulator
4063   int i;
4064 
4065   assert(inv * n[0] == -1UL, &quot;broken inverse in Montgomery multiply&quot;);
4066 
4067   for (i = 0; i &lt; len; i++) {
4068     int j;
4069     int end = (i+1)/2;
4070     for (j = 0; j &lt; end; j++) {
4071       MACC2(a[j], a[i-j], t0, t1, t2);
4072       MACC(m[j], n[i-j], t0, t1, t2);
4073     }
4074     if ((i &amp; 1) == 0) {
4075       MACC(a[j], a[j], t0, t1, t2);
4076     }
4077     for (; j &lt; i; j++) {
4078       MACC(m[j], n[i-j], t0, t1, t2);
4079     }
4080     m[i] = t0 * inv;
4081     MACC(m[i], n[0], t0, t1, t2);
4082 
4083     assert(t0 == 0, &quot;broken Montgomery square&quot;);
4084 
4085     t0 = t1; t1 = t2; t2 = 0;
4086   }
4087 
4088   for (i = len; i &lt; 2*len; i++) {
4089     int start = i-len+1;
4090     int end = start + (len - start)/2;
4091     int j;
4092     for (j = start; j &lt; end; j++) {
4093       MACC2(a[j], a[i-j], t0, t1, t2);
4094       MACC(m[j], n[i-j], t0, t1, t2);
4095     }
4096     if ((i &amp; 1) == 0) {
4097       MACC(a[j], a[j], t0, t1, t2);
4098     }
4099     for (; j &lt; len; j++) {
4100       MACC(m[j], n[i-j], t0, t1, t2);
4101     }
4102     m[i-len] = t0;
4103     t0 = t1; t1 = t2; t2 = 0;
4104   }
4105 
4106   while (t0)
4107     t0 = sub(m, n, t0, len);
4108 }
4109 
4110 // Swap words in a longword.
4111 static unsigned long swap(unsigned long x) {
4112   return (x &lt;&lt; 32) | (x &gt;&gt; 32);
4113 }
4114 
4115 // Copy len longwords from s to d, word-swapping as we go.  The
4116 // destination array is reversed.
4117 static void reverse_words(unsigned long *s, unsigned long *d, int len) {
4118   d += len;
4119   while(len-- &gt; 0) {
4120     d--;
4121     *d = swap(*s);
4122     s++;
4123   }
4124 }
4125 
4126 // The threshold at which squaring is advantageous was determined
4127 // experimentally on an i7-3930K (Ivy Bridge) CPU @ 3.5GHz.
4128 #define MONTGOMERY_SQUARING_THRESHOLD 64
4129 
4130 void SharedRuntime::montgomery_multiply(jint *a_ints, jint *b_ints, jint *n_ints,
4131                                         jint len, jlong inv,
4132                                         jint *m_ints) {
4133   assert(len % 2 == 0, &quot;array length in montgomery_multiply must be even&quot;);
4134   int longwords = len/2;
4135 
4136   // Make very sure we don&#39;t use so much space that the stack might
4137   // overflow.  512 jints corresponds to an 16384-bit integer and
4138   // will use here a total of 8k bytes of stack space.
4139   int total_allocation = longwords * sizeof (unsigned long) * 4;
4140   guarantee(total_allocation &lt;= 8192, &quot;must be&quot;);
4141   unsigned long *scratch = (unsigned long *)alloca(total_allocation);
4142 
4143   // Local scratch arrays
4144   unsigned long
4145     *a = scratch + 0 * longwords,
4146     *b = scratch + 1 * longwords,
4147     *n = scratch + 2 * longwords,
4148     *m = scratch + 3 * longwords;
4149 
4150   reverse_words((unsigned long *)a_ints, a, longwords);
4151   reverse_words((unsigned long *)b_ints, b, longwords);
4152   reverse_words((unsigned long *)n_ints, n, longwords);
4153 
4154   ::montgomery_multiply(a, b, n, m, (unsigned long)inv, longwords);
4155 
4156   reverse_words(m, (unsigned long *)m_ints, longwords);
4157 }
4158 
4159 void SharedRuntime::montgomery_square(jint *a_ints, jint *n_ints,
4160                                       jint len, jlong inv,
4161                                       jint *m_ints) {
4162   assert(len % 2 == 0, &quot;array length in montgomery_square must be even&quot;);
4163   int longwords = len/2;
4164 
4165   // Make very sure we don&#39;t use so much space that the stack might
4166   // overflow.  512 jints corresponds to an 16384-bit integer and
4167   // will use here a total of 6k bytes of stack space.
4168   int total_allocation = longwords * sizeof (unsigned long) * 3;
4169   guarantee(total_allocation &lt;= 8192, &quot;must be&quot;);
4170   unsigned long *scratch = (unsigned long *)alloca(total_allocation);
4171 
4172   // Local scratch arrays
4173   unsigned long
4174     *a = scratch + 0 * longwords,
4175     *n = scratch + 1 * longwords,
4176     *m = scratch + 2 * longwords;
4177 
4178   reverse_words((unsigned long *)a_ints, a, longwords);
4179   reverse_words((unsigned long *)n_ints, n, longwords);
4180 
4181   if (len &gt;= MONTGOMERY_SQUARING_THRESHOLD) {
4182     ::montgomery_square(a, n, m, (unsigned long)inv, longwords);
4183   } else {
4184     ::montgomery_multiply(a, a, n, m, (unsigned long)inv, longwords);
4185   }
4186 
4187   reverse_words(m, (unsigned long *)m_ints, longwords);
4188 }
4189 
4190 #endif // WINDOWS
4191 
4192 #ifdef COMPILER2
4193 // This is here instead of runtime_x86_64.cpp because it uses SimpleRuntimeFrame
4194 //
4195 //------------------------------generate_exception_blob---------------------------
4196 // creates exception blob at the end
4197 // Using exception blob, this code is jumped from a compiled method.
4198 // (see emit_exception_handler in x86_64.ad file)
4199 //
4200 // Given an exception pc at a call we call into the runtime for the
4201 // handler in this method. This handler might merely restore state
4202 // (i.e. callee save registers) unwind the frame and jump to the
4203 // exception handler for the nmethod if there is no Java level handler
4204 // for the nmethod.
4205 //
4206 // This code is entered with a jmp.
4207 //
4208 // Arguments:
4209 //   rax: exception oop
4210 //   rdx: exception pc
4211 //
4212 // Results:
4213 //   rax: exception oop
4214 //   rdx: exception pc in caller or ???
4215 //   destination: exception handler of caller
4216 //
4217 // Note: the exception pc MUST be at a call (precise debug information)
4218 //       Registers rax, rdx, rcx, rsi, rdi, r8-r11 are not callee saved.
4219 //
4220 
4221 void OptoRuntime::generate_exception_blob() {
4222   assert(!OptoRuntime::is_callee_saved_register(RDX_num), &quot;&quot;);
4223   assert(!OptoRuntime::is_callee_saved_register(RAX_num), &quot;&quot;);
4224   assert(!OptoRuntime::is_callee_saved_register(RCX_num), &quot;&quot;);
4225 
4226   assert(SimpleRuntimeFrame::framesize % 4 == 0, &quot;sp not 16-byte aligned&quot;);
4227 
4228   // Allocate space for the code
4229   ResourceMark rm;
4230   // Setup code generation tools
4231   CodeBuffer buffer(&quot;exception_blob&quot;, 2048, 1024);
4232   MacroAssembler* masm = new MacroAssembler(&amp;buffer);
4233 
4234 
4235   address start = __ pc();
4236 
4237   // Exception pc is &#39;return address&#39; for stack walker
4238   __ push(rdx);
4239   __ subptr(rsp, SimpleRuntimeFrame::return_off &lt;&lt; LogBytesPerInt); // Prolog
4240 
4241   // Save callee-saved registers.  See x86_64.ad.
4242 
4243   // rbp is an implicitly saved callee saved register (i.e., the calling
4244   // convention will save/restore it in the prolog/epilog). Other than that
4245   // there are no callee save registers now that adapter frames are gone.
4246 
4247   __ movptr(Address(rsp, SimpleRuntimeFrame::rbp_off &lt;&lt; LogBytesPerInt), rbp);
4248 
4249   // Store exception in Thread object. We cannot pass any arguments to the
4250   // handle_exception call, since we do not want to make any assumption
4251   // about the size of the frame where the exception happened in.
4252   // c_rarg0 is either rdi (Linux) or rcx (Windows).
4253   __ movptr(Address(r15_thread, JavaThread::exception_oop_offset()),rax);
4254   __ movptr(Address(r15_thread, JavaThread::exception_pc_offset()), rdx);
4255 
4256   // This call does all the hard work.  It checks if an exception handler
4257   // exists in the method.
4258   // If so, it returns the handler address.
4259   // If not, it prepares for stack-unwinding, restoring the callee-save
4260   // registers of the frame being removed.
4261   //
4262   // address OptoRuntime::handle_exception_C(JavaThread* thread)
4263 
4264   // At a method handle call, the stack may not be properly aligned
4265   // when returning with an exception.
4266   address the_pc = __ pc();
4267   __ set_last_Java_frame(noreg, noreg, the_pc);
4268   __ mov(c_rarg0, r15_thread);
4269   __ andptr(rsp, -(StackAlignmentInBytes));    // Align stack
4270   __ call(RuntimeAddress(CAST_FROM_FN_PTR(address, OptoRuntime::handle_exception_C)));
4271 
4272   // Set an oopmap for the call site.  This oopmap will only be used if we
4273   // are unwinding the stack.  Hence, all locations will be dead.
4274   // Callee-saved registers will be the same as the frame above (i.e.,
4275   // handle_exception_stub), since they were restored when we got the
4276   // exception.
4277 
4278   OopMapSet* oop_maps = new OopMapSet();
4279 
4280   oop_maps-&gt;add_gc_map(the_pc - start, new OopMap(SimpleRuntimeFrame::framesize, 0));
4281 
4282   __ reset_last_Java_frame(false);
4283 
4284   // Restore callee-saved registers
4285 
4286   // rbp is an implicitly saved callee-saved register (i.e., the calling
4287   // convention will save restore it in prolog/epilog) Other than that
4288   // there are no callee save registers now that adapter frames are gone.
4289 
4290   __ movptr(rbp, Address(rsp, SimpleRuntimeFrame::rbp_off &lt;&lt; LogBytesPerInt));
4291 
4292   __ addptr(rsp, SimpleRuntimeFrame::return_off &lt;&lt; LogBytesPerInt); // Epilog
4293   __ pop(rdx);                  // No need for exception pc anymore
4294 
4295   // rax: exception handler
4296 
4297   // We have a handler in rax (could be deopt blob).
4298   __ mov(r8, rax);
4299 
4300   // Get the exception oop
4301   __ movptr(rax, Address(r15_thread, JavaThread::exception_oop_offset()));
4302   // Get the exception pc in case we are deoptimized
4303   __ movptr(rdx, Address(r15_thread, JavaThread::exception_pc_offset()));
4304 #ifdef ASSERT
4305   __ movptr(Address(r15_thread, JavaThread::exception_handler_pc_offset()), (int)NULL_WORD);
4306   __ movptr(Address(r15_thread, JavaThread::exception_pc_offset()), (int)NULL_WORD);
4307 #endif
4308   // Clear the exception oop so GC no longer processes it as a root.
4309   __ movptr(Address(r15_thread, JavaThread::exception_oop_offset()), (int)NULL_WORD);
4310 
4311   // rax: exception oop
4312   // r8:  exception handler
4313   // rdx: exception pc
4314   // Jump to handler
4315 
4316   __ jmp(r8);
4317 
4318   // Make sure all code is generated
4319   masm-&gt;flush();
4320 
4321   // Set exception blob
4322   _exception_blob =  ExceptionBlob::create(&amp;buffer, oop_maps, SimpleRuntimeFrame::framesize &gt;&gt; 1);
4323 }
4324 #endif // COMPILER2
4325 
4326 BufferedValueTypeBlob* SharedRuntime::generate_buffered_value_type_adapter(const ValueKlass* vk) {
4327   BufferBlob* buf = BufferBlob::create(&quot;value types pack/unpack&quot;, 16 * K);
4328   CodeBuffer buffer(buf);
4329   short buffer_locs[20];
4330   buffer.insts()-&gt;initialize_shared_locs((relocInfo*)buffer_locs,
4331                                          sizeof(buffer_locs)/sizeof(relocInfo));
4332 
4333   MacroAssembler* masm = new MacroAssembler(&amp;buffer);
4334 
4335   const Array&lt;SigEntry&gt;* sig_vk = vk-&gt;extended_sig();
4336   const Array&lt;VMRegPair&gt;* regs = vk-&gt;return_regs();
4337 
4338   int pack_fields_jobject_off = __ offset();
4339   // Resolve pre-allocated buffer from JNI handle.
4340   // We cannot do this in generate_call_stub() because it requires GC code to be initialized.
4341   __ movptr(rax, Address(r13, 0));
4342   __ resolve_jobject(rax /* value */,
4343                      r15_thread /* thread */,
4344                      r12 /* tmp */);
4345   __ movptr(Address(r13, 0), rax);
4346 
4347   int pack_fields_off = __ offset();
4348 
4349   int j = 1;
4350   for (int i = 0; i &lt; sig_vk-&gt;length(); i++) {
4351     BasicType bt = sig_vk-&gt;at(i)._bt;
4352     if (bt == T_VALUETYPE) {
4353       continue;
4354     }
4355     if (bt == T_VOID) {
4356       if (sig_vk-&gt;at(i-1)._bt == T_LONG ||
4357           sig_vk-&gt;at(i-1)._bt == T_DOUBLE) {
4358         j++;
4359       }
4360       continue;
4361     }
4362     int off = sig_vk-&gt;at(i)._offset;
4363     assert(off &gt; 0, &quot;offset in object should be positive&quot;);
4364     VMRegPair pair = regs-&gt;at(j);
4365     VMReg r_1 = pair.first();
4366     VMReg r_2 = pair.second();
4367     Address to(rax, off);
4368     if (bt == T_FLOAT) {
4369       __ movflt(to, r_1-&gt;as_XMMRegister());
4370     } else if (bt == T_DOUBLE) {
4371       __ movdbl(to, r_1-&gt;as_XMMRegister());
4372     } else {
4373       Register val = r_1-&gt;as_Register();
4374       assert_different_registers(to.base(), val, r14, r13, rbx, rscratch1);
4375       if (is_reference_type(bt)) {
4376         __ store_heap_oop(to, val, r14, r13, rbx, IN_HEAP | ACCESS_WRITE | IS_DEST_UNINITIALIZED);
4377       } else {
4378         __ store_sized_value(to, r_1-&gt;as_Register(), type2aelembytes(bt));
4379       }
4380     }
4381     j++;
4382   }
4383   assert(j == regs-&gt;length(), &quot;missed a field?&quot;);
4384 
4385   __ ret(0);
4386 
4387   int unpack_fields_off = __ offset();
4388 
4389   j = 1;
4390   for (int i = 0; i &lt; sig_vk-&gt;length(); i++) {
4391     BasicType bt = sig_vk-&gt;at(i)._bt;
4392     if (bt == T_VALUETYPE) {
4393       continue;
4394     }
4395     if (bt == T_VOID) {
4396       if (sig_vk-&gt;at(i-1)._bt == T_LONG ||
4397           sig_vk-&gt;at(i-1)._bt == T_DOUBLE) {
4398         j++;
4399       }
4400       continue;
4401     }
4402     int off = sig_vk-&gt;at(i)._offset;
4403     assert(off &gt; 0, &quot;offset in object should be positive&quot;);
4404     VMRegPair pair = regs-&gt;at(j);
4405     VMReg r_1 = pair.first();
4406     VMReg r_2 = pair.second();
4407     Address from(rax, off);
4408     if (bt == T_FLOAT) {
4409       __ movflt(r_1-&gt;as_XMMRegister(), from);
4410     } else if (bt == T_DOUBLE) {
4411       __ movdbl(r_1-&gt;as_XMMRegister(), from);
4412     } else if (bt == T_OBJECT || bt == T_ARRAY) {
4413       assert_different_registers(rax, r_1-&gt;as_Register());
4414       __ load_heap_oop(r_1-&gt;as_Register(), from);
4415     } else {
4416       assert(is_java_primitive(bt), &quot;unexpected basic type&quot;);
4417       assert_different_registers(rax, r_1-&gt;as_Register());
4418       size_t size_in_bytes = type2aelembytes(bt);
4419       __ load_sized_value(r_1-&gt;as_Register(), from, size_in_bytes, bt != T_CHAR &amp;&amp; bt != T_BOOLEAN);
4420     }
4421     j++;
4422   }
4423   assert(j == regs-&gt;length(), &quot;missed a field?&quot;);
4424 
4425   if (StressValueTypeReturnedAsFields) {
4426     __ load_klass(rax, rax);
4427     __ orptr(rax, 1);
4428   }
4429 
4430   __ ret(0);
4431 
4432   __ flush();
4433 
4434   return BufferedValueTypeBlob::create(&amp;buffer, pack_fields_off, pack_fields_jobject_off, unpack_fields_off);
4435 }
    </pre>
  </body>
</html>