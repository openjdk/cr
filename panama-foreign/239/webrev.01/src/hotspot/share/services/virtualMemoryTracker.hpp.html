<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/share/services/virtualMemoryTracker.hpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
  1 /*
  2  * Copyright (c) 2013, 2020, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #ifndef SHARE_SERVICES_VIRTUALMEMORYTRACKER_HPP
 26 #define SHARE_SERVICES_VIRTUALMEMORYTRACKER_HPP
 27 
 28 #if INCLUDE_NMT
 29 
 30 #include &quot;memory/allocation.hpp&quot;
 31 #include &quot;memory/metaspace.hpp&quot;
 32 #include &quot;services/allocationSite.hpp&quot;
 33 #include &quot;services/nmtCommon.hpp&quot;
 34 #include &quot;utilities/linkedlist.hpp&quot;
 35 #include &quot;utilities/nativeCallStack.hpp&quot;
 36 #include &quot;utilities/ostream.hpp&quot;
 37 
 38 
 39 /*
 40  * Virtual memory counter
 41  */
 42 class VirtualMemory {
 43  private:
 44   size_t     _reserved;
 45   size_t     _committed;
 46 
 47  public:
 48   VirtualMemory() : _reserved(0), _committed(0) { }
 49 
 50   inline void reserve_memory(size_t sz) { _reserved += sz; }
 51   inline void commit_memory (size_t sz) {
 52     _committed += sz;
 53     assert(_committed &lt;= _reserved, &quot;Sanity check&quot;);
 54   }
 55 
 56   inline void release_memory (size_t sz) {
 57     assert(_reserved &gt;= sz, &quot;Negative amount&quot;);
 58     _reserved -= sz;
 59   }
 60 
 61   inline void uncommit_memory(size_t sz) {
 62     assert(_committed &gt;= sz, &quot;Negative amount&quot;);
 63     _committed -= sz;
 64   }
 65 
 66   inline size_t reserved()  const { return _reserved;  }
 67   inline size_t committed() const { return _committed; }
 68 };
 69 
 70 // Virtual memory allocation site, keeps track where the virtual memory is reserved.
 71 class VirtualMemoryAllocationSite : public AllocationSite&lt;VirtualMemory&gt; {
 72  public:
 73   VirtualMemoryAllocationSite(const NativeCallStack&amp; stack, MEMFLAGS flag) :
 74     AllocationSite&lt;VirtualMemory&gt;(stack, flag) { }
 75 
 76   inline void reserve_memory(size_t sz)  { data()-&gt;reserve_memory(sz);  }
 77   inline void commit_memory (size_t sz)  { data()-&gt;commit_memory(sz);   }
 78   inline void uncommit_memory(size_t sz) { data()-&gt;uncommit_memory(sz); }
 79   inline void release_memory(size_t sz)  { data()-&gt;release_memory(sz);  }
 80   inline size_t reserved() const  { return peek()-&gt;reserved(); }
 81   inline size_t committed() const { return peek()-&gt;committed(); }
 82 };
 83 
 84 class VirtualMemorySummary;
 85 
 86 // This class represents a snapshot of virtual memory at a given time.
 87 // The latest snapshot is saved in a static area.
 88 class VirtualMemorySnapshot : public ResourceObj {
 89   friend class VirtualMemorySummary;
 90 
 91  private:
 92   VirtualMemory  _virtual_memory[mt_number_of_types];
 93 
 94  public:
 95   inline VirtualMemory* by_type(MEMFLAGS flag) {
 96     int index = NMTUtil::flag_to_index(flag);
 97     return &amp;_virtual_memory[index];
 98   }
 99 
100   inline VirtualMemory* by_index(int index) {
101     assert(index &gt;= 0, &quot;Index out of bound&quot;);
102     assert(index &lt; mt_number_of_types, &quot;Index out of bound&quot;);
103     return &amp;_virtual_memory[index];
104   }
105 
106   inline size_t total_reserved() const {
107     size_t amount = 0;
108     for (int index = 0; index &lt; mt_number_of_types; index ++) {
109       amount += _virtual_memory[index].reserved();
110     }
111     return amount;
112   }
113 
114   inline size_t total_committed() const {
115     size_t amount = 0;
116     for (int index = 0; index &lt; mt_number_of_types; index ++) {
117       amount += _virtual_memory[index].committed();
118     }
119     return amount;
120   }
121 
122   void copy_to(VirtualMemorySnapshot* s) {
123     for (int index = 0; index &lt; mt_number_of_types; index ++) {
124       s-&gt;_virtual_memory[index] = _virtual_memory[index];
125     }
126   }
127 };
128 
129 class VirtualMemorySummary : AllStatic {
130  public:
131   static void initialize();
132 
133   static inline void record_reserved_memory(size_t size, MEMFLAGS flag) {
134     as_snapshot()-&gt;by_type(flag)-&gt;reserve_memory(size);
135   }
136 
137   static inline void record_committed_memory(size_t size, MEMFLAGS flag) {
138     as_snapshot()-&gt;by_type(flag)-&gt;commit_memory(size);
139   }
140 
141   static inline void record_uncommitted_memory(size_t size, MEMFLAGS flag) {
142     as_snapshot()-&gt;by_type(flag)-&gt;uncommit_memory(size);
143   }
144 
145   static inline void record_released_memory(size_t size, MEMFLAGS flag) {
146     as_snapshot()-&gt;by_type(flag)-&gt;release_memory(size);
147   }
148 
149   // Move virtual memory from one memory type to another.
150   // Virtual memory can be reserved before it is associated with a memory type, and tagged
151   // as &#39;unknown&#39;. Once the memory is tagged, the virtual memory will be moved from &#39;unknown&#39;
152   // type to specified memory type.
153   static inline void move_reserved_memory(MEMFLAGS from, MEMFLAGS to, size_t size) {
154     as_snapshot()-&gt;by_type(from)-&gt;release_memory(size);
155     as_snapshot()-&gt;by_type(to)-&gt;reserve_memory(size);
156   }
157 
158   static inline void move_committed_memory(MEMFLAGS from, MEMFLAGS to, size_t size) {
159     as_snapshot()-&gt;by_type(from)-&gt;uncommit_memory(size);
160     as_snapshot()-&gt;by_type(to)-&gt;commit_memory(size);
161   }
162 
163   static void snapshot(VirtualMemorySnapshot* s);
164 
165   static VirtualMemorySnapshot* as_snapshot() {
166     return (VirtualMemorySnapshot*)_snapshot;
167   }
168 
169  private:
170   static size_t _snapshot[CALC_OBJ_SIZE_IN_TYPE(VirtualMemorySnapshot, size_t)];
171 };
172 
173 
174 
175 /*
176  * A virtual memory region
177  */
178 class VirtualMemoryRegion {
179  private:
180   address      _base_address;
181   size_t       _size;
182 
183  public:
184   VirtualMemoryRegion(address addr, size_t size) :
185     _base_address(addr), _size(size) {
186      assert(addr != NULL, &quot;Invalid address&quot;);
187      assert(size &gt; 0, &quot;Invalid size&quot;);
188    }
189 
190   inline address base() const { return _base_address;   }
191   inline address end()  const { return base() + size(); }
192   inline size_t  size() const { return _size;           }
193 
194   inline bool is_empty() const { return size() == 0; }
195 
196   inline bool contain_address(address addr) const {
197     return (addr &gt;= base() &amp;&amp; addr &lt; end());
198   }
199 
200 
201   inline bool contain_region(address addr, size_t size) const {
202     return contain_address(addr) &amp;&amp; contain_address(addr + size - 1);
203   }
204 
205   inline bool same_region(address addr, size_t sz) const {
206     return (addr == base() &amp;&amp; sz == size());
207   }
208 
209 
210   inline bool overlap_region(address addr, size_t sz) const {
211     assert(sz &gt; 0, &quot;Invalid size&quot;);
212     assert(size() &gt; 0, &quot;Invalid size&quot;);
213     return contain_address(addr) ||
214            contain_address(addr + sz - 1);
215   }
216 
217   inline bool adjacent_to(address addr, size_t sz) const {
218     return (addr == end() || (addr + sz) == base());
219   }
220 
221   void exclude_region(address addr, size_t sz) {
222     assert(contain_region(addr, sz), &quot;Not containment&quot;);
223     assert(addr == base() || addr + sz == end(), &quot;Can not exclude from middle&quot;);
224     size_t new_size = size() - sz;
225 
226     if (addr == base()) {
227       set_base(addr + sz);
228     }
229     set_size(new_size);
230   }
231 
232   void expand_region(address addr, size_t sz) {
233     assert(adjacent_to(addr, sz), &quot;Not adjacent regions&quot;);
234     if (base() == addr + sz) {
235       set_base(addr);
236     }
237     set_size(size() + sz);
238   }
239 
240   // Returns 0 if regions overlap; 1 if this region follows rgn;
241   //  -1 if this region precedes rgn.
242   inline int compare(const VirtualMemoryRegion&amp; rgn) const {
243     if (overlap_region(rgn.base(), rgn.size())) {
244       return 0;
245     } else if (base() &gt;= rgn.end()) {
246       return 1;
247     } else {
248       assert(rgn.base() &gt;= end(), &quot;Sanity&quot;);
249       return -1;
250     }
251   }
252 
253   // Returns true if regions overlap, false otherwise.
254   inline bool equals(const VirtualMemoryRegion&amp; rgn) const {
255     return compare(rgn) == 0;
256   }
257 
258  protected:
259   void set_base(address base) {
260     assert(base != NULL, &quot;Sanity check&quot;);
261     _base_address = base;
262   }
263 
264   void set_size(size_t  size) {
265     assert(size &gt; 0, &quot;Sanity check&quot;);
266     _size = size;
267   }
268 };
269 
270 
271 class CommittedMemoryRegion : public VirtualMemoryRegion {
272  private:
273   NativeCallStack  _stack;
274 
275  public:
276   CommittedMemoryRegion(address addr, size_t size, const NativeCallStack&amp; stack) :
277     VirtualMemoryRegion(addr, size), _stack(stack) { }
278 
279   inline void set_call_stack(const NativeCallStack&amp; stack) { _stack = stack; }
280   inline const NativeCallStack* call_stack() const         { return &amp;_stack; }
281 };
282 
283 
284 typedef LinkedListIterator&lt;CommittedMemoryRegion&gt; CommittedRegionIterator;
285 
286 int compare_committed_region(const CommittedMemoryRegion&amp;, const CommittedMemoryRegion&amp;);
287 class ReservedMemoryRegion : public VirtualMemoryRegion {
288  private:
289   SortedLinkedList&lt;CommittedMemoryRegion, compare_committed_region&gt;
290     _committed_regions;
291 
292   NativeCallStack  _stack;
293   MEMFLAGS         _flag;
294 
295  public:
296   ReservedMemoryRegion(address base, size_t size, const NativeCallStack&amp; stack,
297     MEMFLAGS flag = mtNone) :
298     VirtualMemoryRegion(base, size), _stack(stack), _flag(flag) { }
299 
300 
301   ReservedMemoryRegion(address base, size_t size) :
302     VirtualMemoryRegion(base, size), _stack(NativeCallStack::empty_stack()), _flag(mtNone) { }
303 
304   // Copy constructor
305   ReservedMemoryRegion(const ReservedMemoryRegion&amp; rr) :
306     VirtualMemoryRegion(rr.base(), rr.size()) {
307     *this = rr;
308   }
309 
310   inline void  set_call_stack(const NativeCallStack&amp; stack) { _stack = stack; }
311   inline const NativeCallStack* call_stack() const          { return &amp;_stack;  }
312 
313   void  set_flag(MEMFLAGS flag);
314   inline MEMFLAGS flag() const            { return _flag;  }
315 
316   // uncommitted thread stack bottom, above guard pages if there is any.
317   address thread_stack_uncommitted_bottom() const;
318 
319   bool    add_committed_region(address addr, size_t size, const NativeCallStack&amp; stack);
320   bool    remove_uncommitted_region(address addr, size_t size);
321 
322   size_t  committed_size() const;
323 
324   // move committed regions that higher than specified address to
325   // the new region
326   void    move_committed_regions(address addr, ReservedMemoryRegion&amp; rgn);
327 
328   CommittedRegionIterator iterate_committed_regions() const {
329     return CommittedRegionIterator(_committed_regions.head());
330   }
331 
332   ReservedMemoryRegion&amp; operator= (const ReservedMemoryRegion&amp; other) {
333     set_base(other.base());
334     set_size(other.size());
335 
336     _stack =         *other.call_stack();
337     _flag  =         other.flag();
338 
339     CommittedRegionIterator itr = other.iterate_committed_regions();
340     const CommittedMemoryRegion* rgn = itr.next();
341     while (rgn != NULL) {
342       _committed_regions.add(*rgn);
343       rgn = itr.next();
344     }
345 
346     return *this;
347   }
348 
349  private:
350   // The committed region contains the uncommitted region, subtract the uncommitted
351   // region from this committed region
352   bool remove_uncommitted_region(LinkedListNode&lt;CommittedMemoryRegion&gt;* node,
353     address addr, size_t sz);
354 
355   bool add_committed_region(const CommittedMemoryRegion&amp; rgn) {
356     assert(rgn.base() != NULL, &quot;Invalid base address&quot;);
357     assert(size() &gt; 0, &quot;Invalid size&quot;);
358     return _committed_regions.add(rgn) != NULL;
359   }
360 };
361 
362 int compare_reserved_region_base(const ReservedMemoryRegion&amp; r1, const ReservedMemoryRegion&amp; r2);
363 
364 class VirtualMemoryWalker : public StackObj {
365  public:
366    virtual bool do_allocation_site(const ReservedMemoryRegion* rgn) { return false; }
367 };
368 
369 // Main class called from MemTracker to track virtual memory allocations, commits and releases.
370 class VirtualMemoryTracker : AllStatic {
371   friend class VirtualMemoryTrackerTest;
372   friend class CommittedVirtualMemoryTest;
373 
374  public:
375   static bool initialize(NMT_TrackingLevel level);
376 
377   // Late phase initialization
378   static bool late_initialize(NMT_TrackingLevel level);
379 
380   static bool add_reserved_region (address base_addr, size_t size, const NativeCallStack&amp; stack, MEMFLAGS flag = mtNone);
381 
382   static bool add_committed_region      (address base_addr, size_t size, const NativeCallStack&amp; stack);
383   static bool remove_uncommitted_region (address base_addr, size_t size);
384   static bool remove_released_region    (address base_addr, size_t size);
385   static bool remove_released_region    (ReservedMemoryRegion* rgn);
386   static void set_reserved_region_type  (address addr, MEMFLAGS flag);
387 
388   // Given an existing memory mapping registered with NMT, split the mapping in
389   //  two. The newly created two mappings will be registered under the call
390   //  stack and the memory flags of the original section.
391   static bool split_reserved_region(address addr, size_t size, size_t split);
392 
393   // Walk virtual memory data structure for creating baseline, etc.
394   static bool walk_virtual_memory(VirtualMemoryWalker* walker);
395 
396   static bool transition(NMT_TrackingLevel from, NMT_TrackingLevel to);
397 
398   // Snapshot current thread stacks
399   static void snapshot_thread_stacks();
400 
401  private:
402   static SortedLinkedList&lt;ReservedMemoryRegion, compare_reserved_region_base&gt;* _reserved_regions;
403 };
404 
405 
406 class MetaspaceSnapshot : public ResourceObj {
407 private:
408   size_t  _reserved_in_bytes[Metaspace::MetadataTypeCount];
409   size_t  _committed_in_bytes[Metaspace::MetadataTypeCount];
410   size_t  _used_in_bytes[Metaspace::MetadataTypeCount];
411   size_t  _free_in_bytes[Metaspace::MetadataTypeCount];
412 
413 public:
414   MetaspaceSnapshot();
415   size_t reserved_in_bytes(Metaspace::MetadataType type)   const { assert_valid_metadata_type(type); return _reserved_in_bytes[type]; }
416   size_t committed_in_bytes(Metaspace::MetadataType type)  const { assert_valid_metadata_type(type); return _committed_in_bytes[type]; }
417   size_t used_in_bytes(Metaspace::MetadataType type)       const { assert_valid_metadata_type(type); return _used_in_bytes[type]; }
418   size_t free_in_bytes(Metaspace::MetadataType type)       const { assert_valid_metadata_type(type); return _free_in_bytes[type]; }
419 
420   static void snapshot(MetaspaceSnapshot&amp; s);
421 
422 private:
423   static void snapshot(Metaspace::MetadataType type, MetaspaceSnapshot&amp; s);
424 
425   static void assert_valid_metadata_type(Metaspace::MetadataType type) {
426     assert(type == Metaspace::ClassType || type == Metaspace::NonClassType,
427       &quot;Invalid metadata type&quot;);
428   }
429 };
430 
431 #endif // INCLUDE_NMT
432 
433 #endif // SHARE_SERVICES_VIRTUALMEMORYTRACKER_HPP
    </pre>
  </body>
</html>