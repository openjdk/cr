<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Frames src/hotspot/share/memory/metaspace/virtualSpaceNode.cpp</title>
    <link rel="stylesheet" href="../../../../../style.css" />
    <script type="text/javascript" src="../../../../../navigation.js"></script>
  </head>
<body onkeypress="keypress(event);">
<a name="0"></a>
<hr />
<pre>  1 /*
  2  * Copyright (c) 2018, Oracle and/or its affiliates. All rights reserved.
  3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
  4  *
  5  * This code is free software; you can redistribute it and/or modify it
  6  * under the terms of the GNU General Public License version 2 only, as
  7  * published by the Free Software Foundation.
  8  *
  9  * This code is distributed in the hope that it will be useful, but WITHOUT
 10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
 12  * version 2 for more details (a copy is included in the LICENSE file that
 13  * accompanied this code).
 14  *
 15  * You should have received a copy of the GNU General Public License version
 16  * 2 along with this work; if not, write to the Free Software Foundation,
 17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
 18  *
 19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
 20  * or visit www.oracle.com if you need additional information or have any
 21  * questions.
 22  *
 23  */
 24 
 25 #include &quot;precompiled.hpp&quot;
 26 
 27 #include &quot;logging/log.hpp&quot;
 28 #include &quot;logging/logStream.hpp&quot;
 29 #include &quot;memory/metaspace/metachunk.hpp&quot;
 30 #include &quot;memory/metaspace.hpp&quot;
 31 #include &quot;memory/metaspace/chunkManager.hpp&quot;
 32 #include &quot;memory/metaspace/metaDebug.hpp&quot;
 33 #include &quot;memory/metaspace/metaspaceCommon.hpp&quot;
 34 #include &quot;memory/metaspace/occupancyMap.hpp&quot;
 35 #include &quot;memory/metaspace/virtualSpaceNode.hpp&quot;
 36 #include &quot;memory/virtualspace.hpp&quot;
<a name="1" id="anc1"></a><span class="line-added"> 37 #include &quot;runtime/atomic.hpp&quot;</span>
 38 #include &quot;runtime/os.hpp&quot;
 39 #include &quot;services/memTracker.hpp&quot;
 40 #include &quot;utilities/copy.hpp&quot;
 41 #include &quot;utilities/debug.hpp&quot;
 42 #include &quot;utilities/globalDefinitions.hpp&quot;
 43 
 44 namespace metaspace {
 45 
 46 // Decide if large pages should be committed when the memory is reserved.
 47 static bool should_commit_large_pages_when_reserving(size_t bytes) {
 48   if (UseLargePages &amp;&amp; UseLargePagesInMetaspace &amp;&amp; !os::can_commit_large_page_memory()) {
 49     size_t words = bytes / BytesPerWord;
 50     bool is_class = false; // We never reserve large pages for the class space.
 51     if (MetaspaceGC::can_expand(words, is_class) &amp;&amp;
 52         MetaspaceGC::allowed_expansion() &gt;= words) {
 53       return true;
 54     }
 55   }
 56 
 57   return false;
 58 }
 59 
 60 // byte_size is the size of the associated virtualspace.
 61 VirtualSpaceNode::VirtualSpaceNode(bool is_class, size_t bytes) :
 62     _next(NULL), _is_class(is_class), _rs(), _top(NULL), _container_count(0), _occupancy_map(NULL) {
 63   assert_is_aligned(bytes, Metaspace::reserve_alignment());
 64   bool large_pages = should_commit_large_pages_when_reserving(bytes);
 65   _rs = ReservedSpace(bytes, Metaspace::reserve_alignment(), large_pages);
 66 
 67   if (_rs.is_reserved()) {
 68     assert(_rs.base() != NULL, &quot;Catch if we get a NULL address&quot;);
 69     assert(_rs.size() != 0, &quot;Catch if we get a 0 size&quot;);
 70     assert_is_aligned(_rs.base(), Metaspace::reserve_alignment());
 71     assert_is_aligned(_rs.size(), Metaspace::reserve_alignment());
 72 
 73     MemTracker::record_virtual_memory_type((address)_rs.base(), mtClass);
 74   }
 75 }
 76 
 77 void VirtualSpaceNode::purge(ChunkManager* chunk_manager) {
 78   // When a node is purged, lets give it a thorough examination.
 79   DEBUG_ONLY(verify(true);)
 80   Metachunk* chunk = first_chunk();
 81   Metachunk* invalid_chunk = (Metachunk*) top();
 82   while (chunk &lt; invalid_chunk ) {
 83     assert(chunk-&gt;is_tagged_free(), &quot;Should be tagged free&quot;);
 84     MetaWord* next = ((MetaWord*)chunk) + chunk-&gt;word_size();
 85     chunk_manager-&gt;remove_chunk(chunk);
 86     chunk-&gt;remove_sentinel();
 87     assert(chunk-&gt;next() == NULL &amp;&amp;
 88         chunk-&gt;prev() == NULL,
 89         &quot;Was not removed from its list&quot;);
 90     chunk = (Metachunk*) next;
 91   }
 92 }
 93 
 94 void VirtualSpaceNode::print_map(outputStream* st, bool is_class) const {
 95 
 96   if (bottom() == top()) {
 97     return;
 98   }
 99 
100   const size_t spec_chunk_size = is_class ? ClassSpecializedChunk : SpecializedChunk;
101   const size_t small_chunk_size = is_class ? ClassSmallChunk : SmallChunk;
102   const size_t med_chunk_size = is_class ? ClassMediumChunk : MediumChunk;
103 
104   int line_len = 100;
105   const size_t section_len = align_up(spec_chunk_size * line_len, med_chunk_size);
106   line_len = (int)(section_len / spec_chunk_size);
107 
108   static const int NUM_LINES = 4;
109 
110   char* lines[NUM_LINES];
111   for (int i = 0; i &lt; NUM_LINES; i ++) {
112     lines[i] = (char*)os::malloc(line_len, mtInternal);
113   }
114   int pos = 0;
115   const MetaWord* p = bottom();
116   const Metachunk* chunk = (const Metachunk*)p;
117   const MetaWord* chunk_end = p + chunk-&gt;word_size();
118   while (p &lt; top()) {
119     if (pos == line_len) {
120       pos = 0;
121       for (int i = 0; i &lt; NUM_LINES; i ++) {
122         st-&gt;fill_to(22);
123         st-&gt;print_raw(lines[i], line_len);
124         st-&gt;cr();
125       }
126     }
127     if (pos == 0) {
128       st-&gt;print(PTR_FORMAT &quot;:&quot;, p2i(p));
129     }
130     if (p == chunk_end) {
131       chunk = (Metachunk*)p;
132       chunk_end = p + chunk-&gt;word_size();
133     }
134     // line 1: chunk starting points (a dot if that area is a chunk start).
135     lines[0][pos] = p == (const MetaWord*)chunk ? &#39;.&#39; : &#39; &#39;;
136 
137     // Line 2: chunk type (x=spec, s=small, m=medium, h=humongous), uppercase if
138     // chunk is in use.
139     const bool chunk_is_free = ((Metachunk*)chunk)-&gt;is_tagged_free();
140     if (chunk-&gt;word_size() == spec_chunk_size) {
141       lines[1][pos] = chunk_is_free ? &#39;x&#39; : &#39;X&#39;;
142     } else if (chunk-&gt;word_size() == small_chunk_size) {
143       lines[1][pos] = chunk_is_free ? &#39;s&#39; : &#39;S&#39;;
144     } else if (chunk-&gt;word_size() == med_chunk_size) {
145       lines[1][pos] = chunk_is_free ? &#39;m&#39; : &#39;M&#39;;
146     } else if (chunk-&gt;word_size() &gt; med_chunk_size) {
147       lines[1][pos] = chunk_is_free ? &#39;h&#39; : &#39;H&#39;;
148     } else {
149       ShouldNotReachHere();
150     }
151 
152     // Line 3: chunk origin
153     const ChunkOrigin origin = chunk-&gt;get_origin();
154     lines[2][pos] = origin == origin_normal ? &#39; &#39; : &#39;0&#39; + (int) origin;
155 
156     // Line 4: Virgin chunk? Virgin chunks are chunks created as a byproduct of padding or splitting,
157     //         but were never used.
158     lines[3][pos] = chunk-&gt;get_use_count() &gt; 0 ? &#39; &#39; : &#39;v&#39;;
159 
160     p += spec_chunk_size;
161     pos ++;
162   }
163   if (pos &gt; 0) {
164     for (int i = 0; i &lt; NUM_LINES; i ++) {
165       st-&gt;fill_to(22);
166       st-&gt;print_raw(lines[i], line_len);
167       st-&gt;cr();
168     }
169   }
170   for (int i = 0; i &lt; NUM_LINES; i ++) {
171     os::free(lines[i]);
172   }
173 }
174 
175 
176 #ifdef ASSERT
177 
178 // Verify counters, all chunks in this list node and the occupancy map.
179 void VirtualSpaceNode::verify(bool slow) {
180   log_trace(gc, metaspace, freelist)(&quot;verifying %s virtual space node (%s).&quot;,
181     (is_class() ? &quot;class space&quot; : &quot;metaspace&quot;), (slow ? &quot;slow&quot; : &quot;quick&quot;));
182   // Fast mode: just verify chunk counters and basic geometry
183   // Slow mode: verify chunks and occupancy map
184   uintx num_in_use_chunks = 0;
185   Metachunk* chunk = first_chunk();
186   Metachunk* invalid_chunk = (Metachunk*) top();
187 
188   // Iterate the chunks in this node and verify each chunk.
189   while (chunk &lt; invalid_chunk ) {
190     if (slow) {
191       do_verify_chunk(chunk);
192     }
193     if (!chunk-&gt;is_tagged_free()) {
194       num_in_use_chunks ++;
195     }
196     const size_t s = chunk-&gt;word_size();
197     // Prevent endless loop on invalid chunk size.
198     assert(is_valid_chunksize(is_class(), s), &quot;Invalid chunk size: &quot; SIZE_FORMAT &quot;.&quot;, s);
199     MetaWord* next = ((MetaWord*)chunk) + s;
200     chunk = (Metachunk*) next;
201   }
202   assert(_container_count == num_in_use_chunks, &quot;Container count mismatch (real: &quot; UINTX_FORMAT
203       &quot;, counter: &quot; UINTX_FORMAT &quot;.&quot;, num_in_use_chunks, _container_count);
204   // Also verify the occupancy map.
205   if (slow) {
206     occupancy_map()-&gt;verify(bottom(), top());
207   }
208 }
209 
210 // Verify that all free chunks in this node are ideally merged
211 // (there not should be multiple small chunks where a large chunk could exist.)
212 void VirtualSpaceNode::verify_free_chunks_are_ideally_merged() {
213   Metachunk* chunk = first_chunk();
214   Metachunk* invalid_chunk = (Metachunk*) top();
215   // Shorthands.
216   const size_t size_med = (is_class() ? ClassMediumChunk : MediumChunk) * BytesPerWord;
217   const size_t size_small = (is_class() ? ClassSmallChunk : SmallChunk) * BytesPerWord;
218   int num_free_chunks_since_last_med_boundary = -1;
219   int num_free_chunks_since_last_small_boundary = -1;
220   bool error = false;
221   char err[256];
222   while (!error &amp;&amp; chunk &lt; invalid_chunk ) {
223     // Test for missed chunk merge opportunities: count number of free chunks since last chunk boundary.
224     // Reset the counter when encountering a non-free chunk.
225     if (chunk-&gt;get_chunk_type() != HumongousIndex) {
226       if (chunk-&gt;is_tagged_free()) {
227         // Count successive free, non-humongous chunks.
228         if (is_aligned(chunk, size_small)) {
229           if (num_free_chunks_since_last_small_boundary &gt; 0) {
230             error = true;
231             jio_snprintf(err, sizeof(err), &quot;Missed chunk merge opportunity to merge a small chunk preceding &quot; PTR_FORMAT &quot;.&quot;, p2i(chunk));
232           } else {
233             num_free_chunks_since_last_small_boundary = 0;
234           }
235         } else if (num_free_chunks_since_last_small_boundary != -1) {
236           num_free_chunks_since_last_small_boundary ++;
237         }
238         if (is_aligned(chunk, size_med)) {
239           if (num_free_chunks_since_last_med_boundary &gt; 0) {
240             error = true;
241             jio_snprintf(err, sizeof(err), &quot;Missed chunk merge opportunity to merge a medium chunk preceding &quot; PTR_FORMAT &quot;.&quot;, p2i(chunk));
242           } else {
243             num_free_chunks_since_last_med_boundary = 0;
244           }
245         } else if (num_free_chunks_since_last_med_boundary != -1) {
246           num_free_chunks_since_last_med_boundary ++;
247         }
248       } else {
249         // Encountering a non-free chunk, reset counters.
250         num_free_chunks_since_last_med_boundary = -1;
251         num_free_chunks_since_last_small_boundary = -1;
252       }
253     } else {
254       // One cannot merge areas with a humongous chunk in the middle. Reset counters.
255       num_free_chunks_since_last_med_boundary = -1;
256       num_free_chunks_since_last_small_boundary = -1;
257     }
258 
259     if (error) {
260       print_map(tty, is_class());
261       fatal(&quot;%s&quot;, err);
262     }
263 
264     MetaWord* next = ((MetaWord*)chunk) + chunk-&gt;word_size();
265     chunk = (Metachunk*) next;
266   }
267 }
268 #endif // ASSERT
269 
270 void VirtualSpaceNode::inc_container_count() {
271   assert_lock_strong(MetaspaceExpand_lock);
272   _container_count++;
273 }
274 
275 void VirtualSpaceNode::dec_container_count() {
276   assert_lock_strong(MetaspaceExpand_lock);
277   _container_count--;
278 }
279 
280 VirtualSpaceNode::~VirtualSpaceNode() {
281   _rs.release();
282   if (_occupancy_map != NULL) {
283     delete _occupancy_map;
284   }
285 #ifdef ASSERT
286   size_t word_size = sizeof(*this) / BytesPerWord;
287   Copy::fill_to_words((HeapWord*) this, word_size, 0xf1f1f1f1);
288 #endif
289 }
290 
291 size_t VirtualSpaceNode::used_words_in_vs() const {
292   return pointer_delta(top(), bottom(), sizeof(MetaWord));
293 }
294 
295 // Space committed in the VirtualSpace
296 size_t VirtualSpaceNode::capacity_words_in_vs() const {
297   return pointer_delta(end(), bottom(), sizeof(MetaWord));
298 }
299 
300 size_t VirtualSpaceNode::free_words_in_vs() const {
301   return pointer_delta(end(), top(), sizeof(MetaWord));
302 }
303 
304 // Given an address larger than top(), allocate padding chunks until top is at the given address.
305 void VirtualSpaceNode::allocate_padding_chunks_until_top_is_at(MetaWord* target_top) {
306 
307   assert(target_top &gt; top(), &quot;Sanity&quot;);
308 
309   // Padding chunks are added to the freelist.
310   ChunkManager* const chunk_manager = Metaspace::get_chunk_manager(is_class());
311 
312   // shorthands
313   const size_t spec_word_size = chunk_manager-&gt;specialized_chunk_word_size();
314   const size_t small_word_size = chunk_manager-&gt;small_chunk_word_size();
315   const size_t med_word_size = chunk_manager-&gt;medium_chunk_word_size();
316 
317   while (top() &lt; target_top) {
318 
319     // We could make this coding more generic, but right now we only deal with two possible chunk sizes
320     // for padding chunks, so it is not worth it.
321     size_t padding_chunk_word_size = small_word_size;
322     if (is_aligned(top(), small_word_size * sizeof(MetaWord)) == false) {
323       assert_is_aligned(top(), spec_word_size * sizeof(MetaWord)); // Should always hold true.
324       padding_chunk_word_size = spec_word_size;
325     }
326     MetaWord* here = top();
327     assert_is_aligned(here, padding_chunk_word_size * sizeof(MetaWord));
328     inc_top(padding_chunk_word_size);
329 
330     // Create new padding chunk.
331     ChunkIndex padding_chunk_type = get_chunk_type_by_size(padding_chunk_word_size, is_class());
332     assert(padding_chunk_type == SpecializedIndex || padding_chunk_type == SmallIndex, &quot;sanity&quot;);
333 
334     Metachunk* const padding_chunk =
335         ::new (here) Metachunk(padding_chunk_type, is_class(), padding_chunk_word_size, this);
336     assert(padding_chunk == (Metachunk*)here, &quot;Sanity&quot;);
337     DEBUG_ONLY(padding_chunk-&gt;set_origin(origin_pad);)
338     log_trace(gc, metaspace, freelist)(&quot;Created padding chunk in %s at &quot;
339         PTR_FORMAT &quot;, size &quot; SIZE_FORMAT_HEX &quot;.&quot;,
340         (is_class() ? &quot;class space &quot; : &quot;metaspace&quot;),
341         p2i(padding_chunk), padding_chunk-&gt;word_size() * sizeof(MetaWord));
342 
343     // Mark chunk start in occupancy map.
344     occupancy_map()-&gt;set_chunk_starts_at_address((MetaWord*)padding_chunk, true);
345 
346     // Chunks are born as in-use (see MetaChunk ctor). So, before returning
347     // the padding chunk to its chunk manager, mark it as in use (ChunkManager
348     // will assert that).
349     do_update_in_use_info_for_chunk(padding_chunk, true);
350 
351     // Return Chunk to freelist.
352     inc_container_count();
353     chunk_manager-&gt;return_single_chunk(padding_chunk);
354     // Please note: at this point, ChunkManager::return_single_chunk()
355     // may already have merged the padding chunk with neighboring chunks, so
356     // it may have vanished at this point. Do not reference the padding
357     // chunk beyond this point.
358   }
359 
360   assert(top() == target_top, &quot;Sanity&quot;);
361 
362 } // allocate_padding_chunks_until_top_is_at()
363 
364 // Allocates the chunk from the virtual space only.
365 // This interface is also used internally for debugging.  Not all
366 // chunks removed here are necessarily used for allocation.
367 Metachunk* VirtualSpaceNode::take_from_committed(size_t chunk_word_size) {
368   // Non-humongous chunks are to be allocated aligned to their chunk
369   // size. So, start addresses of medium chunks are aligned to medium
370   // chunk size, those of small chunks to small chunk size and so
371   // forth. This facilitates merging of free chunks and reduces
372   // fragmentation. Chunk sizes are spec &lt; small &lt; medium, with each
373   // larger chunk size being a multiple of the next smaller chunk
374   // size.
375   // Because of this alignment, me may need to create a number of padding
376   // chunks. These chunks are created and added to the freelist.
377 
378   // The chunk manager to which we will give our padding chunks.
379   ChunkManager* const chunk_manager = Metaspace::get_chunk_manager(is_class());
380 
381   // shorthands
382   const size_t spec_word_size = chunk_manager-&gt;specialized_chunk_word_size();
383   const size_t small_word_size = chunk_manager-&gt;small_chunk_word_size();
384   const size_t med_word_size = chunk_manager-&gt;medium_chunk_word_size();
385 
386   assert(chunk_word_size == spec_word_size || chunk_word_size == small_word_size ||
387       chunk_word_size &gt;= med_word_size, &quot;Invalid chunk size requested.&quot;);
388 
389   // Chunk alignment (in bytes) == chunk size unless humongous.
390   // Humongous chunks are aligned to the smallest chunk size (spec).
391   const size_t required_chunk_alignment = (chunk_word_size &gt; med_word_size ?
392       spec_word_size : chunk_word_size) * sizeof(MetaWord);
393 
394   // Do we have enough space to create the requested chunk plus
395   // any padding chunks needed?
396   MetaWord* const next_aligned =
397       static_cast&lt;MetaWord*&gt;(align_up(top(), required_chunk_alignment));
398   if (!is_available((next_aligned - top()) + chunk_word_size)) {
399     return NULL;
400   }
401 
402   // Before allocating the requested chunk, allocate padding chunks if necessary.
403   // We only need to do this for small or medium chunks: specialized chunks are the
404   // smallest size, hence always aligned. Homungous chunks are allocated unaligned
405   // (implicitly, also aligned to smallest chunk size).
406   if ((chunk_word_size == med_word_size || chunk_word_size == small_word_size) &amp;&amp; next_aligned &gt; top())  {
407     log_trace(gc, metaspace, freelist)(&quot;Creating padding chunks in %s between %p and %p...&quot;,
408         (is_class() ? &quot;class space &quot; : &quot;metaspace&quot;),
409         top(), next_aligned);
410     allocate_padding_chunks_until_top_is_at(next_aligned);
411     // Now, top should be aligned correctly.
412     assert_is_aligned(top(), required_chunk_alignment);
413   }
414 
415   // Now, top should be aligned correctly.
416   assert_is_aligned(top(), required_chunk_alignment);
417 
418   // Bottom of the new chunk
419   MetaWord* chunk_limit = top();
420   assert(chunk_limit != NULL, &quot;Not safe to call this method&quot;);
421 
422   // The virtual spaces are always expanded by the
423   // commit granularity to enforce the following condition.
424   // Without this the is_available check will not work correctly.
425   assert(_virtual_space.committed_size() == _virtual_space.actual_committed_size(),
426       &quot;The committed memory doesn&#39;t match the expanded memory.&quot;);
427 
428   if (!is_available(chunk_word_size)) {
429     LogTarget(Trace, gc, metaspace, freelist) lt;
430     if (lt.is_enabled()) {
431       LogStream ls(lt);
432       ls.print(&quot;VirtualSpaceNode::take_from_committed() not available &quot; SIZE_FORMAT &quot; words &quot;, chunk_word_size);
433       // Dump some information about the virtual space that is nearly full
434       print_on(&amp;ls);
435     }
436     return NULL;
437   }
438 
439   // Take the space  (bump top on the current virtual space).
440   inc_top(chunk_word_size);
441 
442   // Initialize the chunk
443   ChunkIndex chunk_type = get_chunk_type_by_size(chunk_word_size, is_class());
444   Metachunk* result = ::new (chunk_limit) Metachunk(chunk_type, is_class(), chunk_word_size, this);
445   assert(result == (Metachunk*)chunk_limit, &quot;Sanity&quot;);
446   occupancy_map()-&gt;set_chunk_starts_at_address((MetaWord*)result, true);
447   do_update_in_use_info_for_chunk(result, true);
448 
449   inc_container_count();
450 
451 #ifdef ASSERT
452   EVERY_NTH(VerifyMetaspaceInterval)
453     chunk_manager-&gt;locked_verify(true);
454     verify(true);
455   END_EVERY_NTH
456   do_verify_chunk(result);
457 #endif
458 
459   result-&gt;inc_use_count();
460 
461   return result;
462 }
463 
464 
465 // Expand the virtual space (commit more of the reserved space)
466 bool VirtualSpaceNode::expand_by(size_t min_words, size_t preferred_words) {
467   size_t min_bytes = min_words * BytesPerWord;
468   size_t preferred_bytes = preferred_words * BytesPerWord;
469 
470   size_t uncommitted = virtual_space()-&gt;reserved_size() - virtual_space()-&gt;actual_committed_size();
471 
472   if (uncommitted &lt; min_bytes) {
473     return false;
474   }
475 
476   size_t commit = MIN2(preferred_bytes, uncommitted);
477   bool result = virtual_space()-&gt;expand_by(commit, false);
478 
479   if (result) {
480     log_trace(gc, metaspace, freelist)(&quot;Expanded %s virtual space list node by &quot; SIZE_FORMAT &quot; words.&quot;,
481         (is_class() ? &quot;class&quot; : &quot;non-class&quot;), commit);
482     DEBUG_ONLY(Atomic::inc(&amp;g_internal_statistics.num_committed_space_expanded));
483   } else {
484     log_trace(gc, metaspace, freelist)(&quot;Failed to expand %s virtual space list node by &quot; SIZE_FORMAT &quot; words.&quot;,
485         (is_class() ? &quot;class&quot; : &quot;non-class&quot;), commit);
486   }
487 
488   assert(result, &quot;Failed to commit memory&quot;);
489 
490   return result;
491 }
492 
493 Metachunk* VirtualSpaceNode::get_chunk_vs(size_t chunk_word_size) {
494   assert_lock_strong(MetaspaceExpand_lock);
495   Metachunk* result = take_from_committed(chunk_word_size);
496   return result;
497 }
498 
499 bool VirtualSpaceNode::initialize() {
500 
501   if (!_rs.is_reserved()) {
502     return false;
503   }
504 
505   // These are necessary restriction to make sure that the virtual space always
506   // grows in steps of Metaspace::commit_alignment(). If both base and size are
507   // aligned only the middle alignment of the VirtualSpace is used.
508   assert_is_aligned(_rs.base(), Metaspace::commit_alignment());
509   assert_is_aligned(_rs.size(), Metaspace::commit_alignment());
510 
511   // ReservedSpaces marked as special will have the entire memory
512   // pre-committed. Setting a committed size will make sure that
513   // committed_size and actual_committed_size agrees.
514   size_t pre_committed_size = _rs.special() ? _rs.size() : 0;
515 
516   bool result = virtual_space()-&gt;initialize_with_granularity(_rs, pre_committed_size,
517       Metaspace::commit_alignment());
518   if (result) {
519     assert(virtual_space()-&gt;committed_size() == virtual_space()-&gt;actual_committed_size(),
520         &quot;Checking that the pre-committed memory was registered by the VirtualSpace&quot;);
521 
522     set_top((MetaWord*)virtual_space()-&gt;low());
523   }
524 
525   // Initialize Occupancy Map.
526   const size_t smallest_chunk_size = is_class() ? ClassSpecializedChunk : SpecializedChunk;
527   _occupancy_map = new OccupancyMap(bottom(), reserved_words(), smallest_chunk_size);
528 
529   return result;
530 }
531 
532 void VirtualSpaceNode::print_on(outputStream* st, size_t scale) const {
533   size_t used_words = used_words_in_vs();
534   size_t commit_words = committed_words();
535   size_t res_words = reserved_words();
536   VirtualSpace* vs = virtual_space();
537 
538   st-&gt;print(&quot;node @&quot; PTR_FORMAT &quot;: &quot;, p2i(this));
539   st-&gt;print(&quot;reserved=&quot;);
540   print_scaled_words(st, res_words, scale);
541   st-&gt;print(&quot;, committed=&quot;);
542   print_scaled_words_and_percentage(st, commit_words, res_words, scale);
543   st-&gt;print(&quot;, used=&quot;);
544   print_scaled_words_and_percentage(st, used_words, res_words, scale);
545   st-&gt;cr();
546   st-&gt;print(&quot;   [&quot; PTR_FORMAT &quot;, &quot; PTR_FORMAT &quot;, &quot;
547       PTR_FORMAT &quot;, &quot; PTR_FORMAT &quot;)&quot;,
548       p2i(bottom()), p2i(top()), p2i(end()),
549       p2i(vs-&gt;high_boundary()));
550 }
551 
552 #ifdef ASSERT
553 void VirtualSpaceNode::mangle() {
554   size_t word_size = capacity_words_in_vs();
555   Copy::fill_to_words((HeapWord*) low(), word_size, 0xf1f1f1f1);
556 }
557 #endif // ASSERT
558 
559 void VirtualSpaceNode::retire(ChunkManager* chunk_manager) {
560   assert(is_class() == chunk_manager-&gt;is_class(), &quot;Wrong ChunkManager?&quot;);
561 #ifdef ASSERT
562   verify(false);
563   EVERY_NTH(VerifyMetaspaceInterval)
564     verify(true);
565   END_EVERY_NTH
566 #endif
567   for (int i = (int)MediumIndex; i &gt;= (int)ZeroIndex; --i) {
568     ChunkIndex index = (ChunkIndex)i;
569     size_t chunk_size = chunk_manager-&gt;size_by_index(index);
570 
571     while (free_words_in_vs() &gt;= chunk_size) {
572       Metachunk* chunk = get_chunk_vs(chunk_size);
573       // Chunk will be allocated aligned, so allocation may require
574       // additional padding chunks. That may cause above allocation to
575       // fail. Just ignore the failed allocation and continue with the
576       // next smaller chunk size. As the VirtualSpaceNode comitted
577       // size should be a multiple of the smallest chunk size, we
578       // should always be able to fill the VirtualSpace completely.
579       if (chunk == NULL) {
580         break;
581       }
582       chunk_manager-&gt;return_single_chunk(chunk);
583     }
584   }
585   assert(free_words_in_vs() == 0, &quot;should be empty now&quot;);
586 }
587 
588 } // namespace metaspace
<a name="2" id="anc2"></a>
<a name="3" id="anc3"></a><b style="font-size: large; color: red">--- EOF ---</b>
















































































</pre>
<input id="eof" value="3" type="hidden" />
</body>
</html>