<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>New src/hotspot/cpu/s390/sharedRuntime_s390.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
  <body>
    <pre>
   1 /*
   2  * Copyright (c) 2016, 2020, Oracle and/or its affiliates. All rights reserved.
   3  * Copyright (c) 2016, 2019, SAP SE. All rights reserved.
   4  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   5  *
   6  * This code is free software; you can redistribute it and/or modify it
   7  * under the terms of the GNU General Public License version 2 only, as
   8  * published by the Free Software Foundation.
   9  *
  10  * This code is distributed in the hope that it will be useful, but WITHOUT
  11  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  12  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  13  * version 2 for more details (a copy is included in the LICENSE file that
  14  * accompanied this code).
  15  *
  16  * You should have received a copy of the GNU General Public License version
  17  * 2 along with this work; if not, write to the Free Software Foundation,
  18  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  19  *
  20  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  21  * or visit www.oracle.com if you need additional information or have any
  22  * questions.
  23  *
  24  */
  25 
  26 #include &quot;precompiled.hpp&quot;
  27 #include &quot;asm/macroAssembler.inline.hpp&quot;
  28 #include &quot;code/debugInfoRec.hpp&quot;
  29 #include &quot;code/icBuffer.hpp&quot;
  30 #include &quot;code/vtableStubs.hpp&quot;
  31 #include &quot;gc/shared/gcLocker.hpp&quot;
  32 #include &quot;interpreter/interpreter.hpp&quot;
  33 #include &quot;interpreter/interp_masm.hpp&quot;
  34 #include &quot;memory/resourceArea.hpp&quot;
  35 #include &quot;nativeInst_s390.hpp&quot;
  36 #include &quot;oops/compiledICHolder.hpp&quot;
  37 #include &quot;oops/klass.inline.hpp&quot;
  38 #include &quot;registerSaver_s390.hpp&quot;
  39 #include &quot;runtime/safepointMechanism.hpp&quot;
  40 #include &quot;runtime/sharedRuntime.hpp&quot;
  41 #include &quot;runtime/vframeArray.hpp&quot;
  42 #include &quot;utilities/align.hpp&quot;
  43 #include &quot;vmreg_s390.inline.hpp&quot;
  44 #ifdef COMPILER1
  45 #include &quot;c1/c1_Runtime1.hpp&quot;
  46 #endif
  47 #ifdef COMPILER2
  48 #include &quot;opto/ad.hpp&quot;
  49 #include &quot;opto/runtime.hpp&quot;
  50 #endif
  51 
  52 #ifdef PRODUCT
  53 #define __ masm-&gt;
  54 #else
  55 #define __ (Verbose ? (masm-&gt;block_comment(FILE_AND_LINE),masm):masm)-&gt;
  56 #endif
  57 
  58 #define BLOCK_COMMENT(str) __ block_comment(str)
  59 #define BIND(label)        bind(label); BLOCK_COMMENT(#label &quot;:&quot;)
  60 
  61 #define RegisterSaver_LiveIntReg(regname) \
  62   { RegisterSaver::int_reg,   regname-&gt;encoding(), regname-&gt;as_VMReg() }
  63 
  64 #define RegisterSaver_LiveFloatReg(regname) \
  65   { RegisterSaver::float_reg, regname-&gt;encoding(), regname-&gt;as_VMReg() }
  66 
  67 // Registers which are not saved/restored, but still they have got a frame slot.
  68 // Used to get same frame size for RegisterSaver_LiveRegs and RegisterSaver_LiveRegsWithoutR2
  69 #define RegisterSaver_ExcludedIntReg(regname) \
  70   { RegisterSaver::excluded_reg, regname-&gt;encoding(), regname-&gt;as_VMReg() }
  71 
  72 // Registers which are not saved/restored, but still they have got a frame slot.
  73 // Used to get same frame size for RegisterSaver_LiveRegs and RegisterSaver_LiveRegsWithoutR2.
  74 #define RegisterSaver_ExcludedFloatReg(regname) \
  75   { RegisterSaver::excluded_reg, regname-&gt;encoding(), regname-&gt;as_VMReg() }
  76 
  77 static const RegisterSaver::LiveRegType RegisterSaver_LiveRegs[] = {
  78   // Live registers which get spilled to the stack. Register positions
  79   // in this array correspond directly to the stack layout.
  80   //
  81   // live float registers:
  82   //
  83   RegisterSaver_LiveFloatReg(Z_F0 ),
  84   // RegisterSaver_ExcludedFloatReg(Z_F1 ), // scratch (Z_fscratch_1)
  85   RegisterSaver_LiveFloatReg(Z_F2 ),
  86   RegisterSaver_LiveFloatReg(Z_F3 ),
  87   RegisterSaver_LiveFloatReg(Z_F4 ),
  88   RegisterSaver_LiveFloatReg(Z_F5 ),
  89   RegisterSaver_LiveFloatReg(Z_F6 ),
  90   RegisterSaver_LiveFloatReg(Z_F7 ),
  91   RegisterSaver_LiveFloatReg(Z_F8 ),
  92   RegisterSaver_LiveFloatReg(Z_F9 ),
  93   RegisterSaver_LiveFloatReg(Z_F10),
  94   RegisterSaver_LiveFloatReg(Z_F11),
  95   RegisterSaver_LiveFloatReg(Z_F12),
  96   RegisterSaver_LiveFloatReg(Z_F13),
  97   RegisterSaver_LiveFloatReg(Z_F14),
  98   RegisterSaver_LiveFloatReg(Z_F15),
  99   //
 100   // RegisterSaver_ExcludedIntReg(Z_R0), // scratch
 101   // RegisterSaver_ExcludedIntReg(Z_R1), // scratch
 102   RegisterSaver_LiveIntReg(Z_R2 ),
 103   RegisterSaver_LiveIntReg(Z_R3 ),
 104   RegisterSaver_LiveIntReg(Z_R4 ),
 105   RegisterSaver_LiveIntReg(Z_R5 ),
 106   RegisterSaver_LiveIntReg(Z_R6 ),
 107   RegisterSaver_LiveIntReg(Z_R7 ),
 108   RegisterSaver_LiveIntReg(Z_R8 ),
 109   RegisterSaver_LiveIntReg(Z_R9 ),
 110   RegisterSaver_LiveIntReg(Z_R10),
 111   RegisterSaver_LiveIntReg(Z_R11),
 112   RegisterSaver_LiveIntReg(Z_R12),
 113   RegisterSaver_LiveIntReg(Z_R13),
 114   // RegisterSaver_ExcludedIntReg(Z_R14), // return pc (Saved in caller frame.)
 115   // RegisterSaver_ExcludedIntReg(Z_R15)  // stack pointer
 116 };
 117 
 118 static const RegisterSaver::LiveRegType RegisterSaver_LiveIntRegs[] = {
 119   // Live registers which get spilled to the stack. Register positions
 120   // in this array correspond directly to the stack layout.
 121   //
 122   // live float registers: All excluded, but still they get a stack slot to get same frame size.
 123   //
 124   RegisterSaver_ExcludedFloatReg(Z_F0 ),
 125   // RegisterSaver_ExcludedFloatReg(Z_F1 ), // scratch (Z_fscratch_1)
 126   RegisterSaver_ExcludedFloatReg(Z_F2 ),
 127   RegisterSaver_ExcludedFloatReg(Z_F3 ),
 128   RegisterSaver_ExcludedFloatReg(Z_F4 ),
 129   RegisterSaver_ExcludedFloatReg(Z_F5 ),
 130   RegisterSaver_ExcludedFloatReg(Z_F6 ),
 131   RegisterSaver_ExcludedFloatReg(Z_F7 ),
 132   RegisterSaver_ExcludedFloatReg(Z_F8 ),
 133   RegisterSaver_ExcludedFloatReg(Z_F9 ),
 134   RegisterSaver_ExcludedFloatReg(Z_F10),
 135   RegisterSaver_ExcludedFloatReg(Z_F11),
 136   RegisterSaver_ExcludedFloatReg(Z_F12),
 137   RegisterSaver_ExcludedFloatReg(Z_F13),
 138   RegisterSaver_ExcludedFloatReg(Z_F14),
 139   RegisterSaver_ExcludedFloatReg(Z_F15),
 140   //
 141   // RegisterSaver_ExcludedIntReg(Z_R0), // scratch
 142   // RegisterSaver_ExcludedIntReg(Z_R1), // scratch
 143   RegisterSaver_LiveIntReg(Z_R2 ),
 144   RegisterSaver_LiveIntReg(Z_R3 ),
 145   RegisterSaver_LiveIntReg(Z_R4 ),
 146   RegisterSaver_LiveIntReg(Z_R5 ),
 147   RegisterSaver_LiveIntReg(Z_R6 ),
 148   RegisterSaver_LiveIntReg(Z_R7 ),
 149   RegisterSaver_LiveIntReg(Z_R8 ),
 150   RegisterSaver_LiveIntReg(Z_R9 ),
 151   RegisterSaver_LiveIntReg(Z_R10),
 152   RegisterSaver_LiveIntReg(Z_R11),
 153   RegisterSaver_LiveIntReg(Z_R12),
 154   RegisterSaver_LiveIntReg(Z_R13),
 155   // RegisterSaver_ExcludedIntReg(Z_R14), // return pc (Saved in caller frame.)
 156   // RegisterSaver_ExcludedIntReg(Z_R15)  // stack pointer
 157 };
 158 
 159 static const RegisterSaver::LiveRegType RegisterSaver_LiveRegsWithoutR2[] = {
 160   // Live registers which get spilled to the stack. Register positions
 161   // in this array correspond directly to the stack layout.
 162   //
 163   // live float registers:
 164   //
 165   RegisterSaver_LiveFloatReg(Z_F0 ),
 166   // RegisterSaver_ExcludedFloatReg(Z_F1 ), // scratch (Z_fscratch_1)
 167   RegisterSaver_LiveFloatReg(Z_F2 ),
 168   RegisterSaver_LiveFloatReg(Z_F3 ),
 169   RegisterSaver_LiveFloatReg(Z_F4 ),
 170   RegisterSaver_LiveFloatReg(Z_F5 ),
 171   RegisterSaver_LiveFloatReg(Z_F6 ),
 172   RegisterSaver_LiveFloatReg(Z_F7 ),
 173   RegisterSaver_LiveFloatReg(Z_F8 ),
 174   RegisterSaver_LiveFloatReg(Z_F9 ),
 175   RegisterSaver_LiveFloatReg(Z_F10),
 176   RegisterSaver_LiveFloatReg(Z_F11),
 177   RegisterSaver_LiveFloatReg(Z_F12),
 178   RegisterSaver_LiveFloatReg(Z_F13),
 179   RegisterSaver_LiveFloatReg(Z_F14),
 180   RegisterSaver_LiveFloatReg(Z_F15),
 181   //
 182   // RegisterSaver_ExcludedIntReg(Z_R0), // scratch
 183   // RegisterSaver_ExcludedIntReg(Z_R1), // scratch
 184   RegisterSaver_ExcludedIntReg(Z_R2), // Omit saving R2.
 185   RegisterSaver_LiveIntReg(Z_R3 ),
 186   RegisterSaver_LiveIntReg(Z_R4 ),
 187   RegisterSaver_LiveIntReg(Z_R5 ),
 188   RegisterSaver_LiveIntReg(Z_R6 ),
 189   RegisterSaver_LiveIntReg(Z_R7 ),
 190   RegisterSaver_LiveIntReg(Z_R8 ),
 191   RegisterSaver_LiveIntReg(Z_R9 ),
 192   RegisterSaver_LiveIntReg(Z_R10),
 193   RegisterSaver_LiveIntReg(Z_R11),
 194   RegisterSaver_LiveIntReg(Z_R12),
 195   RegisterSaver_LiveIntReg(Z_R13),
 196   // RegisterSaver_ExcludedIntReg(Z_R14), // return pc (Saved in caller frame.)
 197   // RegisterSaver_ExcludedIntReg(Z_R15)  // stack pointer
 198 };
 199 
 200 // Live argument registers which get spilled to the stack.
 201 static const RegisterSaver::LiveRegType RegisterSaver_LiveArgRegs[] = {
 202   RegisterSaver_LiveFloatReg(Z_FARG1),
 203   RegisterSaver_LiveFloatReg(Z_FARG2),
 204   RegisterSaver_LiveFloatReg(Z_FARG3),
 205   RegisterSaver_LiveFloatReg(Z_FARG4),
 206   RegisterSaver_LiveIntReg(Z_ARG1),
 207   RegisterSaver_LiveIntReg(Z_ARG2),
 208   RegisterSaver_LiveIntReg(Z_ARG3),
 209   RegisterSaver_LiveIntReg(Z_ARG4),
 210   RegisterSaver_LiveIntReg(Z_ARG5)
 211 };
 212 
 213 static const RegisterSaver::LiveRegType RegisterSaver_LiveVolatileRegs[] = {
 214   // Live registers which get spilled to the stack. Register positions
 215   // in this array correspond directly to the stack layout.
 216   //
 217   // live float registers:
 218   //
 219   RegisterSaver_LiveFloatReg(Z_F0 ),
 220   // RegisterSaver_ExcludedFloatReg(Z_F1 ), // scratch (Z_fscratch_1)
 221   RegisterSaver_LiveFloatReg(Z_F2 ),
 222   RegisterSaver_LiveFloatReg(Z_F3 ),
 223   RegisterSaver_LiveFloatReg(Z_F4 ),
 224   RegisterSaver_LiveFloatReg(Z_F5 ),
 225   RegisterSaver_LiveFloatReg(Z_F6 ),
 226   RegisterSaver_LiveFloatReg(Z_F7 ),
 227   // RegisterSaver_LiveFloatReg(Z_F8 ), // non-volatile
 228   // RegisterSaver_LiveFloatReg(Z_F9 ), // non-volatile
 229   // RegisterSaver_LiveFloatReg(Z_F10), // non-volatile
 230   // RegisterSaver_LiveFloatReg(Z_F11), // non-volatile
 231   // RegisterSaver_LiveFloatReg(Z_F12), // non-volatile
 232   // RegisterSaver_LiveFloatReg(Z_F13), // non-volatile
 233   // RegisterSaver_LiveFloatReg(Z_F14), // non-volatile
 234   // RegisterSaver_LiveFloatReg(Z_F15), // non-volatile
 235   //
 236   // RegisterSaver_ExcludedIntReg(Z_R0), // scratch
 237   // RegisterSaver_ExcludedIntReg(Z_R1), // scratch
 238   RegisterSaver_LiveIntReg(Z_R2 ),
 239   RegisterSaver_LiveIntReg(Z_R3 ),
 240   RegisterSaver_LiveIntReg(Z_R4 ),
 241   RegisterSaver_LiveIntReg(Z_R5 ),
 242   // RegisterSaver_LiveIntReg(Z_R6 ), // non-volatile
 243   // RegisterSaver_LiveIntReg(Z_R7 ), // non-volatile
 244   // RegisterSaver_LiveIntReg(Z_R8 ), // non-volatile
 245   // RegisterSaver_LiveIntReg(Z_R9 ), // non-volatile
 246   // RegisterSaver_LiveIntReg(Z_R10), // non-volatile
 247   // RegisterSaver_LiveIntReg(Z_R11), // non-volatile
 248   // RegisterSaver_LiveIntReg(Z_R12), // non-volatile
 249   // RegisterSaver_LiveIntReg(Z_R13), // non-volatile
 250   // RegisterSaver_ExcludedIntReg(Z_R14), // return pc (Saved in caller frame.)
 251   // RegisterSaver_ExcludedIntReg(Z_R15)  // stack pointer
 252 };
 253 
 254 int RegisterSaver::live_reg_save_size(RegisterSet reg_set) {
 255   int reg_space = -1;
 256   switch (reg_set) {
 257     case all_registers:           reg_space = sizeof(RegisterSaver_LiveRegs); break;
 258     case all_registers_except_r2: reg_space = sizeof(RegisterSaver_LiveRegsWithoutR2); break;
 259     case all_integer_registers:   reg_space = sizeof(RegisterSaver_LiveIntRegs); break;
 260     case all_volatile_registers:  reg_space = sizeof(RegisterSaver_LiveVolatileRegs); break;
 261     case arg_registers:           reg_space = sizeof(RegisterSaver_LiveArgRegs); break;
 262     default: ShouldNotReachHere();
 263   }
 264   return (reg_space / sizeof(RegisterSaver::LiveRegType)) * reg_size;
 265 }
 266 
 267 
 268 int RegisterSaver::live_reg_frame_size(RegisterSet reg_set) {
 269   return live_reg_save_size(reg_set) + frame::z_abi_160_size;
 270 }
 271 
 272 
 273 // return_pc: Specify the register that should be stored as the return pc in the current frame.
 274 OopMap* RegisterSaver::save_live_registers(MacroAssembler* masm, RegisterSet reg_set, Register return_pc) {
 275   // Record volatile registers as callee-save values in an OopMap so
 276   // their save locations will be propagated to the caller frame&#39;s
 277   // RegisterMap during StackFrameStream construction (needed for
 278   // deoptimization; see compiledVFrame::create_stack_value).
 279 
 280   // Calculate frame size.
 281   const int frame_size_in_bytes  = live_reg_frame_size(reg_set);
 282   const int frame_size_in_slots  = frame_size_in_bytes / sizeof(jint);
 283   const int register_save_offset = frame_size_in_bytes - live_reg_save_size(reg_set);
 284 
 285   // OopMap frame size is in c2 stack slots (sizeof(jint)) not bytes or words.
 286   OopMap* map = new OopMap(frame_size_in_slots, 0);
 287 
 288   int regstosave_num = 0;
 289   const RegisterSaver::LiveRegType* live_regs = NULL;
 290 
 291   switch (reg_set) {
 292     case all_registers:
 293       regstosave_num = sizeof(RegisterSaver_LiveRegs)/sizeof(RegisterSaver::LiveRegType);
 294       live_regs      = RegisterSaver_LiveRegs;
 295       break;
 296     case all_registers_except_r2:
 297       regstosave_num = sizeof(RegisterSaver_LiveRegsWithoutR2)/sizeof(RegisterSaver::LiveRegType);;
 298       live_regs      = RegisterSaver_LiveRegsWithoutR2;
 299       break;
 300     case all_integer_registers:
 301       regstosave_num = sizeof(RegisterSaver_LiveIntRegs)/sizeof(RegisterSaver::LiveRegType);
 302       live_regs      = RegisterSaver_LiveIntRegs;
 303       break;
 304     case all_volatile_registers:
 305       regstosave_num = sizeof(RegisterSaver_LiveVolatileRegs)/sizeof(RegisterSaver::LiveRegType);
 306       live_regs      = RegisterSaver_LiveVolatileRegs;
 307       break;
 308     case arg_registers:
 309       regstosave_num = sizeof(RegisterSaver_LiveArgRegs)/sizeof(RegisterSaver::LiveRegType);;
 310       live_regs      = RegisterSaver_LiveArgRegs;
 311       break;
 312     default: ShouldNotReachHere();
 313   }
 314 
 315   // Save return pc in old frame.
 316   __ save_return_pc(return_pc);
 317 
 318   // Push a new frame (includes stack linkage).
 319   // Use return_pc as scratch for push_frame. Z_R0_scratch (the default) and Z_R1_scratch are
 320   // illegally used to pass parameters by RangeCheckStub::emit_code().
 321   __ push_frame(frame_size_in_bytes, return_pc);
 322   // We have to restore return_pc right away.
 323   // Nobody else will. Furthermore, return_pc isn&#39;t necessarily the default (Z_R14).
 324   // Nobody else knows which register we saved.
 325   __ z_lg(return_pc, _z_abi16(return_pc) + frame_size_in_bytes, Z_SP);
 326 
 327   // Register save area in new frame starts above z_abi_160 area.
 328   int offset = register_save_offset;
 329 
 330   Register first = noreg;
 331   Register last  = noreg;
 332   int      first_offset = -1;
 333   bool     float_spilled = false;
 334 
 335   for (int i = 0; i &lt; regstosave_num; i++, offset += reg_size) {
 336     int reg_num  = live_regs[i].reg_num;
 337     int reg_type = live_regs[i].reg_type;
 338 
 339     switch (reg_type) {
 340       case RegisterSaver::int_reg: {
 341         Register reg = as_Register(reg_num);
 342         if (last != reg-&gt;predecessor()) {
 343           if (first != noreg) {
 344             __ z_stmg(first, last, first_offset, Z_SP);
 345           }
 346           first = reg;
 347           first_offset = offset;
 348           DEBUG_ONLY(float_spilled = false);
 349         }
 350         last = reg;
 351         assert(last != Z_R0, &quot;r0 would require special treatment&quot;);
 352         assert(!float_spilled, &quot;for simplicity, do not mix up ints and floats in RegisterSaver_LiveRegs[]&quot;);
 353         break;
 354       }
 355 
 356       case RegisterSaver::excluded_reg: // Not saved/restored, but with dedicated slot.
 357         continue; // Continue with next loop iteration.
 358 
 359       case RegisterSaver::float_reg: {
 360         FloatRegister freg = as_FloatRegister(reg_num);
 361         __ z_std(freg, offset, Z_SP);
 362         DEBUG_ONLY(float_spilled = true);
 363         break;
 364       }
 365 
 366       default:
 367         ShouldNotReachHere();
 368         break;
 369     }
 370 
 371     // Second set_callee_saved is really a waste but we&#39;ll keep things as they were for now
 372     map-&gt;set_callee_saved(VMRegImpl::stack2reg(offset &gt;&gt; 2), live_regs[i].vmreg);
 373     map-&gt;set_callee_saved(VMRegImpl::stack2reg((offset + half_reg_size) &gt;&gt; 2), live_regs[i].vmreg-&gt;next());
 374   }
 375   assert(first != noreg, &quot;Should spill at least one int reg.&quot;);
 376   __ z_stmg(first, last, first_offset, Z_SP);
 377 
 378   // And we&#39;re done.
 379   return map;
 380 }
 381 
 382 
 383 // Generate the OopMap (again, regs where saved before).
 384 OopMap* RegisterSaver::generate_oop_map(MacroAssembler* masm, RegisterSet reg_set) {
 385   // Calculate frame size.
 386   const int frame_size_in_bytes  = live_reg_frame_size(reg_set);
 387   const int frame_size_in_slots  = frame_size_in_bytes / sizeof(jint);
 388   const int register_save_offset = frame_size_in_bytes - live_reg_save_size(reg_set);
 389 
 390   // OopMap frame size is in c2 stack slots (sizeof(jint)) not bytes or words.
 391   OopMap* map = new OopMap(frame_size_in_slots, 0);
 392 
 393   int regstosave_num = 0;
 394   const RegisterSaver::LiveRegType* live_regs = NULL;
 395 
 396   switch (reg_set) {
 397     case all_registers:
 398       regstosave_num = sizeof(RegisterSaver_LiveRegs)/sizeof(RegisterSaver::LiveRegType);
 399       live_regs      = RegisterSaver_LiveRegs;
 400       break;
 401     case all_registers_except_r2:
 402       regstosave_num = sizeof(RegisterSaver_LiveRegsWithoutR2)/sizeof(RegisterSaver::LiveRegType);;
 403       live_regs      = RegisterSaver_LiveRegsWithoutR2;
 404       break;
 405     case all_integer_registers:
 406       regstosave_num = sizeof(RegisterSaver_LiveIntRegs)/sizeof(RegisterSaver::LiveRegType);
 407       live_regs      = RegisterSaver_LiveIntRegs;
 408       break;
 409     case all_volatile_registers:
 410       regstosave_num = sizeof(RegisterSaver_LiveVolatileRegs)/sizeof(RegisterSaver::LiveRegType);
 411       live_regs      = RegisterSaver_LiveVolatileRegs;
 412       break;
 413     case arg_registers:
 414       regstosave_num = sizeof(RegisterSaver_LiveArgRegs)/sizeof(RegisterSaver::LiveRegType);;
 415       live_regs      = RegisterSaver_LiveArgRegs;
 416       break;
 417     default: ShouldNotReachHere();
 418   }
 419 
 420   // Register save area in new frame starts above z_abi_160 area.
 421   int offset = register_save_offset;
 422   for (int i = 0; i &lt; regstosave_num; i++) {
 423     if (live_regs[i].reg_type &lt; RegisterSaver::excluded_reg) {
 424       map-&gt;set_callee_saved(VMRegImpl::stack2reg(offset&gt;&gt;2), live_regs[i].vmreg);
 425       map-&gt;set_callee_saved(VMRegImpl::stack2reg((offset + half_reg_size)&gt;&gt;2), live_regs[i].vmreg-&gt;next());
 426     }
 427     offset += reg_size;
 428   }
 429   return map;
 430 }
 431 
 432 
 433 // Pop the current frame and restore all the registers that we saved.
 434 void RegisterSaver::restore_live_registers(MacroAssembler* masm, RegisterSet reg_set) {
 435   int offset;
 436   const int register_save_offset = live_reg_frame_size(reg_set) - live_reg_save_size(reg_set);
 437 
 438   Register first = noreg;
 439   Register last = noreg;
 440   int      first_offset = -1;
 441   bool     float_spilled = false;
 442 
 443   int regstosave_num = 0;
 444   const RegisterSaver::LiveRegType* live_regs = NULL;
 445 
 446   switch (reg_set) {
 447     case all_registers:
 448       regstosave_num = sizeof(RegisterSaver_LiveRegs)/sizeof(RegisterSaver::LiveRegType);;
 449       live_regs      = RegisterSaver_LiveRegs;
 450       break;
 451     case all_registers_except_r2:
 452       regstosave_num = sizeof(RegisterSaver_LiveRegsWithoutR2)/sizeof(RegisterSaver::LiveRegType);;
 453       live_regs      = RegisterSaver_LiveRegsWithoutR2;
 454       break;
 455     case all_integer_registers:
 456       regstosave_num = sizeof(RegisterSaver_LiveIntRegs)/sizeof(RegisterSaver::LiveRegType);
 457       live_regs      = RegisterSaver_LiveIntRegs;
 458       break;
 459     case all_volatile_registers:
 460       regstosave_num = sizeof(RegisterSaver_LiveVolatileRegs)/sizeof(RegisterSaver::LiveRegType);;
 461       live_regs      = RegisterSaver_LiveVolatileRegs;
 462       break;
 463     case arg_registers:
 464       regstosave_num = sizeof(RegisterSaver_LiveArgRegs)/sizeof(RegisterSaver::LiveRegType);;
 465       live_regs      = RegisterSaver_LiveArgRegs;
 466       break;
 467     default: ShouldNotReachHere();
 468   }
 469 
 470   // Restore all registers (ints and floats).
 471 
 472   // Register save area in new frame starts above z_abi_160 area.
 473   offset = register_save_offset;
 474 
 475   for (int i = 0; i &lt; regstosave_num; i++, offset += reg_size) {
 476     int reg_num  = live_regs[i].reg_num;
 477     int reg_type = live_regs[i].reg_type;
 478 
 479     switch (reg_type) {
 480       case RegisterSaver::excluded_reg:
 481         continue; // Continue with next loop iteration.
 482 
 483       case RegisterSaver::int_reg: {
 484         Register reg = as_Register(reg_num);
 485         if (last != reg-&gt;predecessor()) {
 486           if (first != noreg) {
 487             __ z_lmg(first, last, first_offset, Z_SP);
 488           }
 489           first = reg;
 490           first_offset = offset;
 491           DEBUG_ONLY(float_spilled = false);
 492         }
 493         last = reg;
 494         assert(last != Z_R0, &quot;r0 would require special treatment&quot;);
 495         assert(!float_spilled, &quot;for simplicity, do not mix up ints and floats in RegisterSaver_LiveRegs[]&quot;);
 496         break;
 497       }
 498 
 499       case RegisterSaver::float_reg: {
 500         FloatRegister freg = as_FloatRegister(reg_num);
 501         __ z_ld(freg, offset, Z_SP);
 502         DEBUG_ONLY(float_spilled = true);
 503         break;
 504       }
 505 
 506       default:
 507         ShouldNotReachHere();
 508     }
 509   }
 510   assert(first != noreg, &quot;Should spill at least one int reg.&quot;);
 511   __ z_lmg(first, last, first_offset, Z_SP);
 512 
 513   // Pop the frame.
 514   __ pop_frame();
 515 
 516   // Restore the flags.
 517   __ restore_return_pc();
 518 }
 519 
 520 
 521 // Pop the current frame and restore the registers that might be holding a result.
 522 void RegisterSaver::restore_result_registers(MacroAssembler* masm) {
 523   int i;
 524   int offset;
 525   const int regstosave_num       = sizeof(RegisterSaver_LiveRegs) /
 526                                    sizeof(RegisterSaver::LiveRegType);
 527   const int register_save_offset = live_reg_frame_size(all_registers) - live_reg_save_size(all_registers);
 528 
 529   // Restore all result registers (ints and floats).
 530   offset = register_save_offset;
 531   for (int i = 0; i &lt; regstosave_num; i++, offset += reg_size) {
 532     int reg_num = RegisterSaver_LiveRegs[i].reg_num;
 533     int reg_type = RegisterSaver_LiveRegs[i].reg_type;
 534     switch (reg_type) {
 535       case RegisterSaver::excluded_reg:
 536         continue; // Continue with next loop iteration.
 537       case RegisterSaver::int_reg: {
 538         if (as_Register(reg_num) == Z_RET) { // int result_reg
 539           __ z_lg(as_Register(reg_num), offset, Z_SP);
 540         }
 541         break;
 542       }
 543       case RegisterSaver::float_reg: {
 544         if (as_FloatRegister(reg_num) == Z_FRET) { // float result_reg
 545           __ z_ld(as_FloatRegister(reg_num), offset, Z_SP);
 546         }
 547         break;
 548       }
 549       default:
 550         ShouldNotReachHere();
 551     }
 552   }
 553 }
 554 
 555 size_t SharedRuntime::trampoline_size() {
 556   return MacroAssembler::load_const_size() + 2;
 557 }
 558 
 559 void SharedRuntime::generate_trampoline(MacroAssembler *masm, address destination) {
 560   // Think about using pc-relative branch.
 561   __ load_const(Z_R1_scratch, destination);
 562   __ z_br(Z_R1_scratch);
 563 }
 564 
 565 // ---------------------------------------------------------------------------
 566 void SharedRuntime::save_native_result(MacroAssembler * masm,
 567                                        BasicType ret_type,
 568                                        int frame_slots) {
 569   Address memaddr(Z_SP, frame_slots * VMRegImpl::stack_slot_size);
 570 
 571   switch (ret_type) {
 572     case T_BOOLEAN:  // Save shorter types as int. Do we need sign extension at restore??
 573     case T_BYTE:
 574     case T_CHAR:
 575     case T_SHORT:
 576     case T_INT:
 577       __ reg2mem_opt(Z_RET, memaddr, false);
 578       break;
 579     case T_OBJECT:   // Save pointer types as long.
 580     case T_ARRAY:
 581     case T_ADDRESS:
 582     case T_VOID:
 583     case T_LONG:
 584       __ reg2mem_opt(Z_RET, memaddr);
 585       break;
 586     case T_FLOAT:
 587       __ freg2mem_opt(Z_FRET, memaddr, false);
 588       break;
 589     case T_DOUBLE:
 590       __ freg2mem_opt(Z_FRET, memaddr);
 591       break;
 592     default:
 593       ShouldNotReachHere();
 594       break;
 595   }
 596 }
 597 
 598 void SharedRuntime::restore_native_result(MacroAssembler *masm,
 599                                           BasicType       ret_type,
 600                                           int             frame_slots) {
 601   Address memaddr(Z_SP, frame_slots * VMRegImpl::stack_slot_size);
 602 
 603   switch (ret_type) {
 604     case T_BOOLEAN:  // Restore shorter types as int. Do we need sign extension at restore??
 605     case T_BYTE:
 606     case T_CHAR:
 607     case T_SHORT:
 608     case T_INT:
 609       __ mem2reg_opt(Z_RET, memaddr, false);
 610       break;
 611     case T_OBJECT:   // Restore pointer types as long.
 612     case T_ARRAY:
 613     case T_ADDRESS:
 614     case T_VOID:
 615     case T_LONG:
 616       __ mem2reg_opt(Z_RET, memaddr);
 617       break;
 618     case T_FLOAT:
 619       __ mem2freg_opt(Z_FRET, memaddr, false);
 620       break;
 621     case T_DOUBLE:
 622       __ mem2freg_opt(Z_FRET, memaddr);
 623       break;
 624     default:
 625       ShouldNotReachHere();
 626       break;
 627   }
 628 }
 629 
 630 // ---------------------------------------------------------------------------
 631 // Read the array of BasicTypes from a signature, and compute where the
 632 // arguments should go. Values in the VMRegPair regs array refer to 4-byte
 633 // quantities. Values less than VMRegImpl::stack0 are registers, those above
 634 // refer to 4-byte stack slots. All stack slots are based off of the stack pointer
 635 // as framesizes are fixed.
 636 // VMRegImpl::stack0 refers to the first slot 0(sp).
 637 // VMRegImpl::stack0+1 refers to the memory word 4-byes higher. Registers
 638 // up to RegisterImpl::number_of_registers are the 64-bit integer registers.
 639 
 640 // Note: the INPUTS in sig_bt are in units of Java argument words, which are
 641 // either 32-bit or 64-bit depending on the build. The OUTPUTS are in 32-bit
 642 // units regardless of build.
 643 
 644 // The Java calling convention is a &quot;shifted&quot; version of the C ABI.
 645 // By skipping the first C ABI register we can call non-static jni methods
 646 // with small numbers of arguments without having to shuffle the arguments
 647 // at all. Since we control the java ABI we ought to at least get some
 648 // advantage out of it.
 649 int SharedRuntime::java_calling_convention(const BasicType *sig_bt,
 650                                            VMRegPair *regs,
 651                                            int total_args_passed,
 652                                            int is_outgoing) {
 653   // c2c calling conventions for compiled-compiled calls.
 654 
 655   // An int/float occupies 1 slot here.
 656   const int inc_stk_for_intfloat   = 1; // 1 slots for ints and floats.
 657   const int inc_stk_for_longdouble = 2; // 2 slots for longs and doubles.
 658 
 659   const VMReg z_iarg_reg[5] = {
 660     Z_R2-&gt;as_VMReg(),
 661     Z_R3-&gt;as_VMReg(),
 662     Z_R4-&gt;as_VMReg(),
 663     Z_R5-&gt;as_VMReg(),
 664     Z_R6-&gt;as_VMReg()
 665   };
 666   const VMReg z_farg_reg[4] = {
 667     Z_F0-&gt;as_VMReg(),
 668     Z_F2-&gt;as_VMReg(),
 669     Z_F4-&gt;as_VMReg(),
 670     Z_F6-&gt;as_VMReg()
 671   };
 672   const int z_num_iarg_registers = sizeof(z_iarg_reg) / sizeof(z_iarg_reg[0]);
 673   const int z_num_farg_registers = sizeof(z_farg_reg) / sizeof(z_farg_reg[0]);
 674 
 675   assert(RegisterImpl::number_of_arg_registers == z_num_iarg_registers, &quot;iarg reg count mismatch&quot;);
 676   assert(FloatRegisterImpl::number_of_arg_registers == z_num_farg_registers, &quot;farg reg count mismatch&quot;);
 677 
 678   int i;
 679   int stk = 0;
 680   int ireg = 0;
 681   int freg = 0;
 682 
 683   for (int i = 0; i &lt; total_args_passed; ++i) {
 684     switch (sig_bt[i]) {
 685       case T_BOOLEAN:
 686       case T_CHAR:
 687       case T_BYTE:
 688       case T_SHORT:
 689       case T_INT:
 690         if (ireg &lt; z_num_iarg_registers) {
 691           // Put int/ptr in register.
 692           regs[i].set1(z_iarg_reg[ireg]);
 693           ++ireg;
 694         } else {
 695           // Put int/ptr on stack.
 696           regs[i].set1(VMRegImpl::stack2reg(stk));
 697           stk += inc_stk_for_intfloat;
 698         }
 699         break;
 700       case T_LONG:
 701         assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i+1] == T_VOID, &quot;expecting half&quot;);
 702         if (ireg &lt; z_num_iarg_registers) {
 703           // Put long in register.
 704           regs[i].set2(z_iarg_reg[ireg]);
 705           ++ireg;
 706         } else {
 707           // Put long on stack and align to 2 slots.
 708           if (stk &amp; 0x1) { ++stk; }
 709           regs[i].set2(VMRegImpl::stack2reg(stk));
 710           stk += inc_stk_for_longdouble;
 711         }
 712         break;
 713       case T_OBJECT:
 714       case T_ARRAY:
 715       case T_ADDRESS:
 716         if (ireg &lt; z_num_iarg_registers) {
 717           // Put ptr in register.
 718           regs[i].set2(z_iarg_reg[ireg]);
 719           ++ireg;
 720         } else {
 721           // Put ptr on stack and align to 2 slots, because
 722           // &quot;64-bit pointers record oop-ishness on 2 aligned adjacent
 723           // registers.&quot; (see OopFlow::build_oop_map).
 724           if (stk &amp; 0x1) { ++stk; }
 725           regs[i].set2(VMRegImpl::stack2reg(stk));
 726           stk += inc_stk_for_longdouble;
 727         }
 728         break;
 729       case T_FLOAT:
 730         if (freg &lt; z_num_farg_registers) {
 731           // Put float in register.
 732           regs[i].set1(z_farg_reg[freg]);
 733           ++freg;
 734         } else {
 735           // Put float on stack.
 736           regs[i].set1(VMRegImpl::stack2reg(stk));
 737           stk += inc_stk_for_intfloat;
 738         }
 739         break;
 740       case T_DOUBLE:
 741         assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i+1] == T_VOID, &quot;expecting half&quot;);
 742         if (freg &lt; z_num_farg_registers) {
 743           // Put double in register.
 744           regs[i].set2(z_farg_reg[freg]);
 745           ++freg;
 746         } else {
 747           // Put double on stack and align to 2 slots.
 748           if (stk &amp; 0x1) { ++stk; }
 749           regs[i].set2(VMRegImpl::stack2reg(stk));
 750           stk += inc_stk_for_longdouble;
 751         }
 752         break;
 753       case T_VOID:
 754         assert(i != 0 &amp;&amp; (sig_bt[i - 1] == T_LONG || sig_bt[i - 1] == T_DOUBLE), &quot;expecting half&quot;);
 755         // Do not count halves.
 756         regs[i].set_bad();
 757         break;
 758       default:
 759         ShouldNotReachHere();
 760     }
 761   }
 762   return align_up(stk, 2);
 763 }
 764 
 765 int SharedRuntime::c_calling_convention(const BasicType *sig_bt,
 766                                         VMRegPair *regs,
 767                                         VMRegPair *regs2,
 768                                         int total_args_passed) {
 769   assert(regs2 == NULL, &quot;second VMRegPair array not used on this platform&quot;);
 770 
 771   // Calling conventions for C runtime calls and calls to JNI native methods.
 772   const VMReg z_iarg_reg[5] = {
 773     Z_R2-&gt;as_VMReg(),
 774     Z_R3-&gt;as_VMReg(),
 775     Z_R4-&gt;as_VMReg(),
 776     Z_R5-&gt;as_VMReg(),
 777     Z_R6-&gt;as_VMReg()
 778   };
 779   const VMReg z_farg_reg[4] = {
 780     Z_F0-&gt;as_VMReg(),
 781     Z_F2-&gt;as_VMReg(),
 782     Z_F4-&gt;as_VMReg(),
 783     Z_F6-&gt;as_VMReg()
 784   };
 785   const int z_num_iarg_registers = sizeof(z_iarg_reg) / sizeof(z_iarg_reg[0]);
 786   const int z_num_farg_registers = sizeof(z_farg_reg) / sizeof(z_farg_reg[0]);
 787 
 788   // Check calling conventions consistency.
 789   assert(RegisterImpl::number_of_arg_registers == z_num_iarg_registers, &quot;iarg reg count mismatch&quot;);
 790   assert(FloatRegisterImpl::number_of_arg_registers == z_num_farg_registers, &quot;farg reg count mismatch&quot;);
 791 
 792   // Avoid passing C arguments in the wrong stack slots.
 793 
 794   // &#39;Stk&#39; counts stack slots. Due to alignment, 32 bit values occupy
 795   // 2 such slots, like 64 bit values do.
 796   const int inc_stk_for_intfloat   = 2; // 2 slots for ints and floats.
 797   const int inc_stk_for_longdouble = 2; // 2 slots for longs and doubles.
 798 
 799   int i;
 800   // Leave room for C-compatible ABI
 801   int stk = (frame::z_abi_160_size - frame::z_jit_out_preserve_size) / VMRegImpl::stack_slot_size;
 802   int freg = 0;
 803   int ireg = 0;
 804 
 805   // We put the first 5 arguments into registers and the rest on the
 806   // stack. Float arguments are already in their argument registers
 807   // due to c2c calling conventions (see calling_convention).
 808   for (int i = 0; i &lt; total_args_passed; ++i) {
 809     switch (sig_bt[i]) {
 810       case T_BOOLEAN:
 811       case T_CHAR:
 812       case T_BYTE:
 813       case T_SHORT:
 814       case T_INT:
 815         // Fall through, handle as long.
 816       case T_LONG:
 817       case T_OBJECT:
 818       case T_ARRAY:
 819       case T_ADDRESS:
 820       case T_METADATA:
 821         // Oops are already boxed if required (JNI).
 822         if (ireg &lt; z_num_iarg_registers) {
 823           regs[i].set2(z_iarg_reg[ireg]);
 824           ++ireg;
 825         } else {
 826           regs[i].set2(VMRegImpl::stack2reg(stk));
 827           stk += inc_stk_for_longdouble;
 828         }
 829         break;
 830       case T_FLOAT:
 831         if (freg &lt; z_num_farg_registers) {
 832           regs[i].set1(z_farg_reg[freg]);
 833           ++freg;
 834         } else {
 835           regs[i].set1(VMRegImpl::stack2reg(stk+1));
 836           stk +=  inc_stk_for_intfloat;
 837         }
 838         break;
 839       case T_DOUBLE:
 840         assert((i + 1) &lt; total_args_passed &amp;&amp; sig_bt[i+1] == T_VOID, &quot;expecting half&quot;);
 841         if (freg &lt; z_num_farg_registers) {
 842           regs[i].set2(z_farg_reg[freg]);
 843           ++freg;
 844         } else {
 845           // Put double on stack.
 846           regs[i].set2(VMRegImpl::stack2reg(stk));
 847           stk += inc_stk_for_longdouble;
 848         }
 849         break;
 850       case T_VOID:
 851         // Do not count halves.
 852         regs[i].set_bad();
 853         break;
 854       default:
 855         ShouldNotReachHere();
 856     }
 857   }
 858   return align_up(stk, 2);
 859 }
 860 
 861 ////////////////////////////////////////////////////////////////////////
 862 //
 863 //  Argument shufflers
 864 //
 865 ////////////////////////////////////////////////////////////////////////
 866 
 867 //----------------------------------------------------------------------
 868 // The java_calling_convention describes stack locations as ideal slots on
 869 // a frame with no abi restrictions. Since we must observe abi restrictions
 870 // (like the placement of the register window) the slots must be biased by
 871 // the following value.
 872 //----------------------------------------------------------------------
 873 static int reg2slot(VMReg r) {
 874   return r-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
 875 }
 876 
 877 static int reg2offset(VMReg r) {
 878   return reg2slot(r) * VMRegImpl::stack_slot_size;
 879 }
 880 
 881 static void verify_oop_args(MacroAssembler *masm,
 882                             int total_args_passed,
 883                             const BasicType *sig_bt,
 884                             const VMRegPair *regs) {
 885   if (!VerifyOops) { return; }
 886 
 887   for (int i = 0; i &lt; total_args_passed; i++) {
 888     if (is_reference_type(sig_bt[i])) {
 889       VMReg r = regs[i].first();
 890       assert(r-&gt;is_valid(), &quot;bad oop arg&quot;);
 891 
 892       if (r-&gt;is_stack()) {
 893         __ z_lg(Z_R0_scratch,
 894                 Address(Z_SP, r-&gt;reg2stack() * VMRegImpl::stack_slot_size + wordSize));
 895         __ verify_oop(Z_R0_scratch, FILE_AND_LINE);
 896       } else {
 897         __ verify_oop(r-&gt;as_Register(), FILE_AND_LINE);
 898       }
 899     }
 900   }
 901 }
 902 
 903 static void gen_special_dispatch(MacroAssembler *masm,
 904                                  int total_args_passed,
 905                                  vmIntrinsics::ID special_dispatch,
 906                                  const BasicType *sig_bt,
 907                                  const VMRegPair *regs) {
 908   verify_oop_args(masm, total_args_passed, sig_bt, regs);
 909 
 910   // Now write the args into the outgoing interpreter space.
 911   bool     has_receiver   = false;
 912   Register receiver_reg   = noreg;
 913   int      member_arg_pos = -1;
 914   Register member_reg     = noreg;
 915   int      ref_kind       = MethodHandles::signature_polymorphic_intrinsic_ref_kind(special_dispatch);
 916 
 917   if (ref_kind != 0) {
 918     member_arg_pos = total_args_passed - 1;  // trailing MemberName argument
 919     member_reg = Z_R9;                       // Known to be free at this point.
 920     has_receiver = MethodHandles::ref_kind_has_receiver(ref_kind);
 921   } else {
 922     guarantee(special_dispatch == vmIntrinsics::_invokeBasic, &quot;special_dispatch=%d&quot;, special_dispatch);
 923     has_receiver = true;
 924   }
 925 
 926   if (member_reg != noreg) {
 927     // Load the member_arg into register, if necessary.
 928     assert(member_arg_pos &gt;= 0 &amp;&amp; member_arg_pos &lt; total_args_passed, &quot;oob&quot;);
 929     assert(sig_bt[member_arg_pos] == T_OBJECT, &quot;dispatch argument must be an object&quot;);
 930 
 931     VMReg r = regs[member_arg_pos].first();
 932     assert(r-&gt;is_valid(), &quot;bad member arg&quot;);
 933 
 934     if (r-&gt;is_stack()) {
 935       __ z_lg(member_reg, Address(Z_SP, reg2offset(r)));
 936     } else {
 937       // No data motion is needed.
 938       member_reg = r-&gt;as_Register();
 939     }
 940   }
 941 
 942   if (has_receiver) {
 943     // Make sure the receiver is loaded into a register.
 944     assert(total_args_passed &gt; 0, &quot;oob&quot;);
 945     assert(sig_bt[0] == T_OBJECT, &quot;receiver argument must be an object&quot;);
 946 
 947     VMReg r = regs[0].first();
 948     assert(r-&gt;is_valid(), &quot;bad receiver arg&quot;);
 949 
 950     if (r-&gt;is_stack()) {
 951       // Porting note: This assumes that compiled calling conventions always
 952       // pass the receiver oop in a register. If this is not true on some
 953       // platform, pick a temp and load the receiver from stack.
 954       assert(false, &quot;receiver always in a register&quot;);
 955       receiver_reg = Z_R13;  // Known to be free at this point.
 956       __ z_lg(receiver_reg, Address(Z_SP, reg2offset(r)));
 957     } else {
 958       // No data motion is needed.
 959       receiver_reg = r-&gt;as_Register();
 960     }
 961   }
 962 
 963   // Figure out which address we are really jumping to:
 964   MethodHandles::generate_method_handle_dispatch(masm, special_dispatch,
 965                                                  receiver_reg, member_reg,
 966                                                  /*for_compiler_entry:*/ true);
 967 }
 968 
 969 ////////////////////////////////////////////////////////////////////////
 970 //
 971 //  Argument shufflers
 972 //
 973 ////////////////////////////////////////////////////////////////////////
 974 
 975 // Is the size of a vector size (in bytes) bigger than a size saved by default?
 976 // 8 bytes registers are saved by default on z/Architecture.
 977 bool SharedRuntime::is_wide_vector(int size) {
 978   // Note, MaxVectorSize == 8 on this platform.
 979   assert(size &lt;= 8, &quot;%d bytes vectors are not supported&quot;, size);
 980   return size &gt; 8;
 981 }
 982 
 983 //----------------------------------------------------------------------
 984 // An oop arg. Must pass a handle not the oop itself
 985 //----------------------------------------------------------------------
 986 static void object_move(MacroAssembler *masm,
 987                         OopMap *map,
 988                         int oop_handle_offset,
 989                         int framesize_in_slots,
 990                         VMRegPair src,
 991                         VMRegPair dst,
 992                         bool is_receiver,
 993                         int *receiver_offset) {
 994   int frame_offset = framesize_in_slots*VMRegImpl::stack_slot_size;
 995 
 996   assert(!is_receiver || (is_receiver &amp;&amp; (*receiver_offset == -1)), &quot;only one receiving object per call, please.&quot;);
 997 
 998   // Must pass a handle. First figure out the location we use as a handle.
 999 
1000   if (src.first()-&gt;is_stack()) {
1001     // Oop is already on the stack, put handle on stack or in register
1002     // If handle will be on the stack, use temp reg to calculate it.
1003     Register rHandle = dst.first()-&gt;is_stack() ? Z_R1 : dst.first()-&gt;as_Register();
1004     Label    skip;
1005     int      slot_in_older_frame = reg2slot(src.first());
1006 
1007     guarantee(!is_receiver, &quot;expecting receiver in register&quot;);
1008     map-&gt;set_oop(VMRegImpl::stack2reg(slot_in_older_frame + framesize_in_slots));
1009 
1010     __ add2reg(rHandle, reg2offset(src.first())+frame_offset, Z_SP);
1011     __ load_and_test_long(Z_R0, Address(rHandle));
1012     __ z_brne(skip);
1013     // Use a NULL handle if oop is NULL.
1014     __ clear_reg(rHandle, true, false);
1015     __ bind(skip);
1016 
1017     // Copy handle to the right place (register or stack).
1018     if (dst.first()-&gt;is_stack()) {
1019       __ z_stg(rHandle, reg2offset(dst.first()), Z_SP);
1020     } // else
1021       // nothing to do. rHandle uses the correct register
1022   } else {
1023     // Oop is passed in an input register. We must flush it to the stack.
1024     const Register rOop = src.first()-&gt;as_Register();
1025     const Register rHandle = dst.first()-&gt;is_stack() ? Z_R1 : dst.first()-&gt;as_Register();
1026     int            oop_slot = (rOop-&gt;encoding()-Z_ARG1-&gt;encoding()) * VMRegImpl::slots_per_word + oop_handle_offset;
1027     int            oop_slot_offset = oop_slot*VMRegImpl::stack_slot_size;
1028     NearLabel skip;
1029 
1030     if (is_receiver) {
1031       *receiver_offset = oop_slot_offset;
1032     }
1033     map-&gt;set_oop(VMRegImpl::stack2reg(oop_slot));
1034 
1035     // Flush Oop to stack, calculate handle.
1036     __ z_stg(rOop, oop_slot_offset, Z_SP);
1037     __ add2reg(rHandle, oop_slot_offset, Z_SP);
1038 
1039     // If Oop == NULL, use a NULL handle.
1040     __ compare64_and_branch(rOop, (RegisterOrConstant)0L, Assembler::bcondNotEqual, skip);
1041     __ clear_reg(rHandle, true, false);
1042     __ bind(skip);
1043 
1044     // Copy handle to the right place (register or stack).
1045     if (dst.first()-&gt;is_stack()) {
1046       __ z_stg(rHandle, reg2offset(dst.first()), Z_SP);
1047     } // else
1048       // nothing to do here, since rHandle = dst.first()-&gt;as_Register in this case.
1049   }
1050 }
1051 
1052 //----------------------------------------------------------------------
1053 // A float arg. May have to do float reg to int reg conversion
1054 //----------------------------------------------------------------------
1055 static void float_move(MacroAssembler *masm,
1056                        VMRegPair src,
1057                        VMRegPair dst,
1058                        int framesize_in_slots,
1059                        int workspace_slot_offset) {
1060   int frame_offset = framesize_in_slots * VMRegImpl::stack_slot_size;
1061   int workspace_offset = workspace_slot_offset * VMRegImpl::stack_slot_size;
1062 
1063   // We do not accept an argument in a VMRegPair to be spread over two slots,
1064   // no matter what physical location (reg or stack) the slots may have.
1065   // We just check for the unaccepted slot to be invalid.
1066   assert(!src.second()-&gt;is_valid(), &quot;float in arg spread over two slots&quot;);
1067   assert(!dst.second()-&gt;is_valid(), &quot;float out arg spread over two slots&quot;);
1068 
1069   if (src.first()-&gt;is_stack()) {
1070     if (dst.first()-&gt;is_stack()) {
1071       // stack -&gt; stack. The easiest of the bunch.
1072       __ z_mvc(Address(Z_SP, reg2offset(dst.first())),
1073                Address(Z_SP, reg2offset(src.first()) + frame_offset), sizeof(float));
1074     } else {
1075       // stack to reg
1076       Address memaddr(Z_SP, reg2offset(src.first()) + frame_offset);
1077       if (dst.first()-&gt;is_Register()) {
1078         __ mem2reg_opt(dst.first()-&gt;as_Register(), memaddr, false);
1079       } else {
1080         __ mem2freg_opt(dst.first()-&gt;as_FloatRegister(), memaddr, false);
1081       }
1082     }
1083   } else if (src.first()-&gt;is_Register()) {
1084     if (dst.first()-&gt;is_stack()) {
1085       // gpr -&gt; stack
1086       __ reg2mem_opt(src.first()-&gt;as_Register(),
1087                      Address(Z_SP, reg2offset(dst.first()), false ));
1088     } else {
1089       if (dst.first()-&gt;is_Register()) {
1090         // gpr -&gt; gpr
1091         __ move_reg_if_needed(dst.first()-&gt;as_Register(), T_INT,
1092                               src.first()-&gt;as_Register(), T_INT);
1093       } else {
1094         if (VM_Version::has_FPSupportEnhancements()) {
1095           // gpr -&gt; fpr. Exploit z10 capability of direct transfer.
1096           __ z_ldgr(dst.first()-&gt;as_FloatRegister(), src.first()-&gt;as_Register());
1097         } else {
1098           // gpr -&gt; fpr. Use work space on stack to transfer data.
1099           Address   stackaddr(Z_SP, workspace_offset);
1100 
1101           __ reg2mem_opt(src.first()-&gt;as_Register(), stackaddr, false);
1102           __ mem2freg_opt(dst.first()-&gt;as_FloatRegister(), stackaddr, false);
1103         }
1104       }
1105     }
1106   } else {
1107     if (dst.first()-&gt;is_stack()) {
1108       // fpr -&gt; stack
1109       __ freg2mem_opt(src.first()-&gt;as_FloatRegister(),
1110                       Address(Z_SP, reg2offset(dst.first())), false);
1111     } else {
1112       if (dst.first()-&gt;is_Register()) {
1113         if (VM_Version::has_FPSupportEnhancements()) {
1114           // fpr -&gt; gpr.
1115           __ z_lgdr(dst.first()-&gt;as_Register(), src.first()-&gt;as_FloatRegister());
1116         } else {
1117           // fpr -&gt; gpr. Use work space on stack to transfer data.
1118           Address   stackaddr(Z_SP, workspace_offset);
1119 
1120           __ freg2mem_opt(src.first()-&gt;as_FloatRegister(), stackaddr, false);
1121           __ mem2reg_opt(dst.first()-&gt;as_Register(), stackaddr, false);
1122         }
1123       } else {
1124         // fpr -&gt; fpr
1125         __ move_freg_if_needed(dst.first()-&gt;as_FloatRegister(), T_FLOAT,
1126                                src.first()-&gt;as_FloatRegister(), T_FLOAT);
1127       }
1128     }
1129   }
1130 }
1131 
1132 //----------------------------------------------------------------------
1133 // A double arg. May have to do double reg to long reg conversion
1134 //----------------------------------------------------------------------
1135 static void double_move(MacroAssembler *masm,
1136                         VMRegPair src,
1137                         VMRegPair dst,
1138                         int framesize_in_slots,
1139                         int workspace_slot_offset) {
1140   int frame_offset = framesize_in_slots*VMRegImpl::stack_slot_size;
1141   int workspace_offset = workspace_slot_offset*VMRegImpl::stack_slot_size;
1142 
1143   // Since src is always a java calling convention we know that the
1144   // src pair is always either all registers or all stack (and aligned?)
1145 
1146   if (src.first()-&gt;is_stack()) {
1147     if (dst.first()-&gt;is_stack()) {
1148       // stack -&gt; stack. The easiest of the bunch.
1149       __ z_mvc(Address(Z_SP, reg2offset(dst.first())),
1150                Address(Z_SP, reg2offset(src.first()) + frame_offset), sizeof(double));
1151     } else {
1152       // stack to reg
1153       Address stackaddr(Z_SP, reg2offset(src.first()) + frame_offset);
1154 
1155       if (dst.first()-&gt;is_Register()) {
1156         __ mem2reg_opt(dst.first()-&gt;as_Register(), stackaddr);
1157       } else {
1158         __ mem2freg_opt(dst.first()-&gt;as_FloatRegister(), stackaddr);
1159       }
1160     }
1161   } else if (src.first()-&gt;is_Register()) {
1162     if (dst.first()-&gt;is_stack()) {
1163       // gpr -&gt; stack
1164       __ reg2mem_opt(src.first()-&gt;as_Register(),
1165                      Address(Z_SP, reg2offset(dst.first())));
1166     } else {
1167       if (dst.first()-&gt;is_Register()) {
1168         // gpr -&gt; gpr
1169         __ move_reg_if_needed(dst.first()-&gt;as_Register(), T_LONG,
1170                               src.first()-&gt;as_Register(), T_LONG);
1171       } else {
1172         if (VM_Version::has_FPSupportEnhancements()) {
1173           // gpr -&gt; fpr. Exploit z10 capability of direct transfer.
1174           __ z_ldgr(dst.first()-&gt;as_FloatRegister(), src.first()-&gt;as_Register());
1175         } else {
1176           // gpr -&gt; fpr. Use work space on stack to transfer data.
1177           Address stackaddr(Z_SP, workspace_offset);
1178           __ reg2mem_opt(src.first()-&gt;as_Register(), stackaddr);
1179           __ mem2freg_opt(dst.first()-&gt;as_FloatRegister(), stackaddr);
1180         }
1181       }
1182     }
1183   } else {
1184     if (dst.first()-&gt;is_stack()) {
1185       // fpr -&gt; stack
1186       __ freg2mem_opt(src.first()-&gt;as_FloatRegister(),
1187                       Address(Z_SP, reg2offset(dst.first())));
1188     } else {
1189       if (dst.first()-&gt;is_Register()) {
1190         if (VM_Version::has_FPSupportEnhancements()) {
1191           // fpr -&gt; gpr. Exploit z10 capability of direct transfer.
1192           __ z_lgdr(dst.first()-&gt;as_Register(), src.first()-&gt;as_FloatRegister());
1193         } else {
1194           // fpr -&gt; gpr. Use work space on stack to transfer data.
1195           Address stackaddr(Z_SP, workspace_offset);
1196 
1197           __ freg2mem_opt(src.first()-&gt;as_FloatRegister(), stackaddr);
1198           __ mem2reg_opt(dst.first()-&gt;as_Register(), stackaddr);
1199         }
1200       } else {
1201         // fpr -&gt; fpr
1202         // In theory these overlap but the ordering is such that this is likely a nop.
1203         __ move_freg_if_needed(dst.first()-&gt;as_FloatRegister(), T_DOUBLE,
1204                                src.first()-&gt;as_FloatRegister(), T_DOUBLE);
1205       }
1206     }
1207   }
1208 }
1209 
1210 //----------------------------------------------------------------------
1211 // A long arg.
1212 //----------------------------------------------------------------------
1213 static void long_move(MacroAssembler *masm,
1214                       VMRegPair src,
1215                       VMRegPair dst,
1216                       int framesize_in_slots) {
1217   int frame_offset = framesize_in_slots*VMRegImpl::stack_slot_size;
1218 
1219   if (src.first()-&gt;is_stack()) {
1220     if (dst.first()-&gt;is_stack()) {
1221       // stack -&gt; stack. The easiest of the bunch.
1222       __ z_mvc(Address(Z_SP, reg2offset(dst.first())),
1223                Address(Z_SP, reg2offset(src.first()) + frame_offset), sizeof(long));
1224     } else {
1225       // stack to reg
1226       assert(dst.first()-&gt;is_Register(), &quot;long dst value must be in GPR&quot;);
1227       __ mem2reg_opt(dst.first()-&gt;as_Register(),
1228                       Address(Z_SP, reg2offset(src.first()) + frame_offset));
1229     }
1230   } else {
1231     // reg to reg
1232     assert(src.first()-&gt;is_Register(), &quot;long src value must be in GPR&quot;);
1233     if (dst.first()-&gt;is_stack()) {
1234       // reg -&gt; stack
1235       __ reg2mem_opt(src.first()-&gt;as_Register(),
1236                      Address(Z_SP, reg2offset(dst.first())));
1237     } else {
1238       // reg -&gt; reg
1239       assert(dst.first()-&gt;is_Register(), &quot;long dst value must be in GPR&quot;);
1240       __ move_reg_if_needed(dst.first()-&gt;as_Register(),
1241                             T_LONG, src.first()-&gt;as_Register(), T_LONG);
1242     }
1243   }
1244 }
1245 
1246 
1247 //----------------------------------------------------------------------
1248 // A int-like arg.
1249 //----------------------------------------------------------------------
1250 // On z/Architecture we will store integer like items to the stack as 64 bit
1251 // items, according to the z/Architecture ABI, even though Java would only store
1252 // 32 bits for a parameter.
1253 // We do sign extension for all base types. That is ok since the only
1254 // unsigned base type is T_CHAR, and T_CHAR uses only 16 bits of an int.
1255 // Sign extension 32-&gt;64 bit will thus not affect the value.
1256 //----------------------------------------------------------------------
1257 static void move32_64(MacroAssembler *masm,
1258                       VMRegPair src,
1259                       VMRegPair dst,
1260                       int framesize_in_slots) {
1261   int frame_offset = framesize_in_slots * VMRegImpl::stack_slot_size;
1262 
1263   if (src.first()-&gt;is_stack()) {
1264     Address memaddr(Z_SP, reg2offset(src.first()) + frame_offset);
1265     if (dst.first()-&gt;is_stack()) {
1266       // stack -&gt; stack. MVC not posible due to sign extension.
1267       Address firstaddr(Z_SP, reg2offset(dst.first()));
1268       __ mem2reg_signed_opt(Z_R0_scratch, memaddr);
1269       __ reg2mem_opt(Z_R0_scratch, firstaddr);
1270     } else {
1271       // stack -&gt; reg, sign extended
1272       __ mem2reg_signed_opt(dst.first()-&gt;as_Register(), memaddr);
1273     }
1274   } else {
1275     if (dst.first()-&gt;is_stack()) {
1276       // reg -&gt; stack, sign extended
1277       Address firstaddr(Z_SP, reg2offset(dst.first()));
1278       __ z_lgfr(src.first()-&gt;as_Register(), src.first()-&gt;as_Register());
1279       __ reg2mem_opt(src.first()-&gt;as_Register(), firstaddr);
1280     } else {
1281       // reg -&gt; reg, sign extended
1282       __ z_lgfr(dst.first()-&gt;as_Register(), src.first()-&gt;as_Register());
1283     }
1284   }
1285 }
1286 
1287 static void save_or_restore_arguments(MacroAssembler *masm,
1288                                       const int stack_slots,
1289                                       const int total_in_args,
1290                                       const int arg_save_area,
1291                                       OopMap *map,
1292                                       VMRegPair *in_regs,
1293                                       BasicType *in_sig_bt) {
1294 
1295   // If map is non-NULL then the code should store the values,
1296   // otherwise it should load them.
1297   int slot = arg_save_area;
1298   // Handle double words first.
1299   for (int i = 0; i &lt; total_in_args; i++) {
1300     if (in_regs[i].first()-&gt;is_FloatRegister() &amp;&amp; in_sig_bt[i] == T_DOUBLE) {
1301       int offset = slot * VMRegImpl::stack_slot_size;
1302       slot += VMRegImpl::slots_per_word;
1303       assert(slot &lt;= stack_slots, &quot;overflow (after DOUBLE stack slot)&quot;);
1304       const FloatRegister   freg = in_regs[i].first()-&gt;as_FloatRegister();
1305       Address   stackaddr(Z_SP, offset);
1306       if (map != NULL) {
1307         __ freg2mem_opt(freg, stackaddr);
1308       } else {
1309         __ mem2freg_opt(freg, stackaddr);
1310       }
1311     } else if (in_regs[i].first()-&gt;is_Register() &amp;&amp;
1312                (in_sig_bt[i] == T_LONG || in_sig_bt[i] == T_ARRAY)) {
1313       int offset = slot * VMRegImpl::stack_slot_size;
1314       const Register   reg = in_regs[i].first()-&gt;as_Register();
1315       if (map != NULL) {
1316         __ z_stg(reg, offset, Z_SP);
1317         if (in_sig_bt[i] == T_ARRAY) {
1318           map-&gt;set_oop(VMRegImpl::stack2reg(slot));
1319         }
1320       } else {
1321         __ z_lg(reg, offset, Z_SP);
1322       }
1323       slot += VMRegImpl::slots_per_word;
1324       assert(slot &lt;= stack_slots, &quot;overflow (after LONG/ARRAY stack slot)&quot;);
1325     }
1326   }
1327 
1328   // Save or restore single word registers.
1329   for (int i = 0; i &lt; total_in_args; i++) {
1330     if (in_regs[i].first()-&gt;is_Register()) {
1331       int offset = slot * VMRegImpl::stack_slot_size;
1332       // Value lives in an input register. Save it on stack.
1333       switch (in_sig_bt[i]) {
1334         case T_BOOLEAN:
1335         case T_CHAR:
1336         case T_BYTE:
1337         case T_SHORT:
1338         case T_INT: {
1339           const Register   reg = in_regs[i].first()-&gt;as_Register();
1340           Address   stackaddr(Z_SP, offset);
1341           if (map != NULL) {
1342             __ z_st(reg, stackaddr);
1343           } else {
1344             __ z_lgf(reg, stackaddr);
1345           }
1346           slot++;
1347           assert(slot &lt;= stack_slots, &quot;overflow (after INT or smaller stack slot)&quot;);
1348           break;
1349         }
1350         case T_ARRAY:
1351         case T_LONG:
1352           // handled above
1353           break;
1354         case T_OBJECT:
1355         default: ShouldNotReachHere();
1356       }
1357     } else if (in_regs[i].first()-&gt;is_FloatRegister()) {
1358       if (in_sig_bt[i] == T_FLOAT) {
1359         int offset = slot * VMRegImpl::stack_slot_size;
1360         slot++;
1361         assert(slot &lt;= stack_slots, &quot;overflow (after FLOAT stack slot)&quot;);
1362         const FloatRegister   freg = in_regs[i].first()-&gt;as_FloatRegister();
1363         Address   stackaddr(Z_SP, offset);
1364         if (map != NULL) {
1365           __ freg2mem_opt(freg, stackaddr, false);
1366         } else {
1367           __ mem2freg_opt(freg, stackaddr, false);
1368         }
1369       }
1370     } else if (in_regs[i].first()-&gt;is_stack() &amp;&amp;
1371                in_sig_bt[i] == T_ARRAY &amp;&amp; map != NULL) {
1372       int offset_in_older_frame = in_regs[i].first()-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots();
1373       map-&gt;set_oop(VMRegImpl::stack2reg(offset_in_older_frame + stack_slots));
1374     }
1375   }
1376 }
1377 
1378 // Check GCLocker::needs_gc and enter the runtime if it&#39;s true. This
1379 // keeps a new JNI critical region from starting until a GC has been
1380 // forced. Save down any oops in registers and describe them in an OopMap.
1381 static void check_needs_gc_for_critical_native(MacroAssembler   *masm,
1382                                                 const int stack_slots,
1383                                                 const int total_in_args,
1384                                                 const int arg_save_area,
1385                                                 OopMapSet *oop_maps,
1386                                                 VMRegPair *in_regs,
1387                                                 BasicType *in_sig_bt) {
1388   __ block_comment(&quot;check GCLocker::needs_gc&quot;);
1389   Label cont;
1390 
1391   // Check GCLocker::_needs_gc flag.
1392   __ load_const_optimized(Z_R1_scratch, (long) GCLocker::needs_gc_address());
1393   __ z_cli(0, Z_R1_scratch, 0);
1394   __ z_bre(cont);
1395 
1396   // Save down any values that are live in registers and call into the
1397   // runtime to halt for a GC.
1398   OopMap *map = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
1399 
1400   save_or_restore_arguments(masm, stack_slots, total_in_args,
1401                             arg_save_area, map, in_regs, in_sig_bt);
1402   address the_pc = __ pc();
1403   __ set_last_Java_frame(Z_SP, noreg);
1404 
1405   __ block_comment(&quot;block_for_jni_critical&quot;);
1406   __ z_lgr(Z_ARG1, Z_thread);
1407 
1408   address entry_point = CAST_FROM_FN_PTR(address, SharedRuntime::block_for_jni_critical);
1409   __ call_c(entry_point);
1410   oop_maps-&gt;add_gc_map(__ offset(), map);
1411 
1412   __ reset_last_Java_frame();
1413 
1414   // Reload all the register arguments.
1415   save_or_restore_arguments(masm, stack_slots, total_in_args,
1416                             arg_save_area, NULL, in_regs, in_sig_bt);
1417 
1418   __ bind(cont);
1419 
1420   if (StressCriticalJNINatives) {
1421     // Stress register saving
1422     OopMap *map = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
1423     save_or_restore_arguments(masm, stack_slots, total_in_args,
1424                               arg_save_area, map, in_regs, in_sig_bt);
1425 
1426     // Destroy argument registers.
1427     for (int i = 0; i &lt; total_in_args; i++) {
1428       if (in_regs[i].first()-&gt;is_Register()) {
1429         // Don&#39;t set CC.
1430         __ clear_reg(in_regs[i].first()-&gt;as_Register(), true, false);
1431       } else {
1432         if (in_regs[i].first()-&gt;is_FloatRegister()) {
1433           FloatRegister fr = in_regs[i].first()-&gt;as_FloatRegister();
1434           __ z_lcdbr(fr, fr);
1435         }
1436       }
1437     }
1438 
1439     save_or_restore_arguments(masm, stack_slots, total_in_args,
1440                               arg_save_area, NULL, in_regs, in_sig_bt);
1441   }
1442 }
1443 
1444 static void move_ptr(MacroAssembler *masm,
1445                      VMRegPair src,
1446                      VMRegPair dst,
1447                      int framesize_in_slots) {
1448   int frame_offset = framesize_in_slots * VMRegImpl::stack_slot_size;
1449 
1450   if (src.first()-&gt;is_stack()) {
1451     if (dst.first()-&gt;is_stack()) {
1452       // stack to stack
1453       __ mem2reg_opt(Z_R0_scratch, Address(Z_SP, reg2offset(src.first()) + frame_offset));
1454       __ reg2mem_opt(Z_R0_scratch, Address(Z_SP, reg2offset(dst.first())));
1455     } else {
1456       // stack to reg
1457       __ mem2reg_opt(dst.first()-&gt;as_Register(),
1458                      Address(Z_SP, reg2offset(src.first()) + frame_offset));
1459     }
1460   } else {
1461     if (dst.first()-&gt;is_stack()) {
1462       // reg to stack
1463     __ reg2mem_opt(src.first()-&gt;as_Register(), Address(Z_SP, reg2offset(dst.first())));
1464     } else {
1465     __ lgr_if_needed(dst.first()-&gt;as_Register(), src.first()-&gt;as_Register());
1466     }
1467   }
1468 }
1469 
1470 // Unpack an array argument into a pointer to the body and the length
1471 // if the array is non-null, otherwise pass 0 for both.
1472 static void unpack_array_argument(MacroAssembler *masm,
1473                                    VMRegPair reg,
1474                                    BasicType in_elem_type,
1475                                    VMRegPair body_arg,
1476                                    VMRegPair length_arg,
1477                                    int framesize_in_slots) {
1478   Register tmp_reg = Z_tmp_2;
1479   Register tmp2_reg = Z_tmp_1;
1480 
1481   assert(!body_arg.first()-&gt;is_Register() || body_arg.first()-&gt;as_Register() != tmp_reg,
1482          &quot;possible collision&quot;);
1483   assert(!length_arg.first()-&gt;is_Register() || length_arg.first()-&gt;as_Register() != tmp_reg,
1484          &quot;possible collision&quot;);
1485 
1486   // Pass the length, ptr pair.
1487   NearLabel set_out_args;
1488   VMRegPair tmp, tmp2;
1489 
1490   tmp.set_ptr(tmp_reg-&gt;as_VMReg());
1491   tmp2.set_ptr(tmp2_reg-&gt;as_VMReg());
1492   if (reg.first()-&gt;is_stack()) {
1493     // Load the arg up from the stack.
1494     move_ptr(masm, reg, tmp, framesize_in_slots);
1495     reg = tmp;
1496   }
1497 
1498   const Register first = reg.first()-&gt;as_Register();
1499 
1500   // Don&#39;t set CC, indicate unused result.
1501   (void) __ clear_reg(tmp2_reg, true, false);
1502   if (tmp_reg != first) {
1503     __ clear_reg(tmp_reg, true, false);  // Don&#39;t set CC.
1504   }
1505   __ compare64_and_branch(first, (RegisterOrConstant)0L, Assembler::bcondEqual, set_out_args);
1506   __ z_lgf(tmp2_reg, Address(first, arrayOopDesc::length_offset_in_bytes()));
1507   __ add2reg(tmp_reg, arrayOopDesc::base_offset_in_bytes(in_elem_type), first);
1508 
1509   __ bind(set_out_args);
1510   move_ptr(masm, tmp, body_arg, framesize_in_slots);
1511   move32_64(masm, tmp2, length_arg, framesize_in_slots);
1512 }
1513 
1514 //----------------------------------------------------------------------
1515 // Wrap a JNI call.
1516 //----------------------------------------------------------------------
1517 #undef USE_RESIZE_FRAME
1518 nmethod *SharedRuntime::generate_native_wrapper(MacroAssembler *masm,
1519                                                 const methodHandle&amp; method,
1520                                                 int compile_id,
1521                                                 BasicType *in_sig_bt,
1522                                                 VMRegPair *in_regs,
1523                                                 BasicType ret_type,
1524                                                 address critical_entry) {
1525   int total_in_args = method-&gt;size_of_parameters();
1526   if (method-&gt;is_method_handle_intrinsic()) {
1527     vmIntrinsics::ID iid = method-&gt;intrinsic_id();
1528     intptr_t start = (intptr_t) __ pc();
1529     int vep_offset = ((intptr_t) __ pc()) - start;
1530 
1531     gen_special_dispatch(masm, total_in_args,
1532                          method-&gt;intrinsic_id(), in_sig_bt, in_regs);
1533 
1534     int frame_complete = ((intptr_t)__ pc()) - start; // Not complete, period.
1535 
1536     __ flush();
1537 
1538     int stack_slots = SharedRuntime::out_preserve_stack_slots();  // No out slots at all, actually.
1539 
1540     return nmethod::new_native_nmethod(method,
1541                                        compile_id,
1542                                        masm-&gt;code(),
1543                                        vep_offset,
1544                                        frame_complete,
1545                                        stack_slots / VMRegImpl::slots_per_word,
1546                                        in_ByteSize(-1),
1547                                        in_ByteSize(-1),
1548                                        (OopMapSet *) NULL);
1549   }
1550 
1551 
1552   ///////////////////////////////////////////////////////////////////////
1553   //
1554   //  Precalculations before generating any code
1555   //
1556   ///////////////////////////////////////////////////////////////////////
1557 
1558   bool is_critical_native = true;
1559   address native_func = critical_entry;
1560   if (native_func == NULL) {
1561     native_func = method-&gt;native_function();
1562     is_critical_native = false;
1563   }
1564   assert(native_func != NULL, &quot;must have function&quot;);
1565 
1566   //---------------------------------------------------------------------
1567   // We have received a description of where all the java args are located
1568   // on entry to the wrapper. We need to convert these args to where
1569   // the jni function will expect them. To figure out where they go
1570   // we convert the java signature to a C signature by inserting
1571   // the hidden arguments as arg[0] and possibly arg[1] (static method).
1572   //
1573   // The first hidden argument arg[0] is a pointer to the JNI environment.
1574   // It is generated for every call.
1575   // The second argument arg[1] to the JNI call, which is hidden for static
1576   // methods, is the boxed lock object. For static calls, the lock object
1577   // is the static method itself. The oop is constructed here. for instance
1578   // calls, the lock is performed on the object itself, the pointer of
1579   // which is passed as the first visible argument.
1580   //---------------------------------------------------------------------
1581 
1582   // Additionally, on z/Architecture we must convert integers
1583   // to longs in the C signature. We do this in advance in order to have
1584   // no trouble with indexes into the bt-arrays.
1585   // So convert the signature and registers now, and adjust the total number
1586   // of in-arguments accordingly.
1587   bool method_is_static = method-&gt;is_static();
1588   int  total_c_args     = total_in_args;
1589 
1590   if (!is_critical_native) {
1591     int n_hidden_args = method_is_static ? 2 : 1;
1592     total_c_args += n_hidden_args;
1593   } else {
1594     // No JNIEnv*, no this*, but unpacked arrays (base+length).
1595     for (int i = 0; i &lt; total_in_args; i++) {
1596       if (in_sig_bt[i] == T_ARRAY) {
1597         total_c_args ++;
1598       }
1599     }
1600   }
1601 
1602   BasicType *out_sig_bt = NEW_RESOURCE_ARRAY(BasicType, total_c_args);
1603   VMRegPair *out_regs   = NEW_RESOURCE_ARRAY(VMRegPair, total_c_args);
1604   BasicType* in_elem_bt = NULL;
1605 
1606   // Create the signature for the C call:
1607   //   1) add the JNIEnv*
1608   //   2) add the class if the method is static
1609   //   3) copy the rest of the incoming signature (shifted by the number of
1610   //      hidden arguments)
1611 
1612   int argc = 0;
1613   if (!is_critical_native) {
1614     out_sig_bt[argc++] = T_ADDRESS;
1615     if (method-&gt;is_static()) {
1616       out_sig_bt[argc++] = T_OBJECT;
1617     }
1618 
1619     for (int i = 0; i &lt; total_in_args; i++) {
1620       out_sig_bt[argc++] = in_sig_bt[i];
1621     }
1622   } else {
1623     in_elem_bt = NEW_RESOURCE_ARRAY(BasicType, total_in_args);
1624     SignatureStream ss(method-&gt;signature());
1625     int o = 0;
1626     for (int i = 0; i &lt; total_in_args; i++, o++) {
1627       if (in_sig_bt[i] == T_ARRAY) {
1628         // Arrays are passed as tuples (int, elem*).
1629         ss.skip_array_prefix(1);  // skip one &#39;[&#39;
1630         assert(ss.is_primitive(), &quot;primitive type expected&quot;);
1631         in_elem_bt[o] = ss.type();
1632       } else {
1633         in_elem_bt[o] = T_VOID;
1634       }
1635       if (in_sig_bt[i] != T_VOID) {
1636         assert(in_sig_bt[i] == ss.type() ||
1637                in_sig_bt[i] == T_ARRAY, &quot;must match&quot;);
1638         ss.next();
1639       }
1640     }
1641     assert(total_in_args == o, &quot;must match&quot;);
1642 
1643     for (int i = 0; i &lt; total_in_args; i++) {
1644       if (in_sig_bt[i] == T_ARRAY) {
1645         // Arrays are passed as tuples (int, elem*).
1646         out_sig_bt[argc++] = T_INT;
1647         out_sig_bt[argc++] = T_ADDRESS;
1648       } else {
1649         out_sig_bt[argc++] = in_sig_bt[i];
1650       }
1651     }
1652   }
1653 
1654   ///////////////////////////////////////////////////////////////////////
1655   // Now figure out where the args must be stored and how much stack space
1656   // they require (neglecting out_preserve_stack_slots but providing space
1657   // for storing the first five register arguments).
1658   // It&#39;s weird, see int_stk_helper.
1659   ///////////////////////////////////////////////////////////////////////
1660 
1661   //---------------------------------------------------------------------
1662   // Compute framesize for the wrapper.
1663   //
1664   // - We need to handlize all oops passed in registers.
1665   // - We must create space for them here that is disjoint from the save area.
1666   // - We always just allocate 5 words for storing down these object.
1667   //   This allows us to simply record the base and use the Ireg number to
1668   //   decide which slot to use.
1669   // - Note that the reg number used to index the stack slot is the inbound
1670   //   number, not the outbound number.
1671   // - We must shuffle args to match the native convention,
1672   //   and to include var-args space.
1673   //---------------------------------------------------------------------
1674 
1675   //---------------------------------------------------------------------
1676   // Calculate the total number of stack slots we will need:
1677   // - 1) abi requirements
1678   // - 2) outgoing args
1679   // - 3) space for inbound oop handle area
1680   // - 4) space for handlizing a klass if static method
1681   // - 5) space for a lock if synchronized method
1682   // - 6) workspace (save rtn value, int&lt;-&gt;float reg moves, ...)
1683   // - 7) filler slots for alignment
1684   //---------------------------------------------------------------------
1685   // Here is how the space we have allocated will look like.
1686   // Since we use resize_frame, we do not create a new stack frame,
1687   // but just extend the one we got with our own data area.
1688   //
1689   // If an offset or pointer name points to a separator line, it is
1690   // assumed that addressing with offset 0 selects storage starting
1691   // at the first byte above the separator line.
1692   //
1693   //
1694   //     ...                   ...
1695   //      | caller&#39;s frame      |
1696   // FP-&gt; |---------------------|
1697   //      | filler slots, if any|
1698   //     7| #slots == mult of 2 |
1699   //      |---------------------|
1700   //      | work space          |
1701   //     6| 2 slots = 8 bytes   |
1702   //      |---------------------|
1703   //     5| lock box (if sync)  |
1704   //      |---------------------| &lt;- lock_slot_offset
1705   //     4| klass (if static)   |
1706   //      |---------------------| &lt;- klass_slot_offset
1707   //     3| oopHandle area      |
1708   //      | (save area for      |
1709   //      |  critical natives)  |
1710   //      |                     |
1711   //      |                     |
1712   //      |---------------------| &lt;- oop_handle_offset
1713   //     2| outbound memory     |
1714   //     ...                   ...
1715   //      | based arguments     |
1716   //      |---------------------|
1717   //      | vararg              |
1718   //     ...                   ...
1719   //      | area                |
1720   //      |---------------------| &lt;- out_arg_slot_offset
1721   //     1| out_preserved_slots |
1722   //     ...                   ...
1723   //      | (z_abi spec)        |
1724   // SP-&gt; |---------------------| &lt;- FP_slot_offset (back chain)
1725   //     ...                   ...
1726   //
1727   //---------------------------------------------------------------------
1728 
1729   // *_slot_offset indicates offset from SP in #stack slots
1730   // *_offset      indicates offset from SP in #bytes
1731 
1732   int stack_slots = c_calling_convention(out_sig_bt, out_regs, /*regs2=*/NULL, total_c_args) + // 1+2
1733                     SharedRuntime::out_preserve_stack_slots(); // see c_calling_convention
1734 
1735   // Now the space for the inbound oop handle area.
1736   int total_save_slots = RegisterImpl::number_of_arg_registers * VMRegImpl::slots_per_word;
1737   if (is_critical_native) {
1738     // Critical natives may have to call out so they need a save area
1739     // for register arguments.
1740     int double_slots = 0;
1741     int single_slots = 0;
1742     for (int i = 0; i &lt; total_in_args; i++) {
1743       if (in_regs[i].first()-&gt;is_Register()) {
1744         const Register reg = in_regs[i].first()-&gt;as_Register();
1745         switch (in_sig_bt[i]) {
1746           case T_BOOLEAN:
1747           case T_BYTE:
1748           case T_SHORT:
1749           case T_CHAR:
1750           case T_INT:
1751           // Fall through.
1752           case T_ARRAY:
1753           case T_LONG: double_slots++; break;
1754           default:  ShouldNotReachHere();
1755         }
1756       } else {
1757         if (in_regs[i].first()-&gt;is_FloatRegister()) {
1758           switch (in_sig_bt[i]) {
1759             case T_FLOAT:  single_slots++; break;
1760             case T_DOUBLE: double_slots++; break;
1761             default:  ShouldNotReachHere();
1762           }
1763         }
1764       }
1765     }  // for
1766     total_save_slots = double_slots * 2 + align_up(single_slots, 2); // Round to even.
1767   }
1768 
1769   int oop_handle_slot_offset = stack_slots;
1770   stack_slots += total_save_slots;                                        // 3)
1771 
1772   int klass_slot_offset = 0;
1773   int klass_offset      = -1;
1774   if (method_is_static &amp;&amp; !is_critical_native) {                          // 4)
1775     klass_slot_offset  = stack_slots;
1776     klass_offset       = klass_slot_offset * VMRegImpl::stack_slot_size;
1777     stack_slots       += VMRegImpl::slots_per_word;
1778   }
1779 
1780   int lock_slot_offset = 0;
1781   int lock_offset      = -1;
1782   if (method-&gt;is_synchronized()) {                                        // 5)
1783     lock_slot_offset   = stack_slots;
1784     lock_offset        = lock_slot_offset * VMRegImpl::stack_slot_size;
1785     stack_slots       += VMRegImpl::slots_per_word;
1786   }
1787 
1788   int workspace_slot_offset= stack_slots;                                 // 6)
1789   stack_slots         += 2;
1790 
1791   // Now compute actual number of stack words we need.
1792   // Round to align stack properly.
1793   stack_slots = align_up(stack_slots,                                     // 7)
1794                          frame::alignment_in_bytes / VMRegImpl::stack_slot_size);
1795   int frame_size_in_bytes = stack_slots * VMRegImpl::stack_slot_size;
1796 
1797 
1798   ///////////////////////////////////////////////////////////////////////
1799   // Now we can start generating code
1800   ///////////////////////////////////////////////////////////////////////
1801 
1802   unsigned int wrapper_CodeStart  = __ offset();
1803   unsigned int wrapper_UEPStart;
1804   unsigned int wrapper_VEPStart;
1805   unsigned int wrapper_FrameDone;
1806   unsigned int wrapper_CRegsSet;
1807   Label     handle_pending_exception;
1808   Label     ic_miss;
1809 
1810   //---------------------------------------------------------------------
1811   // Unverified entry point (UEP)
1812   //---------------------------------------------------------------------
1813   wrapper_UEPStart = __ offset();
1814 
1815   // check ic: object class &lt;-&gt; cached class
1816   if (!method_is_static) __ nmethod_UEP(ic_miss);
1817   // Fill with nops (alignment of verified entry point).
1818   __ align(CodeEntryAlignment);
1819 
1820   //---------------------------------------------------------------------
1821   // Verified entry point (VEP)
1822   //---------------------------------------------------------------------
1823   wrapper_VEPStart = __ offset();
1824 
1825   if (VM_Version::supports_fast_class_init_checks() &amp;&amp; method-&gt;needs_clinit_barrier()) {
1826     Label L_skip_barrier;
1827     Register klass = Z_R1_scratch;
1828     // Notify OOP recorder (don&#39;t need the relocation)
1829     AddressLiteral md = __ constant_metadata_address(method-&gt;method_holder());
1830     __ load_const_optimized(klass, md.value());
1831     __ clinit_barrier(klass, Z_thread, &amp;L_skip_barrier /*L_fast_path*/);
1832 
1833     __ load_const_optimized(klass, SharedRuntime::get_handle_wrong_method_stub());
1834     __ z_br(klass);
1835 
1836     __ bind(L_skip_barrier);
1837   }
1838 
1839   __ save_return_pc();
1840   __ generate_stack_overflow_check(frame_size_in_bytes);  // Check before creating frame.
1841 #ifndef USE_RESIZE_FRAME
1842   __ push_frame(frame_size_in_bytes);                     // Create a new frame for the wrapper.
1843 #else
1844   __ resize_frame(-frame_size_in_bytes, Z_R0_scratch);    // No new frame for the wrapper.
1845                                                           // Just resize the existing one.
1846 #endif
1847 
1848   wrapper_FrameDone = __ offset();
1849 
1850   __ verify_thread();
1851 
1852   // Native nmethod wrappers never take possession of the oop arguments.
1853   // So the caller will gc the arguments.
1854   // The only thing we need an oopMap for is if the call is static.
1855   //
1856   // An OopMap for lock (and class if static), and one for the VM call itself
1857   OopMapSet  *oop_maps        = new OopMapSet();
1858   OopMap     *map             = new OopMap(stack_slots * 2, 0 /* arg_slots*/);
1859 
1860   if (is_critical_native) {
1861     check_needs_gc_for_critical_native(masm, stack_slots, total_in_args,
1862                                        oop_handle_slot_offset, oop_maps, in_regs, in_sig_bt);
1863   }
1864 
1865 
1866   //////////////////////////////////////////////////////////////////////
1867   //
1868   // The Grand Shuffle
1869   //
1870   //////////////////////////////////////////////////////////////////////
1871   //
1872   // We immediately shuffle the arguments so that for any vm call we have
1873   // to make from here on out (sync slow path, jvmti, etc.) we will have
1874   // captured the oops from our caller and have a valid oopMap for them.
1875   //
1876   //--------------------------------------------------------------------
1877   // Natives require 1 or 2 extra arguments over the normal ones: the JNIEnv*
1878   // (derived from JavaThread* which is in Z_thread) and, if static,
1879   // the class mirror instead of a receiver. This pretty much guarantees that
1880   // register layout will not match. We ignore these extra arguments during
1881   // the shuffle. The shuffle is described by the two calling convention
1882   // vectors we have in our possession. We simply walk the java vector to
1883   // get the source locations and the c vector to get the destinations.
1884   //
1885   // This is a trick. We double the stack slots so we can claim
1886   // the oops in the caller&#39;s frame. Since we are sure to have
1887   // more args than the caller doubling is enough to make
1888   // sure we can capture all the incoming oop args from the caller.
1889   //--------------------------------------------------------------------
1890 
1891   // Record sp-based slot for receiver on stack for non-static methods.
1892   int receiver_offset = -1;
1893 
1894   //--------------------------------------------------------------------
1895   // We move the arguments backwards because the floating point registers
1896   // destination will always be to a register with a greater or equal
1897   // register number or the stack.
1898   //   jix is the index of the incoming Java arguments.
1899   //   cix is the index of the outgoing C arguments.
1900   //--------------------------------------------------------------------
1901 
1902 #ifdef ASSERT
1903   bool reg_destroyed[RegisterImpl::number_of_registers];
1904   bool freg_destroyed[FloatRegisterImpl::number_of_registers];
1905   for (int r = 0; r &lt; RegisterImpl::number_of_registers; r++) {
1906     reg_destroyed[r] = false;
1907   }
1908   for (int f = 0; f &lt; FloatRegisterImpl::number_of_registers; f++) {
1909     freg_destroyed[f] = false;
1910   }
1911 #endif // ASSERT
1912 
1913   for (int jix = total_in_args - 1, cix = total_c_args - 1; jix &gt;= 0; jix--, cix--) {
1914 #ifdef ASSERT
1915     if (in_regs[jix].first()-&gt;is_Register()) {
1916       assert(!reg_destroyed[in_regs[jix].first()-&gt;as_Register()-&gt;encoding()], &quot;ack!&quot;);
1917     } else {
1918       if (in_regs[jix].first()-&gt;is_FloatRegister()) {
1919         assert(!freg_destroyed[in_regs[jix].first()-&gt;as_FloatRegister()-&gt;encoding()], &quot;ack!&quot;);
1920       }
1921     }
1922     if (out_regs[cix].first()-&gt;is_Register()) {
1923       reg_destroyed[out_regs[cix].first()-&gt;as_Register()-&gt;encoding()] = true;
1924     } else {
1925       if (out_regs[cix].first()-&gt;is_FloatRegister()) {
1926         freg_destroyed[out_regs[cix].first()-&gt;as_FloatRegister()-&gt;encoding()] = true;
1927       }
1928     }
1929 #endif // ASSERT
1930 
1931     switch (in_sig_bt[jix]) {
1932       // Due to casting, small integers should only occur in pairs with type T_LONG.
1933       case T_BOOLEAN:
1934       case T_CHAR:
1935       case T_BYTE:
1936       case T_SHORT:
1937       case T_INT:
1938         // Move int and do sign extension.
1939         move32_64(masm, in_regs[jix], out_regs[cix], stack_slots);
1940         break;
1941 
1942       case T_LONG :
1943         long_move(masm, in_regs[jix], out_regs[cix], stack_slots);
1944         break;
1945 
1946       case T_ARRAY:
1947         if (is_critical_native) {
1948           int body_arg = cix;
1949           cix -= 1; // Point to length arg.
1950           unpack_array_argument(masm, in_regs[jix], in_elem_bt[jix], out_regs[body_arg], out_regs[cix], stack_slots);
1951           break;
1952         }
1953         // else fallthrough
1954       case T_OBJECT:
1955         assert(!is_critical_native, &quot;no oop arguments&quot;);
1956         object_move(masm, map, oop_handle_slot_offset, stack_slots, in_regs[jix], out_regs[cix],
1957                     ((jix == 0) &amp;&amp; (!method_is_static)),
1958                     &amp;receiver_offset);
1959         break;
1960       case T_VOID:
1961         break;
1962 
1963       case T_FLOAT:
1964         float_move(masm, in_regs[jix], out_regs[cix], stack_slots, workspace_slot_offset);
1965         break;
1966 
1967       case T_DOUBLE:
1968         assert(jix+1 &lt;  total_in_args &amp;&amp; in_sig_bt[jix+1]  == T_VOID &amp;&amp; out_sig_bt[cix+1] == T_VOID, &quot;bad arg list&quot;);
1969         double_move(masm, in_regs[jix], out_regs[cix], stack_slots, workspace_slot_offset);
1970         break;
1971 
1972       case T_ADDRESS:
1973         assert(false, &quot;found T_ADDRESS in java args&quot;);
1974         break;
1975 
1976       default:
1977         ShouldNotReachHere();
1978     }
1979   }
1980 
1981   //--------------------------------------------------------------------
1982   // Pre-load a static method&#39;s oop into ARG2.
1983   // Used both by locking code and the normal JNI call code.
1984   //--------------------------------------------------------------------
1985   if (method_is_static &amp;&amp; !is_critical_native) {
1986     __ set_oop_constant(JNIHandles::make_local(method-&gt;method_holder()-&gt;java_mirror()), Z_ARG2);
1987 
1988     // Now handlize the static class mirror in ARG2. It&#39;s known not-null.
1989     __ z_stg(Z_ARG2, klass_offset, Z_SP);
1990     map-&gt;set_oop(VMRegImpl::stack2reg(klass_slot_offset));
1991     __ add2reg(Z_ARG2, klass_offset, Z_SP);
1992   }
1993 
1994   // Get JNIEnv* which is first argument to native.
1995   if (!is_critical_native) {
1996     __ add2reg(Z_ARG1, in_bytes(JavaThread::jni_environment_offset()), Z_thread);
1997   }
1998 
1999   //////////////////////////////////////////////////////////////////////
2000   // We have all of the arguments setup at this point.
2001   // We MUST NOT touch any outgoing regs from this point on.
2002   // So if we must call out we must push a new frame.
2003   //////////////////////////////////////////////////////////////////////
2004 
2005 
2006   // Calc the current pc into Z_R10 and into wrapper_CRegsSet.
2007   // Both values represent the same position.
2008   __ get_PC(Z_R10);                // PC into register
2009   wrapper_CRegsSet = __ offset();  // and into into variable.
2010 
2011   // Z_R10 now has the pc loaded that we will use when we finally call to native.
2012 
2013   // We use the same pc/oopMap repeatedly when we call out.
2014   oop_maps-&gt;add_gc_map((int)(wrapper_CRegsSet-wrapper_CodeStart), map);
2015 
2016   // Lock a synchronized method.
2017 
2018   if (method-&gt;is_synchronized()) {
2019     assert(!is_critical_native, &quot;unhandled&quot;);
2020 
2021     // ATTENTION: args and Z_R10 must be preserved.
2022     Register r_oop  = Z_R11;
2023     Register r_box  = Z_R12;
2024     Register r_tmp1 = Z_R13;
2025     Register r_tmp2 = Z_R7;
2026     Label done;
2027 
2028     // Load the oop for the object or class. R_carg2_classorobject contains
2029     // either the handlized oop from the incoming arguments or the handlized
2030     // class mirror (if the method is static).
2031     __ z_lg(r_oop, 0, Z_ARG2);
2032 
2033     lock_offset = (lock_slot_offset * VMRegImpl::stack_slot_size);
2034     // Get the lock box slot&#39;s address.
2035     __ add2reg(r_box, lock_offset, Z_SP);
2036 
2037 #ifdef ASSERT
2038     if (UseBiasedLocking)
2039       // Making the box point to itself will make it clear it went unused
2040       // but also be obviously invalid.
2041       __ z_stg(r_box, 0, r_box);
2042 #endif // ASSERT
2043 
2044     // Try fastpath for locking.
2045     // Fast_lock kills r_temp_1, r_temp_2. (Don&#39;t use R1 as temp, won&#39;t work!)
2046     __ compiler_fast_lock_object(r_oop, r_box, r_tmp1, r_tmp2);
2047     __ z_bre(done);
2048 
2049     //-------------------------------------------------------------------------
2050     // None of the above fast optimizations worked so we have to get into the
2051     // slow case of monitor enter. Inline a special case of call_VM that
2052     // disallows any pending_exception.
2053     //-------------------------------------------------------------------------
2054 
2055     Register oldSP = Z_R11;
2056 
2057     __ z_lgr(oldSP, Z_SP);
2058 
2059     RegisterSaver::save_live_registers(masm, RegisterSaver::arg_registers);
2060 
2061     // Prepare arguments for call.
2062     __ z_lg(Z_ARG1, 0, Z_ARG2); // Ynboxed class mirror or unboxed object.
2063     __ add2reg(Z_ARG2, lock_offset, oldSP);
2064     __ z_lgr(Z_ARG3, Z_thread);
2065 
2066     __ set_last_Java_frame(oldSP, Z_R10 /* gc map pc */);
2067 
2068     // Do the call.
2069     __ load_const_optimized(Z_R1_scratch, CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_locking_C));
2070     __ call(Z_R1_scratch);
2071 
2072     __ reset_last_Java_frame();
2073 
2074     RegisterSaver::restore_live_registers(masm, RegisterSaver::arg_registers);
2075 #ifdef ASSERT
2076     { Label L;
2077       __ load_and_test_long(Z_R0, Address(Z_thread, Thread::pending_exception_offset()));
2078       __ z_bre(L);
2079       __ stop(&quot;no pending exception allowed on exit from IR::monitorenter&quot;);
2080       __ bind(L);
2081     }
2082 #endif
2083     __ bind(done);
2084   } // lock for synchronized methods
2085 
2086 
2087   //////////////////////////////////////////////////////////////////////
2088   // Finally just about ready to make the JNI call.
2089   //////////////////////////////////////////////////////////////////////
2090 
2091   // Use that pc we placed in Z_R10 a while back as the current frame anchor.
2092   __ set_last_Java_frame(Z_SP, Z_R10);
2093 
2094   // Transition from _thread_in_Java to _thread_in_native.
2095   __ set_thread_state(_thread_in_native);
2096 
2097 
2098   //////////////////////////////////////////////////////////////////////
2099   // This is the JNI call.
2100   //////////////////////////////////////////////////////////////////////
2101 
2102   __ call_c(native_func);
2103 
2104 
2105   //////////////////////////////////////////////////////////////////////
2106   // We have survived the call once we reach here.
2107   //////////////////////////////////////////////////////////////////////
2108 
2109 
2110   //--------------------------------------------------------------------
2111   // Unpack native results.
2112   //--------------------------------------------------------------------
2113   // For int-types, we do any needed sign-extension required.
2114   // Care must be taken that the return value (in Z_ARG1 = Z_RET = Z_R2
2115   // or in Z_FARG0 = Z_FRET = Z_F0) will survive any VM calls for
2116   // blocking or unlocking.
2117   // An OOP result (handle) is done specially in the slow-path code.
2118   //--------------------------------------------------------------------
2119   switch (ret_type) {
2120     case T_VOID:    break;         // Nothing to do!
2121     case T_FLOAT:   break;         // Got it where we want it (unless slow-path)
2122     case T_DOUBLE:  break;         // Got it where we want it (unless slow-path)
2123     case T_LONG:    break;         // Got it where we want it (unless slow-path)
2124     case T_OBJECT:  break;         // Really a handle.
2125                                    // Cannot de-handlize until after reclaiming jvm_lock.
2126     case T_ARRAY:   break;
2127 
2128     case T_BOOLEAN:                // 0 -&gt; false(0); !0 -&gt; true(1)
2129       __ z_lngfr(Z_RET, Z_RET);    // Force sign bit on except for zero.
2130       __ z_srlg(Z_RET, Z_RET, 63); // Shift sign bit into least significant pos.
2131       break;
2132     case T_BYTE:    __ z_lgbr(Z_RET, Z_RET);  break; // sign extension
2133     case T_CHAR:    __ z_llghr(Z_RET, Z_RET); break; // unsigned result
2134     case T_SHORT:   __ z_lghr(Z_RET, Z_RET);  break; // sign extension
2135     case T_INT:     __ z_lgfr(Z_RET, Z_RET);  break; // sign-extend for beauty.
2136 
2137     default:
2138       ShouldNotReachHere();
2139       break;
2140   }
2141 
2142 
2143   // Switch thread to &quot;native transition&quot; state before reading the synchronization state.
2144   // This additional state is necessary because reading and testing the synchronization
2145   // state is not atomic w.r.t. GC, as this scenario demonstrates:
2146   //   - Java thread A, in _thread_in_native state, loads _not_synchronized and is preempted.
2147   //   - VM thread changes sync state to synchronizing and suspends threads for GC.
2148   //   - Thread A is resumed to finish this native method, but doesn&#39;t block here since it
2149   //     didn&#39;t see any synchronization in progress, and escapes.
2150 
2151   // Transition from _thread_in_native to _thread_in_native_trans.
2152   __ set_thread_state(_thread_in_native_trans);
2153 
2154   // Safepoint synchronization
2155   //--------------------------------------------------------------------
2156   // Must we block?
2157   //--------------------------------------------------------------------
2158   // Block, if necessary, before resuming in _thread_in_Java state.
2159   // In order for GC to work, don&#39;t clear the last_Java_sp until after blocking.
2160   //--------------------------------------------------------------------
2161   Label after_transition;
2162   {
2163     Label no_block, sync;
2164 
2165     save_native_result(masm, ret_type, workspace_slot_offset); // Make Z_R2 available as work reg.
2166 
2167     // Force this write out before the read below.
2168     __ z_fence();
2169 
2170     __ safepoint_poll(sync, Z_R1);
2171 
2172     __ load_and_test_int(Z_R0, Address(Z_thread, JavaThread::suspend_flags_offset()));
2173     __ z_bre(no_block);
2174 
2175     // Block. Save any potential method result value before the operation and
2176     // use a leaf call to leave the last_Java_frame setup undisturbed. Doing this
2177     // lets us share the oopMap we used when we went native rather than create
2178     // a distinct one for this pc.
2179     //
2180     __ bind(sync);
2181     __ z_acquire();
2182 
2183     address entry_point = is_critical_native ? CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans_and_transition)
2184                                              : CAST_FROM_FN_PTR(address, JavaThread::check_special_condition_for_native_trans);
2185 
2186     __ call_VM_leaf(entry_point, Z_thread);
2187 
2188     if (is_critical_native) {
2189       restore_native_result(masm, ret_type, workspace_slot_offset);
2190       __ z_bru(after_transition); // No thread state transition here.
2191     }
2192     __ bind(no_block);
2193     restore_native_result(masm, ret_type, workspace_slot_offset);
2194   }
2195 
2196   //--------------------------------------------------------------------
2197   // Thread state is thread_in_native_trans. Any safepoint blocking has
2198   // already happened so we can now change state to _thread_in_Java.
2199   //--------------------------------------------------------------------
2200   // Transition from _thread_in_native_trans to _thread_in_Java.
2201   __ set_thread_state(_thread_in_Java);
2202   __ bind(after_transition);
2203 
2204 
2205   //--------------------------------------------------------------------
2206   // Reguard any pages if necessary.
2207   // Protect native result from being destroyed.
2208   //--------------------------------------------------------------------
2209 
2210   Label no_reguard;
2211 
2212   __ z_cli(Address(Z_thread, JavaThread::stack_guard_state_offset() + in_ByteSize(sizeof(JavaThread::StackGuardState) - 1)),
2213            JavaThread::stack_guard_yellow_reserved_disabled);
2214 
2215   __ z_bre(no_reguard);
2216 
2217   save_native_result(masm, ret_type, workspace_slot_offset);
2218   __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::reguard_yellow_pages), Z_method);
2219   restore_native_result(masm, ret_type, workspace_slot_offset);
2220 
2221   __ bind(no_reguard);
2222 
2223 
2224   // Synchronized methods (slow path only)
2225   // No pending exceptions for now.
2226   //--------------------------------------------------------------------
2227   // Handle possibly pending exception (will unlock if necessary).
2228   // Native result is, if any is live, in Z_FRES or Z_RES.
2229   //--------------------------------------------------------------------
2230   // Unlock
2231   //--------------------------------------------------------------------
2232   if (method-&gt;is_synchronized()) {
2233     const Register r_oop        = Z_R11;
2234     const Register r_box        = Z_R12;
2235     const Register r_tmp1       = Z_R13;
2236     const Register r_tmp2       = Z_R7;
2237     Label done;
2238 
2239     // Get unboxed oop of class mirror or object ...
2240     int   offset = method_is_static ? klass_offset : receiver_offset;
2241 
2242     assert(offset != -1, &quot;&quot;);
2243     __ z_lg(r_oop, offset, Z_SP);
2244 
2245     // ... and address of lock object box.
2246     __ add2reg(r_box, lock_offset, Z_SP);
2247 
2248     // Try fastpath for unlocking.
2249     __ compiler_fast_unlock_object(r_oop, r_box, r_tmp1, r_tmp2); // Don&#39;t use R1 as temp.
2250     __ z_bre(done);
2251 
2252     // Slow path for unlocking.
2253     // Save and restore any potential method result value around the unlocking operation.
2254     const Register R_exc = Z_R11;
2255 
2256     save_native_result(masm, ret_type, workspace_slot_offset);
2257 
2258     // Must save pending exception around the slow-path VM call. Since it&#39;s a
2259     // leaf call, the pending exception (if any) can be kept in a register.
2260     __ z_lg(R_exc, Address(Z_thread, Thread::pending_exception_offset()));
2261     assert(R_exc-&gt;is_nonvolatile(), &quot;exception register must be non-volatile&quot;);
2262 
2263     // Must clear pending-exception before re-entering the VM. Since this is
2264     // a leaf call, pending-exception-oop can be safely kept in a register.
2265     __ clear_mem(Address(Z_thread, Thread::pending_exception_offset()), sizeof(intptr_t));
2266 
2267     // Inline a special case of call_VM that disallows any pending_exception.
2268 
2269     // Get locked oop from the handle we passed to jni.
2270     __ z_lg(Z_ARG1, offset, Z_SP);
2271     __ add2reg(Z_ARG2, lock_offset, Z_SP);
2272     __ z_lgr(Z_ARG3, Z_thread);
2273 
2274     __ load_const_optimized(Z_R1_scratch, CAST_FROM_FN_PTR(address, SharedRuntime::complete_monitor_unlocking_C));
2275 
2276     __ call(Z_R1_scratch);
2277 
2278 #ifdef ASSERT
2279     {
2280       Label L;
2281       __ load_and_test_long(Z_R0, Address(Z_thread, Thread::pending_exception_offset()));
2282       __ z_bre(L);
2283       __ stop(&quot;no pending exception allowed on exit from IR::monitorexit&quot;);
2284       __ bind(L);
2285     }
2286 #endif
2287 
2288     // Check_forward_pending_exception jump to forward_exception if any pending
2289     // exception is set. The forward_exception routine expects to see the
2290     // exception in pending_exception and not in a register. Kind of clumsy,
2291     // since all folks who branch to forward_exception must have tested
2292     // pending_exception first and hence have it in a register already.
2293     __ z_stg(R_exc, Address(Z_thread, Thread::pending_exception_offset()));
2294     restore_native_result(masm, ret_type, workspace_slot_offset);
2295     __ z_bru(done);
2296     __ z_illtrap(0x66);
2297 
2298     __ bind(done);
2299   }
2300 
2301 
2302   //--------------------------------------------------------------------
2303   // Clear &quot;last Java frame&quot; SP and PC.
2304   //--------------------------------------------------------------------
2305   __ verify_thread(); // Z_thread must be correct.
2306 
2307   __ reset_last_Java_frame();
2308 
2309   // Unpack oop result, e.g. JNIHandles::resolve result.
2310   if (is_reference_type(ret_type)) {
2311     __ resolve_jobject(Z_RET, /* tmp1 */ Z_R13, /* tmp2 */ Z_R7);
2312   }
2313 
2314   if (CheckJNICalls) {
2315     // clear_pending_jni_exception_check
2316     __ clear_mem(Address(Z_thread, JavaThread::pending_jni_exception_check_fn_offset()), sizeof(oop));
2317   }
2318 
2319   // Reset handle block.
2320   if (!is_critical_native) {
2321     __ z_lg(Z_R1_scratch, Address(Z_thread, JavaThread::active_handles_offset()));
2322     __ clear_mem(Address(Z_R1_scratch, JNIHandleBlock::top_offset_in_bytes()), 4);
2323 
2324     // Check for pending exceptions.
2325     __ load_and_test_long(Z_R0, Address(Z_thread, Thread::pending_exception_offset()));
2326     __ z_brne(handle_pending_exception);
2327   }
2328 
2329 
2330   //////////////////////////////////////////////////////////////////////
2331   // Return
2332   //////////////////////////////////////////////////////////////////////
2333 
2334 
2335 #ifndef USE_RESIZE_FRAME
2336   __ pop_frame();                     // Pop wrapper frame.
2337 #else
2338   __ resize_frame(frame_size_in_bytes, Z_R0_scratch);  // Revert stack extension.
2339 #endif
2340   __ restore_return_pc();             // This is the way back to the caller.
2341   __ z_br(Z_R14);
2342 
2343 
2344   //////////////////////////////////////////////////////////////////////
2345   // Out-of-line calls to the runtime.
2346   //////////////////////////////////////////////////////////////////////
2347 
2348 
2349   if (!is_critical_native) {
2350 
2351     //---------------------------------------------------------------------
2352     // Handler for pending exceptions (out-of-line).
2353     //---------------------------------------------------------------------
2354     // Since this is a native call, we know the proper exception handler
2355     // is the empty function. We just pop this frame and then jump to
2356     // forward_exception_entry. Z_R14 will contain the native caller&#39;s
2357     // return PC.
2358     __ bind(handle_pending_exception);
2359     __ pop_frame();
2360     __ load_const_optimized(Z_R1_scratch, StubRoutines::forward_exception_entry());
2361     __ restore_return_pc();
2362     __ z_br(Z_R1_scratch);
2363 
2364     //---------------------------------------------------------------------
2365     // Handler for a cache miss (out-of-line)
2366     //---------------------------------------------------------------------
2367     __ call_ic_miss_handler(ic_miss, 0x77, 0, Z_R1_scratch);
2368   }
2369   __ flush();
2370 
2371 
2372   //////////////////////////////////////////////////////////////////////
2373   // end of code generation
2374   //////////////////////////////////////////////////////////////////////
2375 
2376 
2377   nmethod *nm = nmethod::new_native_nmethod(method,
2378                                             compile_id,
2379                                             masm-&gt;code(),
2380                                             (int)(wrapper_VEPStart-wrapper_CodeStart),
2381                                             (int)(wrapper_FrameDone-wrapper_CodeStart),
2382                                             stack_slots / VMRegImpl::slots_per_word,
2383                                             (method_is_static ? in_ByteSize(klass_offset) : in_ByteSize(receiver_offset)),
2384                                             in_ByteSize(lock_offset),
2385                                             oop_maps);
2386 
2387   if (is_critical_native) {
2388     nm-&gt;set_lazy_critical_native(true);
2389   }
2390 
2391   return nm;
2392 }
2393 
2394 static address gen_c2i_adapter(MacroAssembler  *masm,
2395                                int total_args_passed,
2396                                int comp_args_on_stack,
2397                                const BasicType *sig_bt,
2398                                const VMRegPair *regs,
2399                                Label &amp;skip_fixup) {
2400   // Before we get into the guts of the C2I adapter, see if we should be here
2401   // at all. We&#39;ve come from compiled code and are attempting to jump to the
2402   // interpreter, which means the caller made a static call to get here
2403   // (vcalls always get a compiled target if there is one). Check for a
2404   // compiled target. If there is one, we need to patch the caller&#39;s call.
2405 
2406   // These two defs MUST MATCH code in gen_i2c2i_adapter!
2407   const Register ientry = Z_R11;
2408   const Register code   = Z_R11;
2409 
2410   address c2i_entrypoint;
2411   Label   patch_callsite;
2412 
2413   // Regular (verified) c2i entry point.
2414   c2i_entrypoint = __ pc();
2415 
2416   // Call patching needed?
2417   __ load_and_test_long(Z_R0_scratch, method_(code));
2418   __ z_lg(ientry, method_(interpreter_entry));  // Preload interpreter entry (also if patching).
2419   __ z_brne(patch_callsite);                    // Patch required if code != NULL (compiled target exists).
2420 
2421   __ bind(skip_fixup);  // Return point from patch_callsite.
2422 
2423   // Since all args are passed on the stack, total_args_passed*wordSize is the
2424   // space we need. We need ABI scratch area but we use the caller&#39;s since
2425   // it has already been allocated.
2426 
2427   const int abi_scratch = frame::z_top_ijava_frame_abi_size;
2428   int       extraspace  = align_up(total_args_passed, 2)*wordSize + abi_scratch;
2429   Register  sender_SP   = Z_R10;
2430   Register  value       = Z_R12;
2431 
2432   // Remember the senderSP so we can pop the interpreter arguments off of the stack.
2433   // In addition, frame manager expects initial_caller_sp in Z_R10.
2434   __ z_lgr(sender_SP, Z_SP);
2435 
2436   // This should always fit in 14 bit immediate.
2437   __ resize_frame(-extraspace, Z_R0_scratch);
2438 
2439   // We use the caller&#39;s ABI scratch area (out_preserved_stack_slots) for the initial
2440   // args. This essentially moves the callers ABI scratch area from the top to the
2441   // bottom of the arg area.
2442 
2443   int st_off =  extraspace - wordSize;
2444 
2445   // Now write the args into the outgoing interpreter space.
2446   for (int i = 0; i &lt; total_args_passed; i++) {
2447     VMReg r_1 = regs[i].first();
2448     VMReg r_2 = regs[i].second();
2449     if (!r_1-&gt;is_valid()) {
2450       assert(!r_2-&gt;is_valid(), &quot;&quot;);
2451       continue;
2452     }
2453     if (r_1-&gt;is_stack()) {
2454       // The calling convention produces OptoRegs that ignore the preserve area (abi scratch).
2455       // We must account for it here.
2456       int ld_off = (r_1-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;
2457 
2458       if (!r_2-&gt;is_valid()) {
2459         __ z_mvc(Address(Z_SP, st_off), Address(sender_SP, ld_off), sizeof(void*));
2460       } else {
2461         // longs are given 2 64-bit slots in the interpreter,
2462         // but the data is passed in only 1 slot.
2463         if (sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {
2464 #ifdef ASSERT
2465           __ clear_mem(Address(Z_SP, st_off), sizeof(void *));
2466 #endif
2467           st_off -= wordSize;
2468         }
2469         __ z_mvc(Address(Z_SP, st_off), Address(sender_SP, ld_off), sizeof(void*));
2470       }
2471     } else {
2472       if (r_1-&gt;is_Register()) {
2473         if (!r_2-&gt;is_valid()) {
2474           __ z_st(r_1-&gt;as_Register(), st_off, Z_SP);
2475         } else {
2476           // longs are given 2 64-bit slots in the interpreter, but the
2477           // data is passed in only 1 slot.
2478           if (sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {
2479 #ifdef ASSERT
2480             __ clear_mem(Address(Z_SP, st_off), sizeof(void *));
2481 #endif
2482             st_off -= wordSize;
2483           }
2484           __ z_stg(r_1-&gt;as_Register(), st_off, Z_SP);
2485         }
2486       } else {
2487         assert(r_1-&gt;is_FloatRegister(), &quot;&quot;);
2488         if (!r_2-&gt;is_valid()) {
2489           __ z_ste(r_1-&gt;as_FloatRegister(), st_off, Z_SP);
2490         } else {
2491           // In 64bit, doubles are given 2 64-bit slots in the interpreter, but the
2492           // data is passed in only 1 slot.
2493           // One of these should get known junk...
2494 #ifdef ASSERT
2495           __ z_lzdr(Z_F1);
2496           __ z_std(Z_F1, st_off, Z_SP);
2497 #endif
2498           st_off-=wordSize;
2499           __ z_std(r_1-&gt;as_FloatRegister(), st_off, Z_SP);
2500         }
2501       }
2502     }
2503     st_off -= wordSize;
2504   }
2505 
2506 
2507   // Jump to the interpreter just as if interpreter was doing it.
2508   __ add2reg(Z_esp, st_off, Z_SP);
2509 
2510   // Frame_manager expects initial_caller_sp (= SP without resize by c2i) in Z_R10.
2511   __ z_br(ientry);
2512 
2513 
2514   // Prevent illegal entry to out-of-line code.
2515   __ z_illtrap(0x22);
2516 
2517   // Generate out-of-line runtime call to patch caller,
2518   // then continue as interpreted.
2519 
2520   // IF you lose the race you go interpreted.
2521   // We don&#39;t see any possible endless c2i -&gt; i2c -&gt; c2i ...
2522   // transitions no matter how rare.
2523   __ bind(patch_callsite);
2524 
2525   RegisterSaver::save_live_registers(masm, RegisterSaver::arg_registers);
2526   __ call_VM_leaf(CAST_FROM_FN_PTR(address, SharedRuntime::fixup_callers_callsite), Z_method, Z_R14);
2527   RegisterSaver::restore_live_registers(masm, RegisterSaver::arg_registers);
2528   __ z_bru(skip_fixup);
2529 
2530   // end of out-of-line code
2531 
2532   return c2i_entrypoint;
2533 }
2534 
2535 // On entry, the following registers are set
2536 //
2537 //    Z_thread  r8  - JavaThread*
2538 //    Z_method  r9  - callee&#39;s method (method to be invoked)
2539 //    Z_esp     r7  - operand (or expression) stack pointer of caller. one slot above last arg.
2540 //    Z_SP      r15 - SP prepared by call stub such that caller&#39;s outgoing args are near top
2541 //
2542 void SharedRuntime::gen_i2c_adapter(MacroAssembler *masm,
2543                                     int total_args_passed,
2544                                     int comp_args_on_stack,
2545                                     const BasicType *sig_bt,
2546                                     const VMRegPair *regs) {
2547   const Register value = Z_R12;
2548   const Register ld_ptr= Z_esp;
2549 
2550   int ld_offset = total_args_passed * wordSize;
2551 
2552   // Cut-out for having no stack args.
2553   if (comp_args_on_stack) {
2554     // Sig words on the stack are greater than VMRegImpl::stack0. Those in
2555     // registers are below. By subtracting stack0, we either get a negative
2556     // number (all values in registers) or the maximum stack slot accessed.
2557     // Convert VMRegImpl (4 byte) stack slots to words.
2558     int comp_words_on_stack = align_up(comp_args_on_stack*VMRegImpl::stack_slot_size, wordSize)&gt;&gt;LogBytesPerWord;
2559     // Round up to miminum stack alignment, in wordSize
2560     comp_words_on_stack = align_up(comp_words_on_stack, 2);
2561 
2562     __ resize_frame(-comp_words_on_stack*wordSize, Z_R0_scratch);
2563   }
2564 
2565   // Now generate the shuffle code. Pick up all register args and move the
2566   // rest through register value=Z_R12.
2567   for (int i = 0; i &lt; total_args_passed; i++) {
2568     if (sig_bt[i] == T_VOID) {
2569       assert(i &gt; 0 &amp;&amp; (sig_bt[i-1] == T_LONG || sig_bt[i-1] == T_DOUBLE), &quot;missing half&quot;);
2570       continue;
2571     }
2572 
2573     // Pick up 0, 1 or 2 words from ld_ptr.
2574     assert(!regs[i].second()-&gt;is_valid() || regs[i].first()-&gt;next() == regs[i].second(),
2575            &quot;scrambled load targets?&quot;);
2576     VMReg r_1 = regs[i].first();
2577     VMReg r_2 = regs[i].second();
2578     if (!r_1-&gt;is_valid()) {
2579       assert(!r_2-&gt;is_valid(), &quot;&quot;);
2580       continue;
2581     }
2582     if (r_1-&gt;is_FloatRegister()) {
2583       if (!r_2-&gt;is_valid()) {
2584         __ z_le(r_1-&gt;as_FloatRegister(), ld_offset, ld_ptr);
2585         ld_offset-=wordSize;
2586       } else {
2587         // Skip the unused interpreter slot.
2588         __ z_ld(r_1-&gt;as_FloatRegister(), ld_offset - wordSize, ld_ptr);
2589         ld_offset -= 2 * wordSize;
2590       }
2591     } else {
2592       if (r_1-&gt;is_stack()) {
2593         // Must do a memory to memory move.
2594         int st_off = (r_1-&gt;reg2stack() + SharedRuntime::out_preserve_stack_slots()) * VMRegImpl::stack_slot_size;
2595 
2596         if (!r_2-&gt;is_valid()) {
2597           __ z_mvc(Address(Z_SP, st_off), Address(ld_ptr, ld_offset), sizeof(void*));
2598         } else {
2599           // In 64bit, longs are given 2 64-bit slots in the interpreter, but the
2600           // data is passed in only 1 slot.
2601           if (sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {
2602             ld_offset -= wordSize;
2603           }
2604           __ z_mvc(Address(Z_SP, st_off), Address(ld_ptr, ld_offset), sizeof(void*));
2605         }
2606       } else {
2607         if (!r_2-&gt;is_valid()) {
2608           // Not sure we need to do this but it shouldn&#39;t hurt.
2609           if (is_reference_type(sig_bt[i]) || sig_bt[i] == T_ADDRESS) {
2610             __ z_lg(r_1-&gt;as_Register(), ld_offset, ld_ptr);
2611           } else {
2612             __ z_l(r_1-&gt;as_Register(), ld_offset, ld_ptr);
2613           }
2614         } else {
2615           // In 64bit, longs are given 2 64-bit slots in the interpreter, but the
2616           // data is passed in only 1 slot.
2617           if (sig_bt[i] == T_LONG || sig_bt[i] == T_DOUBLE) {
2618             ld_offset -= wordSize;
2619           }
2620           __ z_lg(r_1-&gt;as_Register(), ld_offset, ld_ptr);
2621         }
2622       }
2623       ld_offset -= wordSize;
2624     }
2625   }
2626 
2627   // Jump to the compiled code just as if compiled code was doing it.
2628   // load target address from method oop:
2629   __ z_lg(Z_R1_scratch, Address(Z_method, Method::from_compiled_offset()));
2630 
2631   // Store method oop into thread-&gt;callee_target.
2632   // 6243940: We might end up in handle_wrong_method if
2633   // the callee is deoptimized as we race thru here. If that
2634   // happens we don&#39;t want to take a safepoint because the
2635   // caller frame will look interpreted and arguments are now
2636   // &quot;compiled&quot; so it is much better to make this transition
2637   // invisible to the stack walking code. Unfortunately, if
2638   // we try and find the callee by normal means a safepoint
2639   // is possible. So we stash the desired callee in the thread
2640   // and the vm will find it there should this case occur.
2641   __ z_stg(Z_method, thread_(callee_target));
2642 
2643   __ z_br(Z_R1_scratch);
2644 }
2645 
2646 AdapterHandlerEntry* SharedRuntime::generate_i2c2i_adapters(MacroAssembler *masm,
2647                                                             int total_args_passed,
2648                                                             int comp_args_on_stack,
2649                                                             const BasicType *sig_bt,
2650                                                             const VMRegPair *regs,
2651                                                             AdapterFingerPrint* fingerprint) {
2652   __ align(CodeEntryAlignment);
2653   address i2c_entry = __ pc();
2654   gen_i2c_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs);
2655 
2656   address c2i_unverified_entry;
2657 
2658   Label skip_fixup;
2659   {
2660     Label ic_miss;
2661     const int klass_offset           = oopDesc::klass_offset_in_bytes();
2662     const int holder_klass_offset    = CompiledICHolder::holder_klass_offset();
2663     const int holder_metadata_offset = CompiledICHolder::holder_metadata_offset();
2664 
2665     // Out-of-line call to ic_miss handler.
2666     __ call_ic_miss_handler(ic_miss, 0x11, 0, Z_R1_scratch);
2667 
2668     // Unverified Entry Point UEP
2669     __ align(CodeEntryAlignment);
2670     c2i_unverified_entry = __ pc();
2671 
2672     // Check the pointers.
2673     if (!ImplicitNullChecks || MacroAssembler::needs_explicit_null_check(klass_offset)) {
2674       __ z_ltgr(Z_ARG1, Z_ARG1);
2675       __ z_bre(ic_miss);
2676     }
2677     __ verify_oop(Z_ARG1, FILE_AND_LINE);
2678 
2679     // Check ic: object class &lt;-&gt; cached class
2680     // Compress cached class for comparison. That&#39;s more efficient.
2681     if (UseCompressedClassPointers) {
2682       __ z_lg(Z_R11, holder_klass_offset, Z_method);             // Z_R11 is overwritten a few instructions down anyway.
2683       __ compare_klass_ptr(Z_R11, klass_offset, Z_ARG1, false); // Cached class can&#39;t be zero.
2684     } else {
2685       __ z_clc(klass_offset, sizeof(void *)-1, Z_ARG1, holder_klass_offset, Z_method);
2686     }
2687     __ z_brne(ic_miss);  // Cache miss: call runtime to handle this.
2688 
2689     // This def MUST MATCH code in gen_c2i_adapter!
2690     const Register code = Z_R11;
2691 
2692     __ z_lg(Z_method, holder_metadata_offset, Z_method);
2693     __ load_and_test_long(Z_R0, method_(code));
2694     __ z_brne(ic_miss);  // Cache miss: call runtime to handle this.
2695 
2696     // Fallthru to VEP. Duplicate LTG, but saved taken branch.
2697   }
2698 
2699   address c2i_entry = __ pc();
2700 
2701   // Class initialization barrier for static methods
2702   address c2i_no_clinit_check_entry = NULL;
2703   if (VM_Version::supports_fast_class_init_checks()) {
2704     Label L_skip_barrier;
2705 
2706     { // Bypass the barrier for non-static methods
2707       __ testbit(Address(Z_method, Method::access_flags_offset()), JVM_ACC_STATIC_BIT);
2708       __ z_bfalse(L_skip_barrier); // non-static
2709     }
2710 
2711     Register klass = Z_R11;
2712     __ load_method_holder(klass, Z_method);
2713     __ clinit_barrier(klass, Z_thread, &amp;L_skip_barrier /*L_fast_path*/);
2714 
2715     __ load_const_optimized(klass, SharedRuntime::get_handle_wrong_method_stub());
2716     __ z_br(klass);
2717 
2718     __ bind(L_skip_barrier);
2719     c2i_no_clinit_check_entry = __ pc();
2720   }
2721 
2722   gen_c2i_adapter(masm, total_args_passed, comp_args_on_stack, sig_bt, regs, skip_fixup);
2723 
2724   return AdapterHandlerLibrary::new_entry(fingerprint, i2c_entry, c2i_entry, c2i_unverified_entry, c2i_no_clinit_check_entry);
2725 }
2726 
2727 // This function returns the adjust size (in number of words) to a c2i adapter
2728 // activation for use during deoptimization.
2729 //
2730 // Actually only compiled frames need to be adjusted, but it
2731 // doesn&#39;t harm to adjust entry and interpreter frames, too.
2732 //
2733 int Deoptimization::last_frame_adjust(int callee_parameters, int callee_locals) {
2734   assert(callee_locals &gt;= callee_parameters,
2735           &quot;test and remove; got more parms than locals&quot;);
2736   // Handle the abi adjustment here instead of doing it in push_skeleton_frames.
2737   return (callee_locals - callee_parameters) * Interpreter::stackElementWords +
2738          frame::z_parent_ijava_frame_abi_size / BytesPerWord;
2739 }
2740 
2741 uint SharedRuntime::out_preserve_stack_slots() {
2742   return frame::z_jit_out_preserve_size/VMRegImpl::stack_slot_size;
2743 }
2744 
2745 //
2746 // Frame generation for deopt and uncommon trap blobs.
2747 //
2748 static void push_skeleton_frame(MacroAssembler* masm,
2749                           /* Unchanged */
2750                           Register frame_sizes_reg,
2751                           Register pcs_reg,
2752                           /* Invalidate */
2753                           Register frame_size_reg,
2754                           Register pc_reg) {
2755   BLOCK_COMMENT(&quot;  push_skeleton_frame {&quot;);
2756    __ z_lg(pc_reg, 0, pcs_reg);
2757    __ z_lg(frame_size_reg, 0, frame_sizes_reg);
2758    __ z_stg(pc_reg, _z_abi(return_pc), Z_SP);
2759    Register fp = pc_reg;
2760    __ push_frame(frame_size_reg, fp);
2761 #ifdef ASSERT
2762    // The magic is required for successful walking skeletal frames.
2763    __ load_const_optimized(frame_size_reg/*tmp*/, frame::z_istate_magic_number);
2764    __ z_stg(frame_size_reg, _z_ijava_state_neg(magic), fp);
2765    // Fill other slots that are supposedly not necessary with eye catchers.
2766    __ load_const_optimized(frame_size_reg/*use as tmp*/, 0xdeadbad1);
2767    __ z_stg(frame_size_reg, _z_ijava_state_neg(top_frame_sp), fp);
2768    // The sender_sp of the bottom frame is set before pushing it.
2769    // The sender_sp of non bottom frames is their caller&#39;s top_frame_sp, which
2770    // is unknown here. Luckily it is not needed before filling the frame in
2771    // layout_activation(), we assert this by setting an eye catcher (see
2772    // comments on sender_sp in frame_s390.hpp).
2773    __ z_stg(frame_size_reg, _z_ijava_state_neg(sender_sp), Z_SP);
2774 #endif // ASSERT
2775   BLOCK_COMMENT(&quot;  } push_skeleton_frame&quot;);
2776 }
2777 
2778 // Loop through the UnrollBlock info and create new frames.
2779 static void push_skeleton_frames(MacroAssembler* masm, bool deopt,
2780                             /* read */
2781                             Register unroll_block_reg,
2782                             /* invalidate */
2783                             Register frame_sizes_reg,
2784                             Register number_of_frames_reg,
2785                             Register pcs_reg,
2786                             Register tmp1,
2787                             Register tmp2) {
2788   BLOCK_COMMENT(&quot;push_skeleton_frames {&quot;);
2789   // _number_of_frames is of type int (deoptimization.hpp).
2790   __ z_lgf(number_of_frames_reg,
2791            Address(unroll_block_reg, Deoptimization::UnrollBlock::number_of_frames_offset_in_bytes()));
2792   __ z_lg(pcs_reg,
2793           Address(unroll_block_reg, Deoptimization::UnrollBlock::frame_pcs_offset_in_bytes()));
2794   __ z_lg(frame_sizes_reg,
2795           Address(unroll_block_reg, Deoptimization::UnrollBlock::frame_sizes_offset_in_bytes()));
2796 
2797   // stack: (caller_of_deoptee, ...).
2798 
2799   // If caller_of_deoptee is a compiled frame, then we extend it to make
2800   // room for the callee&#39;s locals and the frame::z_parent_ijava_frame_abi.
2801   // See also Deoptimization::last_frame_adjust() above.
2802   // Note: entry and interpreted frames are adjusted, too. But this doesn&#39;t harm.
2803 
2804   __ z_lgf(Z_R1_scratch,
2805            Address(unroll_block_reg, Deoptimization::UnrollBlock::caller_adjustment_offset_in_bytes()));
2806   __ z_lgr(tmp1, Z_SP);  // Save the sender sp before extending the frame.
2807   __ resize_frame_sub(Z_R1_scratch, tmp2/*tmp*/);
2808   // The oldest skeletal frame requires a valid sender_sp to make it walkable
2809   // (it is required to find the original pc of caller_of_deoptee if it is marked
2810   // for deoptimization - see nmethod::orig_pc_addr()).
2811   __ z_stg(tmp1, _z_ijava_state_neg(sender_sp), Z_SP);
2812 
2813   // Now push the new interpreter frames.
2814   Label loop, loop_entry;
2815 
2816   // Make sure that there is at least one entry in the array.
2817   DEBUG_ONLY(__ z_ltgr(number_of_frames_reg, number_of_frames_reg));
2818   __ asm_assert_ne(&quot;array_size must be &gt; 0&quot;, 0x205);
2819 
2820   __ z_bru(loop_entry);
2821 
2822   __ bind(loop);
2823 
2824   __ add2reg(frame_sizes_reg, wordSize);
2825   __ add2reg(pcs_reg, wordSize);
2826 
2827   __ bind(loop_entry);
2828 
2829   // Allocate a new frame, fill in the pc.
2830   push_skeleton_frame(masm, frame_sizes_reg, pcs_reg, tmp1, tmp2);
2831 
2832   __ z_aghi(number_of_frames_reg, -1);  // Emit AGHI, because it sets the condition code
2833   __ z_brne(loop);
2834 
2835   // Set the top frame&#39;s return pc.
2836   __ add2reg(pcs_reg, wordSize);
2837   __ z_lg(Z_R0_scratch, 0, pcs_reg);
2838   __ z_stg(Z_R0_scratch, _z_abi(return_pc), Z_SP);
2839   BLOCK_COMMENT(&quot;} push_skeleton_frames&quot;);
2840 }
2841 
2842 //------------------------------generate_deopt_blob----------------------------
2843 void SharedRuntime::generate_deopt_blob() {
2844   // Allocate space for the code.
2845   ResourceMark rm;
2846   // Setup code generation tools.
2847   CodeBuffer buffer(&quot;deopt_blob&quot;, 2048, 1024);
2848   InterpreterMacroAssembler* masm = new InterpreterMacroAssembler(&amp;buffer);
2849   Label exec_mode_initialized;
2850   OopMap* map = NULL;
2851   OopMapSet *oop_maps = new OopMapSet();
2852 
2853   unsigned int start_off = __ offset();
2854   Label cont;
2855 
2856   // --------------------------------------------------------------------------
2857   // Normal entry (non-exception case)
2858   //
2859   // We have been called from the deopt handler of the deoptee.
2860   // Z_R14 points behind the call in the deopt handler. We adjust
2861   // it such that it points to the start of the deopt handler.
2862   // The return_pc has been stored in the frame of the deoptee and
2863   // will replace the address of the deopt_handler in the call
2864   // to Deoptimization::fetch_unroll_info below.
2865   // The (int) cast is necessary, because -((unsigned int)14)
2866   // is an unsigned int.
2867   __ add2reg(Z_R14, -(int)NativeCall::max_instruction_size());
2868 
2869   const Register   exec_mode_reg = Z_tmp_1;
2870 
2871   // stack: (deoptee, caller of deoptee, ...)
2872 
2873   // pushes an &quot;unpack&quot; frame
2874   // R14 contains the return address pointing into the deoptimized
2875   // nmethod that was valid just before the nmethod was deoptimized.
2876   // save R14 into the deoptee frame.  the `fetch_unroll_info&#39;
2877   // procedure called below will read it from there.
2878   map = RegisterSaver::save_live_registers(masm, RegisterSaver::all_registers);
2879 
2880   // note the entry point.
2881   __ load_const_optimized(exec_mode_reg, Deoptimization::Unpack_deopt);
2882   __ z_bru(exec_mode_initialized);
2883 
2884 #ifndef COMPILER1
2885   int reexecute_offset = 1; // odd offset will produce odd pc, which triggers an hardware trap
2886 #else
2887   // --------------------------------------------------------------------------
2888   // Reexecute entry
2889   // - Z_R14 = Deopt Handler in nmethod
2890 
2891   int reexecute_offset = __ offset() - start_off;
2892 
2893   // No need to update map as each call to save_live_registers will produce identical oopmap
2894   (void) RegisterSaver::save_live_registers(masm, RegisterSaver::all_registers);
2895 
2896   __ load_const_optimized(exec_mode_reg, Deoptimization::Unpack_reexecute);
2897   __ z_bru(exec_mode_initialized);
2898 #endif
2899 
2900 
2901   // --------------------------------------------------------------------------
2902   // Exception entry. We reached here via a branch. Registers on entry:
2903   // - Z_EXC_OOP (Z_ARG1) = exception oop
2904   // - Z_EXC_PC  (Z_ARG2) = the exception pc.
2905 
2906   int exception_offset = __ offset() - start_off;
2907 
2908   // all registers are dead at this entry point, except for Z_EXC_OOP, and
2909   // Z_EXC_PC which contain the exception oop and exception pc
2910   // respectively.  Set them in TLS and fall thru to the
2911   // unpack_with_exception_in_tls entry point.
2912 
2913   // Store exception oop and pc in thread (location known to GC).
2914   // Need this since the call to &quot;fetch_unroll_info()&quot; may safepoint.
2915   __ z_stg(Z_EXC_OOP, Address(Z_thread, JavaThread::exception_oop_offset()));
2916   __ z_stg(Z_EXC_PC,  Address(Z_thread, JavaThread::exception_pc_offset()));
2917 
2918   // fall through
2919 
2920   int exception_in_tls_offset = __ offset() - start_off;
2921 
2922   // new implementation because exception oop is now passed in JavaThread
2923 
2924   // Prolog for exception case
2925   // All registers must be preserved because they might be used by LinearScan
2926   // Exceptiop oop and throwing PC are passed in JavaThread
2927 
2928   // load throwing pc from JavaThread and us it as the return address of the current frame.
2929   __ z_lg(Z_R1_scratch, Address(Z_thread, JavaThread::exception_pc_offset()));
2930 
2931   // Save everything in sight.
2932   (void) RegisterSaver::save_live_registers(masm, RegisterSaver::all_registers, Z_R1_scratch);
2933 
2934   // Now it is safe to overwrite any register
2935 
2936   // Clear the exception pc field in JavaThread
2937   __ clear_mem(Address(Z_thread, JavaThread::exception_pc_offset()), 8);
2938 
2939   // Deopt during an exception.  Save exec mode for unpack_frames.
2940   __ load_const_optimized(exec_mode_reg, Deoptimization::Unpack_exception);
2941 
2942 
2943 #ifdef ASSERT
2944   // verify that there is really an exception oop in JavaThread
2945   __ z_lg(Z_ARG1, Address(Z_thread, JavaThread::exception_oop_offset()));
2946   __ MacroAssembler::verify_oop(Z_ARG1, FILE_AND_LINE);
2947 
2948   // verify that there is no pending exception
2949   __ asm_assert_mem8_is_zero(in_bytes(Thread::pending_exception_offset()), Z_thread,
2950                              &quot;must not have pending exception here&quot;, __LINE__);
2951 #endif
2952 
2953   // --------------------------------------------------------------------------
2954   // At this point, the live registers are saved and
2955   // the exec_mode_reg has been set up correctly.
2956   __ bind(exec_mode_initialized);
2957 
2958   // stack: (&quot;unpack&quot; frame, deoptee, caller_of_deoptee, ...).
2959 
2960   {
2961   const Register unroll_block_reg  = Z_tmp_2;
2962 
2963   // we need to set `last_Java_frame&#39; because `fetch_unroll_info&#39; will
2964   // call `last_Java_frame()&#39;.  however we can&#39;t block and no gc will
2965   // occur so we don&#39;t need an oopmap. the value of the pc in the
2966   // frame is not particularly important.  it just needs to identify the blob.
2967 
2968   // Don&#39;t set last_Java_pc anymore here (is implicitly NULL then).
2969   // the correct PC is retrieved in pd_last_frame() in that case.
2970   __ set_last_Java_frame(/*sp*/Z_SP, noreg);
2971   // With EscapeAnalysis turned on, this call may safepoint
2972   // despite it&#39;s marked as &quot;leaf call&quot;!
2973   __ call_VM_leaf(CAST_FROM_FN_PTR(address, Deoptimization::fetch_unroll_info), Z_thread, exec_mode_reg);
2974   // Set an oopmap for the call site this describes all our saved volatile registers
2975   int offs = __ offset();
2976   oop_maps-&gt;add_gc_map(offs, map);
2977 
2978   __ reset_last_Java_frame();
2979   // save the return value.
2980   __ z_lgr(unroll_block_reg, Z_RET);
2981   // restore the return registers that have been saved
2982   // (among other registers) by save_live_registers(...).
2983   RegisterSaver::restore_result_registers(masm);
2984 
2985   // reload the exec mode from the UnrollBlock (it might have changed)
2986   __ z_llgf(exec_mode_reg, Address(unroll_block_reg, Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()));
2987 
2988   // In excp_deopt_mode, restore and clear exception oop which we
2989   // stored in the thread during exception entry above. The exception
2990   // oop will be the return value of this stub.
2991   NearLabel skip_restore_excp;
2992   __ compare64_and_branch(exec_mode_reg, Deoptimization::Unpack_exception, Assembler::bcondNotEqual, skip_restore_excp);
2993   __ z_lg(Z_RET, thread_(exception_oop));
2994   __ clear_mem(thread_(exception_oop), 8);
2995   __ bind(skip_restore_excp);
2996 
2997   // remove the &quot;unpack&quot; frame
2998   __ pop_frame();
2999 
3000   // stack: (deoptee, caller of deoptee, ...).
3001 
3002   // pop the deoptee&#39;s frame
3003   __ pop_frame();
3004 
3005   // stack: (caller_of_deoptee, ...).
3006 
3007   // loop through the `UnrollBlock&#39; info and create interpreter frames.
3008   push_skeleton_frames(masm, true/*deopt*/,
3009                   unroll_block_reg,
3010                   Z_tmp_3,
3011                   Z_tmp_4,
3012                   Z_ARG5,
3013                   Z_ARG4,
3014                   Z_ARG3);
3015 
3016   // stack: (skeletal interpreter frame, ..., optional skeletal
3017   // interpreter frame, caller of deoptee, ...).
3018   }
3019 
3020   // push an &quot;unpack&quot; frame taking care of float / int return values.
3021   __ push_frame(RegisterSaver::live_reg_frame_size(RegisterSaver::all_registers));
3022 
3023   // stack: (unpack frame, skeletal interpreter frame, ..., optional
3024   // skeletal interpreter frame, caller of deoptee, ...).
3025 
3026   // spill live volatile registers since we&#39;ll do a call.
3027   __ z_stg(Z_RET, offset_of(frame::z_abi_160_spill, spill[0]), Z_SP);
3028   __ z_std(Z_FRET, offset_of(frame::z_abi_160_spill, spill[1]), Z_SP);
3029 
3030   // let the unpacker layout information in the skeletal frames just allocated.
3031   __ get_PC(Z_RET);
3032   __ set_last_Java_frame(/*sp*/Z_SP, /*pc*/Z_RET);
3033   __ call_VM_leaf(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames),
3034                   Z_thread/*thread*/, exec_mode_reg/*exec_mode*/);
3035 
3036   __ reset_last_Java_frame();
3037 
3038   // restore the volatiles saved above.
3039   __ z_lg(Z_RET, offset_of(frame::z_abi_160_spill, spill[0]), Z_SP);
3040   __ z_ld(Z_FRET, offset_of(frame::z_abi_160_spill, spill[1]), Z_SP);
3041 
3042   // pop the &quot;unpack&quot; frame.
3043   __ pop_frame();
3044   __ restore_return_pc();
3045 
3046   // stack: (top interpreter frame, ..., optional interpreter frame,
3047   // caller of deoptee, ...).
3048 
3049   __ z_lg(Z_fp, _z_abi(callers_sp), Z_SP); // restore frame pointer
3050   __ restore_bcp();
3051   __ restore_locals();
3052   __ restore_esp();
3053 
3054   // return to the interpreter entry point.
3055   __ z_br(Z_R14);
3056 
3057   // Make sure all code is generated
3058   masm-&gt;flush();
3059 
3060   _deopt_blob = DeoptimizationBlob::create(&amp;buffer, oop_maps, 0, exception_offset, reexecute_offset, RegisterSaver::live_reg_frame_size(RegisterSaver::all_registers)/wordSize);
3061   _deopt_blob-&gt;set_unpack_with_exception_in_tls_offset(exception_in_tls_offset);
3062 }
3063 
3064 
3065 #ifdef COMPILER2
3066 //------------------------------generate_uncommon_trap_blob--------------------
3067 void SharedRuntime::generate_uncommon_trap_blob() {
3068   // Allocate space for the code
3069   ResourceMark rm;
3070   // Setup code generation tools
3071   CodeBuffer buffer(&quot;uncommon_trap_blob&quot;, 2048, 1024);
3072   InterpreterMacroAssembler* masm = new InterpreterMacroAssembler(&amp;buffer);
3073 
3074   Register unroll_block_reg = Z_tmp_1;
3075   Register klass_index_reg  = Z_ARG2;
3076   Register unc_trap_reg     = Z_ARG2;
3077 
3078   // stack: (deoptee, caller_of_deoptee, ...).
3079 
3080   // push a dummy &quot;unpack&quot; frame and call
3081   // `Deoptimization::uncommon_trap&#39; to pack the compiled frame into a
3082   // vframe array and return the `UnrollBlock&#39; information.
3083 
3084   // save R14 to compiled frame.
3085   __ save_return_pc();
3086   // push the &quot;unpack_frame&quot;.
3087   __ push_frame_abi160(0);
3088 
3089   // stack: (unpack frame, deoptee, caller_of_deoptee, ...).
3090 
3091   // set the &quot;unpack&quot; frame as last_Java_frame.
3092   // `Deoptimization::uncommon_trap&#39; expects it and considers its
3093   // sender frame as the deoptee frame.
3094   __ get_PC(Z_R1_scratch);
3095   __ set_last_Java_frame(/*sp*/Z_SP, /*pc*/Z_R1_scratch);
3096 
3097   __ z_lgr(klass_index_reg, Z_ARG1);  // passed implicitly as ARG2
3098   __ z_lghi(Z_ARG3, Deoptimization::Unpack_uncommon_trap);  // passed implicitly as ARG3
3099   BLOCK_COMMENT(&quot;call Deoptimization::uncommon_trap()&quot;);
3100   __ call_VM_leaf(CAST_FROM_FN_PTR(address, Deoptimization::uncommon_trap), Z_thread);
3101 
3102   __ reset_last_Java_frame();
3103 
3104   // pop the &quot;unpack&quot; frame
3105   __ pop_frame();
3106 
3107   // stack: (deoptee, caller_of_deoptee, ...).
3108 
3109   // save the return value.
3110   __ z_lgr(unroll_block_reg, Z_RET);
3111 
3112   // pop the deoptee frame.
3113   __ pop_frame();
3114 
3115   // stack: (caller_of_deoptee, ...).
3116 
3117 #ifdef ASSERT
3118   assert(Immediate::is_uimm8(Deoptimization::Unpack_LIMIT), &quot;Code not fit for larger immediates&quot;);
3119   assert(Immediate::is_uimm8(Deoptimization::Unpack_uncommon_trap), &quot;Code not fit for larger immediates&quot;);
3120   const int unpack_kind_byte_offset = Deoptimization::UnrollBlock::unpack_kind_offset_in_bytes()
3121 #ifndef VM_LITTLE_ENDIAN
3122   + 3
3123 #endif
3124   ;
3125   if (Displacement::is_shortDisp(unpack_kind_byte_offset)) {
3126     __ z_cli(unpack_kind_byte_offset, unroll_block_reg, Deoptimization::Unpack_uncommon_trap);
3127   } else {
3128     __ z_cliy(unpack_kind_byte_offset, unroll_block_reg, Deoptimization::Unpack_uncommon_trap);
3129   }
3130   __ asm_assert_eq(&quot;SharedRuntime::generate_deopt_blob: expected Unpack_uncommon_trap&quot;, 0);
3131 #endif
3132 
3133   __ zap_from_to(Z_SP, Z_SP, Z_R0_scratch, Z_R1, 500, -1);
3134 
3135   // allocate new interpreter frame(s) and possibly resize the caller&#39;s frame
3136   // (no more adapters !)
3137   push_skeleton_frames(masm, false/*deopt*/,
3138                   unroll_block_reg,
3139                   Z_tmp_2,
3140                   Z_tmp_3,
3141                   Z_tmp_4,
3142                   Z_ARG5,
3143                   Z_ARG4);
3144 
3145   // stack: (skeletal interpreter frame, ..., optional skeletal
3146   // interpreter frame, (resized) caller of deoptee, ...).
3147 
3148   // push a dummy &quot;unpack&quot; frame taking care of float return values.
3149   // call `Deoptimization::unpack_frames&#39; to layout information in the
3150   // interpreter frames just created
3151 
3152   // push the &quot;unpack&quot; frame
3153    const unsigned int framesize_in_bytes = __ push_frame_abi160(0);
3154 
3155   // stack: (unpack frame, skeletal interpreter frame, ..., optional
3156   // skeletal interpreter frame, (resized) caller of deoptee, ...).
3157 
3158   // set the &quot;unpack&quot; frame as last_Java_frame
3159   __ get_PC(Z_R1_scratch);
3160   __ set_last_Java_frame(/*sp*/Z_SP, /*pc*/Z_R1_scratch);
3161 
3162   // indicate it is the uncommon trap case
3163   BLOCK_COMMENT(&quot;call Deoptimization::Unpack_uncommon_trap()&quot;);
3164   __ load_const_optimized(unc_trap_reg, Deoptimization::Unpack_uncommon_trap);
3165   // let the unpacker layout information in the skeletal frames just allocated.
3166   __ call_VM_leaf(CAST_FROM_FN_PTR(address, Deoptimization::unpack_frames), Z_thread);
3167 
3168   __ reset_last_Java_frame();
3169   // pop the &quot;unpack&quot; frame
3170   __ pop_frame();
3171   // restore LR from top interpreter frame
3172   __ restore_return_pc();
3173 
3174   // stack: (top interpreter frame, ..., optional interpreter frame,
3175   // (resized) caller of deoptee, ...).
3176 
3177   __ z_lg(Z_fp, _z_abi(callers_sp), Z_SP); // restore frame pointer
3178   __ restore_bcp();
3179   __ restore_locals();
3180   __ restore_esp();
3181 
3182   // return to the interpreter entry point
3183   __ z_br(Z_R14);
3184 
3185   masm-&gt;flush();
3186   _uncommon_trap_blob = UncommonTrapBlob::create(&amp;buffer, NULL, framesize_in_bytes/wordSize);
3187 }
3188 #endif // COMPILER2
3189 
3190 
3191 //------------------------------generate_handler_blob------
3192 //
3193 // Generate a special Compile2Runtime blob that saves all registers,
3194 // and setup oopmap.
3195 SafepointBlob* SharedRuntime::generate_handler_blob(address call_ptr, int poll_type) {
3196   assert(StubRoutines::forward_exception_entry() != NULL,
3197          &quot;must be generated before&quot;);
3198 
3199   ResourceMark rm;
3200   OopMapSet *oop_maps = new OopMapSet();
3201   OopMap* map;
3202 
3203   // Allocate space for the code. Setup code generation tools.
3204   CodeBuffer buffer(&quot;handler_blob&quot;, 2048, 1024);
3205   MacroAssembler* masm = new MacroAssembler(&amp;buffer);
3206 
3207   unsigned int start_off = __ offset();
3208   address call_pc = NULL;
3209   int frame_size_in_bytes;
3210 
3211   bool cause_return = (poll_type == POLL_AT_RETURN);
3212   // Make room for return address (or push it again)
3213   if (!cause_return) {
3214     __ z_lg(Z_R14, Address(Z_thread, JavaThread::saved_exception_pc_offset()));
3215   }
3216 
3217   // Save registers, fpu state, and flags
3218   map = RegisterSaver::save_live_registers(masm, RegisterSaver::all_registers);
3219 
3220   if (SafepointMechanism::uses_thread_local_poll() &amp;&amp; !cause_return) {
3221     // Keep a copy of the return pc to detect if it gets modified.
3222     __ z_lgr(Z_R6, Z_R14);
3223   }
3224 
3225   // The following is basically a call_VM. However, we need the precise
3226   // address of the call in order to generate an oopmap. Hence, we do all the
3227   // work outselves.
3228   __ set_last_Java_frame(Z_SP, noreg);
3229 
3230   // call into the runtime to handle the safepoint poll
3231   __ call_VM_leaf(call_ptr, Z_thread);
3232 
3233 
3234   // Set an oopmap for the call site. This oopmap will map all
3235   // oop-registers and debug-info registers as callee-saved. This
3236   // will allow deoptimization at this safepoint to find all possible
3237   // debug-info recordings, as well as let GC find all oops.
3238 
3239   oop_maps-&gt;add_gc_map((int)(__ offset()-start_off), map);
3240 
3241   Label noException;
3242 
3243   __ reset_last_Java_frame();
3244 
3245   __ load_and_test_long(Z_R1, thread_(pending_exception));
3246   __ z_bre(noException);
3247 
3248   // Pending exception case, used (sporadically) by
3249   // api/java_lang/Thread.State/index#ThreadState et al.
3250   RegisterSaver::restore_live_registers(masm, RegisterSaver::all_registers);
3251 
3252   // Jump to forward_exception_entry, with the issuing PC in Z_R14
3253   // so it looks like the original nmethod called forward_exception_entry.
3254   __ load_const_optimized(Z_R1_scratch, StubRoutines::forward_exception_entry());
3255   __ z_br(Z_R1_scratch);
3256 
3257   // No exception case
3258   __ bind(noException);
3259 
3260   if (SafepointMechanism::uses_thread_local_poll() &amp;&amp; !cause_return) {
3261     Label no_adjust;
3262      // If our stashed return pc was modified by the runtime we avoid touching it
3263     const int offset_of_return_pc = _z_abi16(return_pc) + RegisterSaver::live_reg_frame_size(RegisterSaver::all_registers);
3264     __ z_cg(Z_R6, offset_of_return_pc, Z_SP);
3265     __ z_brne(no_adjust);
3266 
3267     // Adjust return pc forward to step over the safepoint poll instruction
3268     __ instr_size(Z_R1_scratch, Z_R6);
3269     __ z_agr(Z_R6, Z_R1_scratch);
3270     __ z_stg(Z_R6, offset_of_return_pc, Z_SP);
3271 
3272     __ bind(no_adjust);
3273   }
3274 
3275   // Normal exit, restore registers and exit.
3276   RegisterSaver::restore_live_registers(masm, RegisterSaver::all_registers);
3277 
3278   __ z_br(Z_R14);
3279 
3280   // Make sure all code is generated
3281   masm-&gt;flush();
3282 
3283   // Fill-out other meta info
3284   return SafepointBlob::create(&amp;buffer, oop_maps, RegisterSaver::live_reg_frame_size(RegisterSaver::all_registers)/wordSize);
3285 }
3286 
3287 
3288 //
3289 // generate_resolve_blob - call resolution (static/virtual/opt-virtual/ic-miss
3290 //
3291 // Generate a stub that calls into vm to find out the proper destination
3292 // of a Java call. All the argument registers are live at this point
3293 // but since this is generic code we don&#39;t know what they are and the caller
3294 // must do any gc of the args.
3295 //
3296 RuntimeStub* SharedRuntime::generate_resolve_blob(address destination, const char* name) {
3297   assert (StubRoutines::forward_exception_entry() != NULL, &quot;must be generated before&quot;);
3298 
3299   // allocate space for the code
3300   ResourceMark rm;
3301 
3302   CodeBuffer buffer(name, 1000, 512);
3303   MacroAssembler* masm                = new MacroAssembler(&amp;buffer);
3304 
3305   OopMapSet *oop_maps = new OopMapSet();
3306   OopMap* map = NULL;
3307 
3308   unsigned int start_off = __ offset();
3309 
3310   map = RegisterSaver::save_live_registers(masm, RegisterSaver::all_registers);
3311 
3312   // We must save a PC from within the stub as return PC
3313   // C code doesn&#39;t store the LR where we expect the PC,
3314   // so we would run into trouble upon stack walking.
3315   __ get_PC(Z_R1_scratch);
3316 
3317   unsigned int frame_complete = __ offset();
3318 
3319   __ set_last_Java_frame(/*sp*/Z_SP, Z_R1_scratch);
3320 
3321   __ call_VM_leaf(destination, Z_thread, Z_method);
3322 
3323 
3324   // Set an oopmap for the call site.
3325   // We need this not only for callee-saved registers, but also for volatile
3326   // registers that the compiler might be keeping live across a safepoint.
3327 
3328   oop_maps-&gt;add_gc_map((int)(frame_complete-start_off), map);
3329 
3330   // clear last_Java_sp
3331   __ reset_last_Java_frame();
3332 
3333   // check for pending exceptions
3334   Label pending;
3335   __ load_and_test_long(Z_R0, Address(Z_thread, Thread::pending_exception_offset()));
3336   __ z_brne(pending);
3337 
3338   __ z_lgr(Z_R1_scratch, Z_R2); // r1 is neither saved nor restored, r2 contains the continuation.
3339   RegisterSaver::restore_live_registers(masm, RegisterSaver::all_registers);
3340 
3341   // get the returned method
3342   __ get_vm_result_2(Z_method);
3343 
3344   // We are back the the original state on entry and ready to go.
3345   __ z_br(Z_R1_scratch);
3346 
3347   // Pending exception after the safepoint
3348 
3349   __ bind(pending);
3350 
3351   RegisterSaver::restore_live_registers(masm, RegisterSaver::all_registers);
3352 
3353   // exception pending =&gt; remove activation and forward to exception handler
3354 
3355   __ z_lgr(Z_R2, Z_R0); // pending_exception
3356   __ clear_mem(Address(Z_thread, JavaThread::vm_result_offset()), sizeof(jlong));
3357   __ load_const_optimized(Z_R1_scratch, StubRoutines::forward_exception_entry());
3358   __ z_br(Z_R1_scratch);
3359 
3360   // -------------
3361   // make sure all code is generated
3362   masm-&gt;flush();
3363 
3364   // return the blob
3365   // frame_size_words or bytes??
3366   return RuntimeStub::new_runtime_stub(name, &amp;buffer, frame_complete, RegisterSaver::live_reg_frame_size(RegisterSaver::all_registers)/wordSize,
3367                                        oop_maps, true);
3368 
3369 }
3370 
3371 //------------------------------Montgomery multiplication------------------------
3372 //
3373 
3374 // Subtract 0:b from carry:a. Return carry.
3375 static unsigned long
3376 sub(unsigned long a[], unsigned long b[], unsigned long carry, long len) {
3377   unsigned long i, c = 8 * (unsigned long)(len - 1);
3378   __asm__ __volatile__ (
3379     &quot;SLGR   %[i], %[i]         \n&quot; // initialize to 0 and pre-set carry
3380     &quot;LGHI   0, 8               \n&quot; // index increment (for BRXLG)
3381     &quot;LGR    1, %[c]            \n&quot; // index limit (for BRXLG)
3382     &quot;0:                        \n&quot;
3383     &quot;LG     %[c], 0(%[i],%[a]) \n&quot;
3384     &quot;SLBG   %[c], 0(%[i],%[b]) \n&quot; // subtract with borrow
3385     &quot;STG    %[c], 0(%[i],%[a]) \n&quot;
3386     &quot;BRXLG  %[i], 0, 0b        \n&quot; // while ((i+=8)&lt;limit);
3387     &quot;SLBGR  %[c], %[c]         \n&quot; // save carry - 1
3388     : [i]&quot;=&amp;a&quot;(i), [c]&quot;+r&quot;(c)
3389     : [a]&quot;a&quot;(a), [b]&quot;a&quot;(b)
3390     : &quot;cc&quot;, &quot;memory&quot;, &quot;r0&quot;, &quot;r1&quot;
3391  );
3392   return carry + c;
3393 }
3394 
3395 // Multiply (unsigned) Long A by Long B, accumulating the double-
3396 // length result into the accumulator formed of T0, T1, and T2.
3397 inline void MACC(unsigned long A[], long A_ind,
3398                  unsigned long B[], long B_ind,
3399                  unsigned long &amp;T0, unsigned long &amp;T1, unsigned long &amp;T2) {
3400   long A_si = 8 * A_ind,
3401        B_si = 8 * B_ind;
3402   __asm__ __volatile__ (
3403     &quot;LG     1, 0(%[A_si],%[A]) \n&quot;
3404     &quot;MLG    0, 0(%[B_si],%[B]) \n&quot; // r0r1 = A * B
3405     &quot;ALGR   %[T0], 1           \n&quot;
3406     &quot;LGHI   1, 0               \n&quot; // r1 = 0
3407     &quot;ALCGR  %[T1], 0           \n&quot;
3408     &quot;ALCGR  %[T2], 1           \n&quot;
3409     : [T0]&quot;+r&quot;(T0), [T1]&quot;+r&quot;(T1), [T2]&quot;+r&quot;(T2)
3410     : [A]&quot;r&quot;(A), [A_si]&quot;r&quot;(A_si), [B]&quot;r&quot;(B), [B_si]&quot;r&quot;(B_si)
3411     : &quot;cc&quot;, &quot;r0&quot;, &quot;r1&quot;
3412  );
3413 }
3414 
3415 // As above, but add twice the double-length result into the
3416 // accumulator.
3417 inline void MACC2(unsigned long A[], long A_ind,
3418                   unsigned long B[], long B_ind,
3419                   unsigned long &amp;T0, unsigned long &amp;T1, unsigned long &amp;T2) {
3420   const unsigned long zero = 0;
3421   long A_si = 8 * A_ind,
3422        B_si = 8 * B_ind;
3423   __asm__ __volatile__ (
3424     &quot;LG     1, 0(%[A_si],%[A]) \n&quot;
3425     &quot;MLG    0, 0(%[B_si],%[B]) \n&quot; // r0r1 = A * B
3426     &quot;ALGR   %[T0], 1           \n&quot;
3427     &quot;ALCGR  %[T1], 0           \n&quot;
3428     &quot;ALCGR  %[T2], %[zero]     \n&quot;
3429     &quot;ALGR   %[T0], 1           \n&quot;
3430     &quot;ALCGR  %[T1], 0           \n&quot;
3431     &quot;ALCGR  %[T2], %[zero]     \n&quot;
3432     : [T0]&quot;+r&quot;(T0), [T1]&quot;+r&quot;(T1), [T2]&quot;+r&quot;(T2)
3433     : [A]&quot;r&quot;(A), [A_si]&quot;r&quot;(A_si), [B]&quot;r&quot;(B), [B_si]&quot;r&quot;(B_si), [zero]&quot;r&quot;(zero)
3434     : &quot;cc&quot;, &quot;r0&quot;, &quot;r1&quot;
3435  );
3436 }
3437 
3438 // Fast Montgomery multiplication. The derivation of the algorithm is
3439 // in &quot;A Cryptographic Library for the Motorola DSP56000,
3440 // Dusse and Kaliski, Proc. EUROCRYPT 90, pp. 230-237&quot;.
3441 static void
3442 montgomery_multiply(unsigned long a[], unsigned long b[], unsigned long n[],
3443                     unsigned long m[], unsigned long inv, int len) {
3444   unsigned long t0 = 0, t1 = 0, t2 = 0; // Triple-precision accumulator
3445   int i;
3446 
3447   assert(inv * n[0] == -1UL, &quot;broken inverse in Montgomery multiply&quot;);
3448 
3449   for (i = 0; i &lt; len; i++) {
3450     int j;
3451     for (j = 0; j &lt; i; j++) {
3452       MACC(a, j, b, i-j, t0, t1, t2);
3453       MACC(m, j, n, i-j, t0, t1, t2);
3454     }
3455     MACC(a, i, b, 0, t0, t1, t2);
3456     m[i] = t0 * inv;
3457     MACC(m, i, n, 0, t0, t1, t2);
3458 
3459     assert(t0 == 0, &quot;broken Montgomery multiply&quot;);
3460 
3461     t0 = t1; t1 = t2; t2 = 0;
3462   }
3463 
3464   for (i = len; i &lt; 2 * len; i++) {
3465     int j;
3466     for (j = i - len + 1; j &lt; len; j++) {
3467       MACC(a, j, b, i-j, t0, t1, t2);
3468       MACC(m, j, n, i-j, t0, t1, t2);
3469     }
3470     m[i-len] = t0;
3471     t0 = t1; t1 = t2; t2 = 0;
3472   }
3473 
3474   while (t0) {
3475     t0 = sub(m, n, t0, len);
3476   }
3477 }
3478 
3479 // Fast Montgomery squaring. This uses asymptotically 25% fewer
3480 // multiplies so it should be up to 25% faster than Montgomery
3481 // multiplication. However, its loop control is more complex and it
3482 // may actually run slower on some machines.
3483 static void
3484 montgomery_square(unsigned long a[], unsigned long n[],
3485                   unsigned long m[], unsigned long inv, int len) {
3486   unsigned long t0 = 0, t1 = 0, t2 = 0; // Triple-precision accumulator
3487   int i;
3488 
3489   assert(inv * n[0] == -1UL, &quot;broken inverse in Montgomery multiply&quot;);
3490 
3491   for (i = 0; i &lt; len; i++) {
3492     int j;
3493     int end = (i+1)/2;
3494     for (j = 0; j &lt; end; j++) {
3495       MACC2(a, j, a, i-j, t0, t1, t2);
3496       MACC(m, j, n, i-j, t0, t1, t2);
3497     }
3498     if ((i &amp; 1) == 0) {
3499       MACC(a, j, a, j, t0, t1, t2);
3500     }
3501     for (; j &lt; i; j++) {
3502       MACC(m, j, n, i-j, t0, t1, t2);
3503     }
3504     m[i] = t0 * inv;
3505     MACC(m, i, n, 0, t0, t1, t2);
3506 
3507     assert(t0 == 0, &quot;broken Montgomery square&quot;);
3508 
3509     t0 = t1; t1 = t2; t2 = 0;
3510   }
3511 
3512   for (i = len; i &lt; 2*len; i++) {
3513     int start = i-len+1;
3514     int end = start + (len - start)/2;
3515     int j;
3516     for (j = start; j &lt; end; j++) {
3517       MACC2(a, j, a, i-j, t0, t1, t2);
3518       MACC(m, j, n, i-j, t0, t1, t2);
3519     }
3520     if ((i &amp; 1) == 0) {
3521       MACC(a, j, a, j, t0, t1, t2);
3522     }
3523     for (; j &lt; len; j++) {
3524       MACC(m, j, n, i-j, t0, t1, t2);
3525     }
3526     m[i-len] = t0;
3527     t0 = t1; t1 = t2; t2 = 0;
3528   }
3529 
3530   while (t0) {
3531     t0 = sub(m, n, t0, len);
3532   }
3533 }
3534 
3535 // The threshold at which squaring is advantageous was determined
3536 // experimentally on an i7-3930K (Ivy Bridge) CPU @ 3.5GHz.
3537 // Value seems to be ok for other platforms, too.
3538 #define MONTGOMERY_SQUARING_THRESHOLD 64
3539 
3540 // Copy len longwords from s to d, word-swapping as we go. The
3541 // destination array is reversed.
3542 static void reverse_words(unsigned long *s, unsigned long *d, int len) {
3543   d += len;
3544   while(len-- &gt; 0) {
3545     d--;
3546     unsigned long s_val = *s;
3547     // Swap words in a longword on little endian machines.
3548 #ifdef VM_LITTLE_ENDIAN
3549      Unimplemented();
3550 #endif
3551     *d = s_val;
3552     s++;
3553   }
3554 }
3555 
3556 void SharedRuntime::montgomery_multiply(jint *a_ints, jint *b_ints, jint *n_ints,
3557                                         jint len, jlong inv,
3558                                         jint *m_ints) {
3559   len = len &amp; 0x7fffFFFF; // C2 does not respect int to long conversion for stub calls.
3560   assert(len % 2 == 0, &quot;array length in montgomery_multiply must be even&quot;);
3561   int longwords = len/2;
3562 
3563   // Make very sure we don&#39;t use so much space that the stack might
3564   // overflow. 512 jints corresponds to an 16384-bit integer and
3565   // will use here a total of 8k bytes of stack space.
3566   int total_allocation = longwords * sizeof (unsigned long) * 4;
3567   guarantee(total_allocation &lt;= 8192, &quot;must be&quot;);
3568   unsigned long *scratch = (unsigned long *)alloca(total_allocation);
3569 
3570   // Local scratch arrays
3571   unsigned long
3572     *a = scratch + 0 * longwords,
3573     *b = scratch + 1 * longwords,
3574     *n = scratch + 2 * longwords,
3575     *m = scratch + 3 * longwords;
3576 
3577   reverse_words((unsigned long *)a_ints, a, longwords);
3578   reverse_words((unsigned long *)b_ints, b, longwords);
3579   reverse_words((unsigned long *)n_ints, n, longwords);
3580 
3581   ::montgomery_multiply(a, b, n, m, (unsigned long)inv, longwords);
3582 
3583   reverse_words(m, (unsigned long *)m_ints, longwords);
3584 }
3585 
3586 void SharedRuntime::montgomery_square(jint *a_ints, jint *n_ints,
3587                                       jint len, jlong inv,
3588                                       jint *m_ints) {
3589   len = len &amp; 0x7fffFFFF; // C2 does not respect int to long conversion for stub calls.
3590   assert(len % 2 == 0, &quot;array length in montgomery_square must be even&quot;);
3591   int longwords = len/2;
3592 
3593   // Make very sure we don&#39;t use so much space that the stack might
3594   // overflow. 512 jints corresponds to an 16384-bit integer and
3595   // will use here a total of 6k bytes of stack space.
3596   int total_allocation = longwords * sizeof (unsigned long) * 3;
3597   guarantee(total_allocation &lt;= 8192, &quot;must be&quot;);
3598   unsigned long *scratch = (unsigned long *)alloca(total_allocation);
3599 
3600   // Local scratch arrays
3601   unsigned long
3602     *a = scratch + 0 * longwords,
3603     *n = scratch + 1 * longwords,
3604     *m = scratch + 2 * longwords;
3605 
3606   reverse_words((unsigned long *)a_ints, a, longwords);
3607   reverse_words((unsigned long *)n_ints, n, longwords);
3608 
3609   if (len &gt;= MONTGOMERY_SQUARING_THRESHOLD) {
3610     ::montgomery_square(a, n, m, (unsigned long)inv, longwords);
3611   } else {
3612     ::montgomery_multiply(a, a, n, m, (unsigned long)inv, longwords);
3613   }
3614 
3615   reverse_words(m, (unsigned long *)m_ints, longwords);
3616 }
3617 
3618 extern &quot;C&quot;
3619 int SpinPause() {
3620   return 0;
3621 }
    </pre>
  </body>
</html>